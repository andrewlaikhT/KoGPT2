{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "95R4RqiOuA7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8da7ed62-5024-4947-c623-5446441e1dd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPruP1vbulVa",
        "colab_type": "text"
      },
      "source": [
        "# 필요한 필수 새팅 작업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OwOX8fsYS5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ebee4db8-baaf-431f-effd-9f4fa4f70e67"
      },
      "source": [
        "!ls drive/'My Drive'/'KoGPT2-FineTuning'/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t img\t\t       LICENSE\t    README.md\t      samples1\n",
            "dataset\t\t jupyter_generator.py  main.ipynb   requirements.txt  util\n",
            "Generator.ipynb  jupyter_main.py       main.py\t    runs\n",
            "generator.py\t kogpt2\t\t       __pycache__  samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHALfG-nWlRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd5e4c9d-f98d-41fb-df4e-cc2cf88b1307"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  gdrive  runs  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m6L6j_nYTTl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "d9cf1693-ffac-47a2-ca83-b18282b39701"
      },
      "source": [
        "!pip install -r drive/'My Drive'/'KoGPT2-FineTuning'/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 3)) (0.1.86)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: transformers>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (2.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 6)) (4.38.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (0.29.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (20.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (1.12.43)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.0.41)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (1.15.43)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.9.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (7.1.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers>=2.1.1->-r drive/My Drive/KoGPT2-FineTuning/requirements.txt (line 5)) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tVxUeuEkl1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a6a9d332-2c94-464a-f789-bb7777d165fd"
      },
      "source": [
        "!pip install tensorboardX "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8M3DCwcYlMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/KoGPT2-FineTuning')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-qz4OLnYlSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jupyter_main import main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Jjj58pd1Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx= 'cuda'\n",
        "cachedir='~/kogpt2/'\n",
        "load_path = 'gdrive/My Drive/KoGPT2-FineTuning/checkpoint/KoGPT2_checkpoint_60000.tar'\n",
        "save_path = 'gdrive/My Drive/KoGPT2-FineTuning/checkpoint'\n",
        "data_file_path = 'gdrive/My Drive/KoGPT2-FineTuning/dataset/lyrics_dataset.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umcGNCCYktXo",
        "colab_type": "text"
      },
      "source": [
        "# 모델 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is4QYvO_Q2Jl",
        "colab_type": "code",
        "outputId": "e75f9956-9a8e-4644-bd10-42c4167eb3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 저장 잘 되는지 테스트\n",
        "drive.mount('/content/gdrive')\n",
        "save_path = './gdrive/My Drive/KoGPT2-FineTuning/checkpoint/'\n",
        "\n",
        "f = open(save_path+ 'KoGPT2_checkpoint_' + str(142) + '.tar', 'w')\n",
        "f.write(\"가자\")\n",
        "f.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8a6jWftYoUI",
        "colab_type": "code",
        "outputId": "02b0c09a-a075-4c4d-d8b5-dc48cb5fdbcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls 'gdrive/My Drive/KoGPT2-FineTuning/dataset'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lyrics_dataset.txt  novel_dataset.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A6oaFXJZEim",
        "colab_type": "code",
        "outputId": "5a8c2699-a5bc-4340-e4b1-e81251d851ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#main(temperature = 0.9, top_p = 0.8, top_k = 40, tmp_sent = \"사랑\", text_size = 500, loops = 3)\n",
        "\n",
        "main(load_path = load_path, data_file_path = data_file_path, save_path = save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n",
            "count check :  ['2', '2', '60000']\n",
            "using cached model\n",
            "(89386,)\n",
            "using cached model\n",
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.60000  loss = 3.15352 avg_loss = 3.15352\n",
            "epoch no.0 train no.60010  loss = 2.73536 avg_loss = 2.69996\n",
            "epoch no.0 train no.60020  loss = 3.32190 avg_loss = 2.67778\n",
            "epoch no.0 train no.60030  loss = 2.89873 avg_loss = 2.66061\n",
            "epoch no.0 train no.60040  loss = 2.47423 avg_loss = 2.52377\n",
            "epoch no.0 train no.60050  loss = 2.87459 avg_loss = 2.54393\n",
            "epoch no.0 train no.60060  loss = 1.99232 avg_loss = 2.51048\n",
            "epoch no.0 train no.60070  loss = 1.38820 avg_loss = 2.50595\n",
            "epoch no.0 train no.60080  loss = 2.17775 avg_loss = 2.49414\n",
            "epoch no.0 train no.60090  loss = 2.99300 avg_loss = 2.48343\n",
            "epoch no.0 train no.60100  loss = 2.90807 avg_loss = 2.48984\n",
            "epoch no.0 train no.60110  loss = 2.47235 avg_loss = 2.50635\n",
            "epoch no.0 train no.60120  loss = 2.52444 avg_loss = 2.49105\n",
            "epoch no.0 train no.60130  loss = 3.22279 avg_loss = 2.51057\n",
            "epoch no.0 train no.60140  loss = 1.63420 avg_loss = 2.45522\n",
            "epoch no.0 train no.60150  loss = 2.41991 avg_loss = 2.43807\n",
            "epoch no.0 train no.60160  loss = 2.88132 avg_loss = 2.45587\n",
            "epoch no.0 train no.60170  loss = 2.85165 avg_loss = 2.46348\n",
            "epoch no.0 train no.60180  loss = 2.27193 avg_loss = 2.46770\n",
            "epoch no.0 train no.60190  loss = 2.19008 avg_loss = 2.48199\n",
            "epoch no.0 train no.60200  loss = 2.62275 avg_loss = 2.49416\n",
            "epoch no.0 train no.60210  loss = 3.29873 avg_loss = 2.50367\n",
            "epoch no.0 train no.60220  loss = 1.99000 avg_loss = 2.50554\n",
            "epoch no.0 train no.60230  loss = 3.29936 avg_loss = 2.52099\n",
            "epoch no.0 train no.60240  loss = 2.79358 avg_loss = 2.53636\n",
            "epoch no.0 train no.60250  loss = 2.69224 avg_loss = 2.53574\n",
            "epoch no.0 train no.60260  loss = 2.79027 avg_loss = 2.54561\n",
            "epoch no.0 train no.60270  loss = 2.86148 avg_loss = 2.53591\n",
            "epoch no.0 train no.60280  loss = 1.85712 avg_loss = 2.54203\n",
            "epoch no.0 train no.60290  loss = 2.14524 avg_loss = 2.52956\n",
            "epoch no.0 train no.60300  loss = 3.07280 avg_loss = 2.55547\n",
            "epoch no.0 train no.60310  loss = 3.64147 avg_loss = 2.56431\n",
            "epoch no.0 train no.60320  loss = 2.52189 avg_loss = 2.55350\n",
            "epoch no.0 train no.60330  loss = 3.26655 avg_loss = 2.55226\n",
            "epoch no.0 train no.60340  loss = 3.24642 avg_loss = 2.55108\n",
            "epoch no.0 train no.60350  loss = 2.45396 avg_loss = 2.54864\n",
            "epoch no.0 train no.60360  loss = 2.96624 avg_loss = 2.53943\n",
            "epoch no.0 train no.60370  loss = 2.87891 avg_loss = 2.50899\n",
            "epoch no.0 train no.60380  loss = 2.89685 avg_loss = 2.50074\n",
            "epoch no.0 train no.60390  loss = 2.02128 avg_loss = 2.50583\n",
            "epoch no.0 train no.60400  loss = 2.59303 avg_loss = 2.48864\n",
            "epoch no.0 train no.60410  loss = 2.28139 avg_loss = 2.48073\n",
            "epoch no.0 train no.60420  loss = 2.17542 avg_loss = 2.46911\n",
            "epoch no.0 train no.60430  loss = 3.01958 avg_loss = 2.46815\n",
            "epoch no.0 train no.60440  loss = 2.57233 avg_loss = 2.45770\n",
            "epoch no.0 train no.60450  loss = 2.02689 avg_loss = 2.46559\n",
            "epoch no.0 train no.60460  loss = 1.62652 avg_loss = 2.44962\n",
            "epoch no.0 train no.60470  loss = 2.55707 avg_loss = 2.44034\n",
            "epoch no.0 train no.60480  loss = 2.35505 avg_loss = 2.45399\n",
            "epoch no.0 train no.60490  loss = 2.29993 avg_loss = 2.45298\n",
            "epoch no.0 train no.60500  loss = 3.30399 avg_loss = 2.45746\n",
            "epoch no.0 train no.60510  loss = 2.32873 avg_loss = 2.43815\n",
            "epoch no.0 train no.60520  loss = 2.29747 avg_loss = 2.44923\n",
            "epoch no.0 train no.60530  loss = 2.49887 avg_loss = 2.46380\n",
            "epoch no.0 train no.60540  loss = 2.22749 avg_loss = 2.49349\n",
            "epoch no.0 train no.60550  loss = 3.67760 avg_loss = 2.50777\n",
            "epoch no.0 train no.60560  loss = 3.04273 avg_loss = 2.49950\n",
            "epoch no.0 train no.60570  loss = 2.12608 avg_loss = 2.48679\n",
            "epoch no.0 train no.60580  loss = 2.42382 avg_loss = 2.47737\n",
            "epoch no.0 train no.60590  loss = 2.59666 avg_loss = 2.47956\n",
            "epoch no.0 train no.60600  loss = 2.26171 avg_loss = 2.48332\n",
            "epoch no.0 train no.60610  loss = 2.75692 avg_loss = 2.49716\n",
            "epoch no.0 train no.60620  loss = 2.63545 avg_loss = 2.49074\n",
            "epoch no.0 train no.60630  loss = 2.97403 avg_loss = 2.50574\n",
            "epoch no.0 train no.60640  loss = 2.94818 avg_loss = 2.51068\n",
            "epoch no.0 train no.60650  loss = 2.94771 avg_loss = 2.50925\n",
            "epoch no.0 train no.60660  loss = 2.43613 avg_loss = 2.51901\n",
            "epoch no.0 train no.60670  loss = 3.14980 avg_loss = 2.51729\n",
            "epoch no.0 train no.60680  loss = 2.92237 avg_loss = 2.52821\n",
            "epoch no.0 train no.60690  loss = 1.28014 avg_loss = 2.56270\n",
            "epoch no.0 train no.60700  loss = 2.81502 avg_loss = 2.59077\n",
            "epoch no.0 train no.60710  loss = 2.37538 avg_loss = 2.56560\n",
            "epoch no.0 train no.60720  loss = 2.97955 avg_loss = 2.56135\n",
            "epoch no.0 train no.60730  loss = 3.16057 avg_loss = 2.56624\n",
            "epoch no.0 train no.60740  loss = 3.32471 avg_loss = 2.56306\n",
            "epoch no.0 train no.60750  loss = 2.39685 avg_loss = 2.56406\n",
            "epoch no.0 train no.60760  loss = 2.35149 avg_loss = 2.57801\n",
            "epoch no.0 train no.60770  loss = 3.21676 avg_loss = 2.56005\n",
            "epoch no.0 train no.60780  loss = 2.42186 avg_loss = 2.53738\n",
            "epoch no.0 train no.60790  loss = 1.76025 avg_loss = 2.53776\n",
            "epoch no.0 train no.60800  loss = 2.38873 avg_loss = 2.53283\n",
            "epoch no.0 train no.60810  loss = 2.50896 avg_loss = 2.52465\n",
            "epoch no.0 train no.60820  loss = 1.42712 avg_loss = 2.50003\n",
            "epoch no.0 train no.60830  loss = 3.22362 avg_loss = 2.50987\n",
            "epoch no.0 train no.60840  loss = 2.80543 avg_loss = 2.51712\n",
            "epoch no.0 train no.60850  loss = 2.54455 avg_loss = 2.52504\n",
            "epoch no.0 train no.60860  loss = 2.20404 avg_loss = 2.52580\n",
            "epoch no.0 train no.60870  loss = 2.58408 avg_loss = 2.53344\n",
            "epoch no.0 train no.60880  loss = 2.18827 avg_loss = 2.51728\n",
            "epoch no.0 train no.60890  loss = 2.91025 avg_loss = 2.51827\n",
            "epoch no.0 train no.60900  loss = 3.14937 avg_loss = 2.51979\n",
            "epoch no.0 train no.60910  loss = 2.05259 avg_loss = 2.50427\n",
            "epoch no.0 train no.60920  loss = 3.60246 avg_loss = 2.52488\n",
            "epoch no.0 train no.60930  loss = 2.84235 avg_loss = 2.51924\n",
            "epoch no.0 train no.60940  loss = 2.29163 avg_loss = 2.49035\n",
            "epoch no.0 train no.60950  loss = 2.07023 avg_loss = 2.48116\n",
            "epoch no.0 train no.60960  loss = 2.43317 avg_loss = 2.47959\n",
            "epoch no.0 train no.60970  loss = 3.02871 avg_loss = 2.48358\n",
            "epoch no.0 train no.60980  loss = 2.63213 avg_loss = 2.48977\n",
            "epoch no.0 train no.60990  loss = 2.30929 avg_loss = 2.49721\n",
            "epoch no.0 train no.61000  loss = 2.84484 avg_loss = 2.49894\n",
            "epoch no.0 train no.61010  loss = 2.48034 avg_loss = 2.49444\n",
            "epoch no.0 train no.61020  loss = 3.26619 avg_loss = 2.50478\n",
            "epoch no.0 train no.61030  loss = 2.02508 avg_loss = 2.47053\n",
            "epoch no.0 train no.61040  loss = 3.85221 avg_loss = 2.48389\n",
            "epoch no.0 train no.61050  loss = 2.28252 avg_loss = 2.48786\n",
            "epoch no.0 train no.61060  loss = 2.27715 avg_loss = 2.47901\n",
            "epoch no.0 train no.61070  loss = 2.85828 avg_loss = 2.49257\n",
            "epoch no.0 train no.61080  loss = 2.34209 avg_loss = 2.47994\n",
            "epoch no.0 train no.61090  loss = 2.46782 avg_loss = 2.45536\n",
            "epoch no.0 train no.61100  loss = 2.67171 avg_loss = 2.47645\n",
            "epoch no.0 train no.61110  loss = 2.64886 avg_loss = 2.47442\n",
            "epoch no.0 train no.61120  loss = 2.39855 avg_loss = 2.44850\n",
            "epoch no.0 train no.61130  loss = 3.08820 avg_loss = 2.45117\n",
            "epoch no.0 train no.61140  loss = 2.19582 avg_loss = 2.45269\n",
            "epoch no.0 train no.61150  loss = 1.67563 avg_loss = 2.44794\n",
            "epoch no.0 train no.61160  loss = 2.15274 avg_loss = 2.42122\n",
            "epoch no.0 train no.61170  loss = 2.31203 avg_loss = 2.43122\n",
            "epoch no.0 train no.61180  loss = 1.86897 avg_loss = 2.39267\n",
            "epoch no.0 train no.61190  loss = 1.80464 avg_loss = 2.38302\n",
            "epoch no.0 train no.61200  loss = 2.67467 avg_loss = 2.36952\n",
            "epoch no.0 train no.61210  loss = 2.38971 avg_loss = 2.39226\n",
            "epoch no.0 train no.61220  loss = 2.10664 avg_loss = 2.42463\n",
            "epoch no.0 train no.61230  loss = 2.55954 avg_loss = 2.40650\n",
            "epoch no.0 train no.61240  loss = 2.31238 avg_loss = 2.40827\n",
            "epoch no.0 train no.61250  loss = 2.88092 avg_loss = 2.43077\n",
            "epoch no.0 train no.61260  loss = 1.68543 avg_loss = 2.42436\n",
            "epoch no.0 train no.61270  loss = 2.23814 avg_loss = 2.45903\n",
            "epoch no.0 train no.61280  loss = 3.24263 avg_loss = 2.45301\n",
            "epoch no.0 train no.61290  loss = 2.96399 avg_loss = 2.44318\n",
            "epoch no.0 train no.61300  loss = 3.20649 avg_loss = 2.45285\n",
            "epoch no.0 train no.61310  loss = 2.65952 avg_loss = 2.46683\n",
            "epoch no.0 train no.61320  loss = 3.39312 avg_loss = 2.47281\n",
            "epoch no.0 train no.61330  loss = 2.40269 avg_loss = 2.46342\n",
            "epoch no.0 train no.61340  loss = 2.19021 avg_loss = 2.49819\n",
            "epoch no.0 train no.61350  loss = 2.14032 avg_loss = 2.55720\n",
            "epoch no.0 train no.61360  loss = 2.31296 avg_loss = 2.54530\n",
            "epoch no.0 train no.61370  loss = 2.31327 avg_loss = 2.57451\n",
            "epoch no.0 train no.61380  loss = 2.70259 avg_loss = 2.55888\n",
            "epoch no.0 train no.61390  loss = 2.29647 avg_loss = 2.57764\n",
            "epoch no.0 train no.61400  loss = 2.59647 avg_loss = 2.58685\n",
            "epoch no.0 train no.61410  loss = 2.49994 avg_loss = 2.59839\n",
            "epoch no.0 train no.61420  loss = 2.65980 avg_loss = 2.58023\n",
            "epoch no.0 train no.61430  loss = 2.26450 avg_loss = 2.54930\n",
            "epoch no.0 train no.61440  loss = 2.65430 avg_loss = 2.55936\n",
            "epoch no.0 train no.61450  loss = 2.60292 avg_loss = 2.55967\n",
            "epoch no.0 train no.61460  loss = 3.04925 avg_loss = 2.55265\n",
            "epoch no.0 train no.61470  loss = 2.75932 avg_loss = 2.57738\n",
            "epoch no.0 train no.61480  loss = 2.83167 avg_loss = 2.57299\n",
            "epoch no.0 train no.61490  loss = 2.01079 avg_loss = 2.56019\n",
            "epoch no.0 train no.61500  loss = 2.36158 avg_loss = 2.52462\n",
            "epoch no.0 train no.61510  loss = 2.64242 avg_loss = 2.52710\n",
            "epoch no.0 train no.61520  loss = 2.56602 avg_loss = 2.52534\n",
            "epoch no.0 train no.61530  loss = 2.51493 avg_loss = 2.51827\n",
            "epoch no.0 train no.61540  loss = 2.67485 avg_loss = 2.52467\n",
            "epoch no.0 train no.61550  loss = 2.18098 avg_loss = 2.52221\n",
            "epoch no.0 train no.61560  loss = 3.45844 avg_loss = 2.50745\n",
            "epoch no.0 train no.61570  loss = 2.26089 avg_loss = 2.51630\n",
            "epoch no.0 train no.61580  loss = 2.94363 avg_loss = 2.53066\n",
            "epoch no.0 train no.61590  loss = 1.76580 avg_loss = 2.52067\n",
            "epoch no.0 train no.61600  loss = 1.71705 avg_loss = 2.50129\n",
            "epoch no.0 train no.61610  loss = 2.50190 avg_loss = 2.51166\n",
            "epoch no.0 train no.61620  loss = 4.14044 avg_loss = 2.52848\n",
            "epoch no.0 train no.61630  loss = 2.89937 avg_loss = 2.52291\n",
            "epoch no.0 train no.61640  loss = 2.18360 avg_loss = 2.50179\n",
            "epoch no.0 train no.61650  loss = 2.05045 avg_loss = 2.49895\n",
            "epoch no.0 train no.61660  loss = 1.63018 avg_loss = 2.50809\n",
            "epoch no.0 train no.61670  loss = 2.36898 avg_loss = 2.51030\n",
            "epoch no.0 train no.61680  loss = 2.07178 avg_loss = 2.49971\n",
            "epoch no.0 train no.61690  loss = 2.41942 avg_loss = 2.50985\n",
            "epoch no.0 train no.61700  loss = 1.71561 avg_loss = 2.50643\n",
            "epoch no.0 train no.61710  loss = 2.83896 avg_loss = 2.52622\n",
            "epoch no.0 train no.61720  loss = 2.74486 avg_loss = 2.52701\n",
            "epoch no.0 train no.61730  loss = 3.22825 avg_loss = 2.51360\n",
            "epoch no.0 train no.61740  loss = 2.16366 avg_loss = 2.49613\n",
            "epoch no.0 train no.61750  loss = 2.57855 avg_loss = 2.49569\n",
            "epoch no.0 train no.61760  loss = 3.52545 avg_loss = 2.49935\n",
            "epoch no.0 train no.61770  loss = 2.68126 avg_loss = 2.51566\n",
            "epoch no.0 train no.61780  loss = 2.37335 avg_loss = 2.51450\n",
            "epoch no.0 train no.61790  loss = 2.93967 avg_loss = 2.52115\n",
            "epoch no.0 train no.61800  loss = 2.65420 avg_loss = 2.51037\n",
            "epoch no.0 train no.61810  loss = 2.83015 avg_loss = 2.49800\n",
            "epoch no.0 train no.61820  loss = 2.77449 avg_loss = 2.49535\n",
            "epoch no.0 train no.61830  loss = 2.81793 avg_loss = 2.50074\n",
            "epoch no.0 train no.61840  loss = 2.83431 avg_loss = 2.50321\n",
            "epoch no.0 train no.61850  loss = 2.63856 avg_loss = 2.48537\n",
            "epoch no.0 train no.61860  loss = 2.42871 avg_loss = 2.45169\n",
            "epoch no.0 train no.61870  loss = 2.95297 avg_loss = 2.46167\n",
            "epoch no.0 train no.61880  loss = 2.63273 avg_loss = 2.45541\n",
            "epoch no.0 train no.61890  loss = 3.20211 avg_loss = 2.46845\n",
            "epoch no.0 train no.61900  loss = 2.22376 avg_loss = 2.47131\n",
            "epoch no.0 train no.61910  loss = 2.87844 avg_loss = 2.50413\n",
            "epoch no.0 train no.61920  loss = 3.15173 avg_loss = 2.51670\n",
            "epoch no.0 train no.61930  loss = 3.06768 avg_loss = 2.51336\n",
            "epoch no.0 train no.61940  loss = 1.68752 avg_loss = 2.50863\n",
            "epoch no.0 train no.61950  loss = 2.32653 avg_loss = 2.47215\n",
            "epoch no.0 train no.61960  loss = 3.14277 avg_loss = 2.49199\n",
            "epoch no.0 train no.61970  loss = 2.28654 avg_loss = 2.50359\n",
            "epoch no.0 train no.61980  loss = 2.46927 avg_loss = 2.50594\n",
            "epoch no.0 train no.61990  loss = 1.29522 avg_loss = 2.47535\n",
            "epoch no.0 train no.62000  loss = 2.64982 avg_loss = 2.48863\n",
            "epoch no.0 train no.62010  loss = 2.53687 avg_loss = 2.46776\n",
            "epoch no.0 train no.62020  loss = 1.05336 avg_loss = 2.45132\n",
            "epoch no.0 train no.62030  loss = 2.62074 avg_loss = 2.42122\n",
            "epoch no.0 train no.62040  loss = 3.14826 avg_loss = 2.45276\n",
            "epoch no.0 train no.62050  loss = 2.50098 avg_loss = 2.45680\n",
            "epoch no.0 train no.62060  loss = 3.26590 avg_loss = 2.46860\n",
            "epoch no.0 train no.62070  loss = 2.51923 avg_loss = 2.46350\n",
            "epoch no.0 train no.62080  loss = 2.72103 avg_loss = 2.45759\n",
            "epoch no.0 train no.62090  loss = 2.28663 avg_loss = 2.46698\n",
            "epoch no.0 train no.62100  loss = 1.55672 avg_loss = 2.48038\n",
            "epoch no.0 train no.62110  loss = 2.82296 avg_loss = 2.47687\n",
            "epoch no.0 train no.62120  loss = 2.12873 avg_loss = 2.45565\n",
            "epoch no.0 train no.62130  loss = 1.96183 avg_loss = 2.42915\n",
            "epoch no.0 train no.62140  loss = 2.02299 avg_loss = 2.40313\n",
            "epoch no.0 train no.62150  loss = 2.92599 avg_loss = 2.39910\n",
            "epoch no.0 train no.62160  loss = 2.27453 avg_loss = 2.42832\n",
            "epoch no.0 train no.62170  loss = 2.24273 avg_loss = 2.41985\n",
            "epoch no.0 train no.62180  loss = 1.88067 avg_loss = 2.43113\n",
            "epoch no.0 train no.62190  loss = 3.07949 avg_loss = 2.44410\n",
            "epoch no.0 train no.62200  loss = 1.99985 avg_loss = 2.44061\n",
            "epoch no.0 train no.62210  loss = 2.90412 avg_loss = 2.44813\n",
            "epoch no.0 train no.62220  loss = 2.44008 avg_loss = 2.43525\n",
            "epoch no.0 train no.62230  loss = 2.72438 avg_loss = 2.43246\n",
            "epoch no.0 train no.62240  loss = 1.80257 avg_loss = 2.42823\n",
            "epoch no.0 train no.62250  loss = 2.62925 avg_loss = 2.41403\n",
            "epoch no.0 train no.62260  loss = 2.16666 avg_loss = 2.41776\n",
            "epoch no.0 train no.62270  loss = 1.95750 avg_loss = 2.42762\n",
            "epoch no.0 train no.62280  loss = 2.00237 avg_loss = 2.41597\n",
            "epoch no.0 train no.62290  loss = 2.58098 avg_loss = 2.41221\n",
            "epoch no.0 train no.62300  loss = 2.28812 avg_loss = 2.41553\n",
            "epoch no.0 train no.62310  loss = 2.29138 avg_loss = 2.43520\n",
            "epoch no.0 train no.62320  loss = 2.47169 avg_loss = 2.43331\n",
            "epoch no.0 train no.62330  loss = 2.39161 avg_loss = 2.44063\n",
            "epoch no.0 train no.62340  loss = 2.29192 avg_loss = 2.46472\n",
            "epoch no.0 train no.62350  loss = 3.70465 avg_loss = 2.47720\n",
            "epoch no.0 train no.62360  loss = 1.96672 avg_loss = 2.45465\n",
            "epoch no.0 train no.62370  loss = 2.59242 avg_loss = 2.44268\n",
            "epoch no.0 train no.62380  loss = 2.46275 avg_loss = 2.45795\n",
            "epoch no.0 train no.62390  loss = 3.41007 avg_loss = 2.44807\n",
            "epoch no.0 train no.62400  loss = 1.99876 avg_loss = 2.45092\n",
            "epoch no.0 train no.62410  loss = 2.18902 avg_loss = 2.43616\n",
            "epoch no.0 train no.62420  loss = 2.99063 avg_loss = 2.43851\n",
            "epoch no.0 train no.62430  loss = 3.14330 avg_loss = 2.44585\n",
            "epoch no.0 train no.62440  loss = 3.06778 avg_loss = 2.48840\n",
            "epoch no.0 train no.62450  loss = 2.11880 avg_loss = 2.48195\n",
            "epoch no.0 train no.62460  loss = 2.80193 avg_loss = 2.47551\n",
            "epoch no.0 train no.62470  loss = 1.85833 avg_loss = 2.45935\n",
            "epoch no.0 train no.62480  loss = 2.28009 avg_loss = 2.47792\n",
            "epoch no.0 train no.62490  loss = 1.82760 avg_loss = 2.46470\n",
            "epoch no.0 train no.62500  loss = 2.37078 avg_loss = 2.47770\n",
            "epoch no.0 train no.62510  loss = 2.68632 avg_loss = 2.49022\n",
            "epoch no.0 train no.62520  loss = 2.43457 avg_loss = 2.47627\n",
            "epoch no.0 train no.62530  loss = 2.31637 avg_loss = 2.47550\n",
            "epoch no.0 train no.62540  loss = 2.46712 avg_loss = 2.48112\n",
            "epoch no.0 train no.62550  loss = 1.39205 avg_loss = 2.46838\n",
            "epoch no.0 train no.62560  loss = 3.58178 avg_loss = 2.47070\n",
            "epoch no.0 train no.62570  loss = 2.30356 avg_loss = 2.47797\n",
            "epoch no.0 train no.62580  loss = 2.33254 avg_loss = 2.48955\n",
            "epoch no.0 train no.62590  loss = 2.88681 avg_loss = 2.50553\n",
            "epoch no.0 train no.62600  loss = 2.84404 avg_loss = 2.53375\n",
            "epoch no.0 train no.62610  loss = 2.87801 avg_loss = 2.54795\n",
            "epoch no.0 train no.62620  loss = 2.18840 avg_loss = 2.55494\n",
            "epoch no.0 train no.62630  loss = 2.65313 avg_loss = 2.56332\n",
            "epoch no.0 train no.62640  loss = 2.97908 avg_loss = 2.57913\n",
            "epoch no.0 train no.62650  loss = 2.29979 avg_loss = 2.55430\n",
            "epoch no.0 train no.62660  loss = 3.62576 avg_loss = 2.57191\n",
            "epoch no.0 train no.62670  loss = 1.96387 avg_loss = 2.55545\n",
            "epoch no.0 train no.62680  loss = 2.13796 avg_loss = 2.54534\n",
            "epoch no.0 train no.62690  loss = 2.55150 avg_loss = 2.53188\n",
            "epoch no.0 train no.62700  loss = 2.16919 avg_loss = 2.52526\n",
            "epoch no.0 train no.62710  loss = 2.28751 avg_loss = 2.52164\n",
            "epoch no.0 train no.62720  loss = 2.60440 avg_loss = 2.53661\n",
            "epoch no.0 train no.62730  loss = 2.11753 avg_loss = 2.52064\n",
            "epoch no.0 train no.62740  loss = 2.39002 avg_loss = 2.50886\n",
            "epoch no.0 train no.62750  loss = 2.97251 avg_loss = 2.52828\n",
            "epoch no.0 train no.62760  loss = 2.57206 avg_loss = 2.55615\n",
            "epoch no.0 train no.62770  loss = 1.69238 avg_loss = 2.52654\n",
            "epoch no.0 train no.62780  loss = 2.31598 avg_loss = 2.49925\n",
            "epoch no.0 train no.62790  loss = 2.04382 avg_loss = 2.49007\n",
            "epoch no.0 train no.62800  loss = 2.56498 avg_loss = 2.48751\n",
            "epoch no.0 train no.62810  loss = 2.60062 avg_loss = 2.47332\n",
            "epoch no.0 train no.62820  loss = 3.49757 avg_loss = 2.50669\n",
            "epoch no.0 train no.62830  loss = 2.96274 avg_loss = 2.50151\n",
            "epoch no.0 train no.62840  loss = 1.67213 avg_loss = 2.51564\n",
            "epoch no.0 train no.62850  loss = 1.99802 avg_loss = 2.46837\n",
            "epoch no.0 train no.62860  loss = 2.70126 avg_loss = 2.46069\n",
            "epoch no.0 train no.62870  loss = 2.97175 avg_loss = 2.46599\n",
            "epoch no.0 train no.62880  loss = 2.30951 avg_loss = 2.48108\n",
            "epoch no.0 train no.62890  loss = 3.27761 avg_loss = 2.47243\n",
            "epoch no.0 train no.62900  loss = 2.80761 avg_loss = 2.48072\n",
            "epoch no.0 train no.62910  loss = 2.97037 avg_loss = 2.47355\n",
            "epoch no.0 train no.62920  loss = 1.77175 avg_loss = 2.46919\n",
            "epoch no.0 train no.62930  loss = 2.97421 avg_loss = 2.48182\n",
            "epoch no.0 train no.62940  loss = 2.45974 avg_loss = 2.49251\n",
            "epoch no.0 train no.62950  loss = 2.28801 avg_loss = 2.49338\n",
            "epoch no.0 train no.62960  loss = 2.13974 avg_loss = 2.48741\n",
            "epoch no.0 train no.62970  loss = 1.88460 avg_loss = 2.49377\n",
            "epoch no.0 train no.62980  loss = 2.40917 avg_loss = 2.48144\n",
            "epoch no.0 train no.62990  loss = 2.50547 avg_loss = 2.45954\n",
            "epoch no.0 train no.63000  loss = 2.44662 avg_loss = 2.46784\n",
            "epoch no.0 train no.63010  loss = 3.15049 avg_loss = 2.49387\n",
            "epoch no.0 train no.63020  loss = 2.72593 avg_loss = 2.47281\n",
            "epoch no.0 train no.63030  loss = 2.18153 avg_loss = 2.48054\n",
            "epoch no.0 train no.63040  loss = 2.43138 avg_loss = 2.46522\n",
            "epoch no.0 train no.63050  loss = 2.52701 avg_loss = 2.47456\n",
            "epoch no.0 train no.63060  loss = 2.91348 avg_loss = 2.46963\n",
            "epoch no.0 train no.63070  loss = 2.74270 avg_loss = 2.47605\n",
            "epoch no.0 train no.63080  loss = 2.62396 avg_loss = 2.46823\n",
            "epoch no.0 train no.63090  loss = 3.01991 avg_loss = 2.48909\n",
            "epoch no.0 train no.63100  loss = 3.03142 avg_loss = 2.47335\n",
            "epoch no.0 train no.63110  loss = 1.92502 avg_loss = 2.46585\n",
            "epoch no.0 train no.63120  loss = 2.74321 avg_loss = 2.45455\n",
            "epoch no.0 train no.63130  loss = 2.28651 avg_loss = 2.44667\n",
            "epoch no.0 train no.63140  loss = 2.93876 avg_loss = 2.42960\n",
            "epoch no.0 train no.63150  loss = 1.83316 avg_loss = 2.44137\n",
            "epoch no.0 train no.63160  loss = 2.38668 avg_loss = 2.43410\n",
            "epoch no.0 train no.63170  loss = 4.35497 avg_loss = 2.44761\n",
            "epoch no.0 train no.63180  loss = 1.94728 avg_loss = 2.42797\n",
            "epoch no.0 train no.63190  loss = 2.63467 avg_loss = 2.44576\n",
            "epoch no.0 train no.63200  loss = 1.69785 avg_loss = 2.45150\n",
            "epoch no.0 train no.63210  loss = 2.37851 avg_loss = 2.47002\n",
            "epoch no.0 train no.63220  loss = 3.25416 avg_loss = 2.48046\n",
            "epoch no.0 train no.63230  loss = 2.61917 avg_loss = 2.46504\n",
            "epoch no.0 train no.63240  loss = 3.47682 avg_loss = 2.48384\n",
            "epoch no.0 train no.63250  loss = 1.83727 avg_loss = 2.47226\n",
            "epoch no.0 train no.63260  loss = 2.74959 avg_loss = 2.46293\n",
            "epoch no.0 train no.63270  loss = 3.53553 avg_loss = 2.47687\n",
            "epoch no.0 train no.63280  loss = 2.41295 avg_loss = 2.48938\n",
            "epoch no.0 train no.63290  loss = 2.43831 avg_loss = 2.49015\n",
            "epoch no.0 train no.63300  loss = 1.49586 avg_loss = 2.47791\n",
            "epoch no.0 train no.63310  loss = 2.94858 avg_loss = 2.49664\n",
            "epoch no.0 train no.63320  loss = 2.51517 avg_loss = 2.48954\n",
            "epoch no.0 train no.63330  loss = 2.23959 avg_loss = 2.49587\n",
            "epoch no.0 train no.63340  loss = 2.24409 avg_loss = 2.47168\n",
            "epoch no.0 train no.63350  loss = 2.69110 avg_loss = 2.46630\n",
            "epoch no.0 train no.63360  loss = 3.16491 avg_loss = 2.46718\n",
            "epoch no.0 train no.63370  loss = 2.56335 avg_loss = 2.45757\n",
            "epoch no.0 train no.63380  loss = 1.91658 avg_loss = 2.46151\n",
            "epoch no.0 train no.63390  loss = 2.08748 avg_loss = 2.43298\n",
            "epoch no.0 train no.63400  loss = 2.94201 avg_loss = 2.44474\n",
            "epoch no.0 train no.63410  loss = 2.91663 avg_loss = 2.43241\n",
            "epoch no.0 train no.63420  loss = 2.01146 avg_loss = 2.40735\n",
            "epoch no.0 train no.63430  loss = 3.31045 avg_loss = 2.42892\n",
            "epoch no.0 train no.63440  loss = 2.43124 avg_loss = 2.45828\n",
            "epoch no.0 train no.63450  loss = 2.63218 avg_loss = 2.47205\n",
            "epoch no.0 train no.63460  loss = 2.95512 avg_loss = 2.44974\n",
            "epoch no.0 train no.63470  loss = 2.46195 avg_loss = 2.45481\n",
            "epoch no.0 train no.63480  loss = 2.56472 avg_loss = 2.42910\n",
            "epoch no.0 train no.63490  loss = 2.60046 avg_loss = 2.44184\n",
            "epoch no.0 train no.63500  loss = 2.87238 avg_loss = 2.43488\n",
            "epoch no.0 train no.63510  loss = 3.05040 avg_loss = 2.44627\n",
            "epoch no.0 train no.63520  loss = 2.50894 avg_loss = 2.43572\n",
            "epoch no.0 train no.63530  loss = 2.84070 avg_loss = 2.43437\n",
            "epoch no.0 train no.63540  loss = 2.78366 avg_loss = 2.43829\n",
            "epoch no.0 train no.63550  loss = 1.94779 avg_loss = 2.44566\n",
            "epoch no.0 train no.63560  loss = 1.86624 avg_loss = 2.42625\n",
            "epoch no.0 train no.63570  loss = 3.21774 avg_loss = 2.47814\n",
            "epoch no.0 train no.63580  loss = 2.52456 avg_loss = 2.47218\n",
            "epoch no.0 train no.63590  loss = 3.11022 avg_loss = 2.46815\n",
            "epoch no.0 train no.63600  loss = 2.28183 avg_loss = 2.47435\n",
            "epoch no.0 train no.63610  loss = 1.70475 avg_loss = 2.46501\n",
            "epoch no.0 train no.63620  loss = 2.44393 avg_loss = 2.47538\n",
            "epoch no.0 train no.63630  loss = 2.10870 avg_loss = 2.46355\n",
            "epoch no.0 train no.63640  loss = 2.64993 avg_loss = 2.44517\n",
            "epoch no.0 train no.63650  loss = 1.90836 avg_loss = 2.41874\n",
            "epoch no.0 train no.63660  loss = 3.21914 avg_loss = 2.44152\n",
            "epoch no.0 train no.63670  loss = 2.27204 avg_loss = 2.45463\n",
            "epoch no.0 train no.63680  loss = 3.17827 avg_loss = 2.45921\n",
            "epoch no.0 train no.63690  loss = 1.66512 avg_loss = 2.44550\n",
            "epoch no.0 train no.63700  loss = 2.26529 avg_loss = 2.44769\n",
            "epoch no.0 train no.63710  loss = 1.74186 avg_loss = 2.43342\n",
            "epoch no.0 train no.63720  loss = 2.15078 avg_loss = 2.45392\n",
            "epoch no.0 train no.63730  loss = 2.27696 avg_loss = 2.46299\n",
            "epoch no.0 train no.63740  loss = 2.39842 avg_loss = 2.46277\n",
            "epoch no.0 train no.63750  loss = 2.26025 avg_loss = 2.44724\n",
            "epoch no.0 train no.63760  loss = 3.42560 avg_loss = 2.45106\n",
            "epoch no.0 train no.63770  loss = 2.80235 avg_loss = 2.45815\n",
            "epoch no.0 train no.63780  loss = 2.02976 avg_loss = 2.45576\n",
            "epoch no.0 train no.63790  loss = 1.82514 avg_loss = 2.44497\n",
            "epoch no.0 train no.63800  loss = 2.54673 avg_loss = 2.44056\n",
            "epoch no.0 train no.63810  loss = 2.64269 avg_loss = 2.45064\n",
            "epoch no.0 train no.63820  loss = 1.91417 avg_loss = 2.44222\n",
            "epoch no.0 train no.63830  loss = 2.69374 avg_loss = 2.43337\n",
            "epoch no.0 train no.63840  loss = 1.96153 avg_loss = 2.45701\n",
            "epoch no.0 train no.63850  loss = 3.52356 avg_loss = 2.45947\n",
            "epoch no.0 train no.63860  loss = 2.89375 avg_loss = 2.47932\n",
            "epoch no.0 train no.63870  loss = 2.50841 avg_loss = 2.46324\n",
            "epoch no.0 train no.63880  loss = 2.52186 avg_loss = 2.46683\n",
            "epoch no.0 train no.63890  loss = 2.82475 avg_loss = 2.47542\n",
            "epoch no.0 train no.63900  loss = 3.32195 avg_loss = 2.47521\n",
            "epoch no.0 train no.63910  loss = 2.39013 avg_loss = 2.45858\n",
            "epoch no.0 train no.63920  loss = 2.79115 avg_loss = 2.47432\n",
            "epoch no.0 train no.63930  loss = 2.51429 avg_loss = 2.48872\n",
            "epoch no.0 train no.63940  loss = 2.33395 avg_loss = 2.46188\n",
            "epoch no.0 train no.63950  loss = 2.39976 avg_loss = 2.48844\n",
            "epoch no.0 train no.63960  loss = 1.58191 avg_loss = 2.47950\n",
            "epoch no.0 train no.63970  loss = 1.80947 avg_loss = 2.47150\n",
            "epoch no.0 train no.63980  loss = 1.55329 avg_loss = 2.47297\n",
            "epoch no.0 train no.63990  loss = 1.90011 avg_loss = 2.46322\n",
            "epoch no.0 train no.64000  loss = 2.37387 avg_loss = 2.45746\n",
            "epoch no.0 train no.64010  loss = 3.04284 avg_loss = 2.44254\n",
            "epoch no.0 train no.64020  loss = 2.48549 avg_loss = 2.45332\n",
            "epoch no.0 train no.64030  loss = 3.41195 avg_loss = 2.46534\n",
            "epoch no.0 train no.64040  loss = 2.88197 avg_loss = 2.45916\n",
            "epoch no.0 train no.64050  loss = 2.93944 avg_loss = 2.47044\n",
            "epoch no.0 train no.64060  loss = 2.84814 avg_loss = 2.46327\n",
            "epoch no.0 train no.64070  loss = 3.51394 avg_loss = 2.45676\n",
            "epoch no.0 train no.64080  loss = 2.07355 avg_loss = 2.45190\n",
            "epoch no.0 train no.64090  loss = 2.95658 avg_loss = 2.46818\n",
            "epoch no.0 train no.64100  loss = 2.97649 avg_loss = 2.46705\n",
            "epoch no.0 train no.64110  loss = 1.01156 avg_loss = 2.47384\n",
            "epoch no.0 train no.64120  loss = 3.10695 avg_loss = 2.49612\n",
            "epoch no.0 train no.64130  loss = 2.10379 avg_loss = 2.49569\n",
            "epoch no.0 train no.64140  loss = 2.26978 avg_loss = 2.51596\n",
            "epoch no.0 train no.64150  loss = 2.38061 avg_loss = 2.51385\n",
            "epoch no.0 train no.64160  loss = 3.02346 avg_loss = 2.52491\n",
            "epoch no.0 train no.64170  loss = 2.66408 avg_loss = 2.51703\n",
            "epoch no.0 train no.64180  loss = 2.41932 avg_loss = 2.51290\n",
            "epoch no.0 train no.64190  loss = 2.44416 avg_loss = 2.51033\n",
            "epoch no.0 train no.64200  loss = 2.07844 avg_loss = 2.52613\n",
            "epoch no.0 train no.64210  loss = 2.38255 avg_loss = 2.52449\n",
            "epoch no.0 train no.64220  loss = 2.25331 avg_loss = 2.53927\n",
            "epoch no.0 train no.64230  loss = 2.93720 avg_loss = 2.54812\n",
            "epoch no.0 train no.64240  loss = 2.69647 avg_loss = 2.54193\n",
            "epoch no.0 train no.64250  loss = 2.60788 avg_loss = 2.53761\n",
            "epoch no.0 train no.64260  loss = 2.17483 avg_loss = 2.54812\n",
            "epoch no.0 train no.64270  loss = 2.87100 avg_loss = 2.56264\n",
            "epoch no.0 train no.64280  loss = 2.36032 avg_loss = 2.55667\n",
            "epoch no.0 train no.64290  loss = 3.41943 avg_loss = 2.55371\n",
            "epoch no.0 train no.64300  loss = 2.64961 avg_loss = 2.55777\n",
            "epoch no.0 train no.64310  loss = 2.17150 avg_loss = 2.55022\n",
            "epoch no.0 train no.64320  loss = 3.61533 avg_loss = 2.55288\n",
            "epoch no.0 train no.64330  loss = 2.92898 avg_loss = 2.55295\n",
            "epoch no.0 train no.64340  loss = 2.25194 avg_loss = 2.53414\n",
            "epoch no.0 train no.64350  loss = 2.30241 avg_loss = 2.51606\n",
            "epoch no.0 train no.64360  loss = 2.07555 avg_loss = 2.51501\n",
            "epoch no.0 train no.64370  loss = 2.57340 avg_loss = 2.51795\n",
            "epoch no.0 train no.64380  loss = 2.87531 avg_loss = 2.49861\n",
            "epoch no.0 train no.64390  loss = 2.51662 avg_loss = 2.48422\n",
            "epoch no.0 train no.64400  loss = 2.94858 avg_loss = 2.47938\n",
            "epoch no.0 train no.64410  loss = 2.05211 avg_loss = 2.46387\n",
            "epoch no.0 train no.64420  loss = 2.88594 avg_loss = 2.48959\n",
            "epoch no.0 train no.64430  loss = 2.55429 avg_loss = 2.48809\n",
            "epoch no.0 train no.64440  loss = 1.95396 avg_loss = 2.49465\n",
            "epoch no.0 train no.64450  loss = 2.63197 avg_loss = 2.49685\n",
            "epoch no.0 train no.64460  loss = 1.88491 avg_loss = 2.50986\n",
            "epoch no.0 train no.64470  loss = 2.12523 avg_loss = 2.50531\n",
            "epoch no.0 train no.64480  loss = 3.15032 avg_loss = 2.50714\n",
            "epoch no.0 train no.64490  loss = 2.64648 avg_loss = 2.51520\n",
            "epoch no.0 train no.64500  loss = 1.63388 avg_loss = 2.51518\n",
            "epoch no.0 train no.64510  loss = 2.26283 avg_loss = 2.50647\n",
            "epoch no.0 train no.64520  loss = 2.75592 avg_loss = 2.50681\n",
            "epoch no.0 train no.64530  loss = 2.14662 avg_loss = 2.51203\n",
            "epoch no.0 train no.64540  loss = 2.21515 avg_loss = 2.48301\n",
            "epoch no.0 train no.64550  loss = 2.52593 avg_loss = 2.49690\n",
            "epoch no.0 train no.64560  loss = 2.33567 avg_loss = 2.47433\n",
            "epoch no.0 train no.64570  loss = 1.91903 avg_loss = 2.45620\n",
            "epoch no.0 train no.64580  loss = 2.46335 avg_loss = 2.46201\n",
            "epoch no.0 train no.64590  loss = 2.88113 avg_loss = 2.47090\n",
            "epoch no.0 train no.64600  loss = 2.26033 avg_loss = 2.48632\n",
            "epoch no.0 train no.64610  loss = 2.67518 avg_loss = 2.48632\n",
            "epoch no.0 train no.64620  loss = 2.55220 avg_loss = 2.47333\n",
            "epoch no.0 train no.64630  loss = 2.30367 avg_loss = 2.47339\n",
            "epoch no.0 train no.64640  loss = 2.47719 avg_loss = 2.48121\n",
            "epoch no.0 train no.64650  loss = 2.67702 avg_loss = 2.47469\n",
            "epoch no.0 train no.64660  loss = 2.73161 avg_loss = 2.47614\n",
            "epoch no.0 train no.64670  loss = 2.72863 avg_loss = 2.49096\n",
            "epoch no.0 train no.64680  loss = 2.85968 avg_loss = 2.50010\n",
            "epoch no.0 train no.64690  loss = 2.73861 avg_loss = 2.48901\n",
            "epoch no.0 train no.64700  loss = 2.46558 avg_loss = 2.48046\n",
            "epoch no.0 train no.64710  loss = 2.65088 avg_loss = 2.49407\n",
            "epoch no.0 train no.64720  loss = 3.15251 avg_loss = 2.47929\n",
            "epoch no.0 train no.64730  loss = 1.26715 avg_loss = 2.44515\n",
            "epoch no.0 train no.64740  loss = 2.09369 avg_loss = 2.42343\n",
            "epoch no.0 train no.64750  loss = 2.40647 avg_loss = 2.42534\n",
            "epoch no.0 train no.64760  loss = 2.18675 avg_loss = 2.41422\n",
            "epoch no.0 train no.64770  loss = 2.36118 avg_loss = 2.42688\n",
            "epoch no.0 train no.64780  loss = 2.59615 avg_loss = 2.45227\n",
            "epoch no.0 train no.64790  loss = 3.00989 avg_loss = 2.47501\n",
            "epoch no.0 train no.64800  loss = 3.62951 avg_loss = 2.51851\n",
            "epoch no.0 train no.64810  loss = 2.38029 avg_loss = 2.51953\n",
            "epoch no.0 train no.64820  loss = 3.09810 avg_loss = 2.53828\n",
            "epoch no.0 train no.64830  loss = 2.17032 avg_loss = 2.53549\n",
            "epoch no.0 train no.64840  loss = 2.52230 avg_loss = 2.57074\n",
            "epoch no.0 train no.64850  loss = 2.25889 avg_loss = 2.57556\n",
            "epoch no.0 train no.64860  loss = 2.25193 avg_loss = 2.56341\n",
            "epoch no.0 train no.64870  loss = 2.99305 avg_loss = 2.55994\n",
            "epoch no.0 train no.64880  loss = 1.98180 avg_loss = 2.55295\n",
            "epoch no.0 train no.64890  loss = 1.95071 avg_loss = 2.53135\n",
            "epoch no.0 train no.64900  loss = 2.40145 avg_loss = 2.51969\n",
            "epoch no.0 train no.64910  loss = 2.66678 avg_loss = 2.54821\n",
            "epoch no.0 train no.64920  loss = 2.76744 avg_loss = 2.54257\n",
            "epoch no.0 train no.64930  loss = 2.94424 avg_loss = 2.53890\n",
            "epoch no.0 train no.64940  loss = 2.36036 avg_loss = 2.54908\n",
            "epoch no.0 train no.64950  loss = 3.32500 avg_loss = 2.51371\n",
            "epoch no.0 train no.64960  loss = 2.28354 avg_loss = 2.50388\n",
            "epoch no.0 train no.64970  loss = 2.12868 avg_loss = 2.50748\n",
            "epoch no.0 train no.64980  loss = 2.76725 avg_loss = 2.50727\n",
            "epoch no.0 train no.64990  loss = 2.98155 avg_loss = 2.49939\n",
            "epoch no.0 train no.65000  loss = 2.72145 avg_loss = 2.49708\n",
            "epoch no.0 train no.65010  loss = 2.79591 avg_loss = 2.49638\n",
            "epoch no.0 train no.65020  loss = 2.96435 avg_loss = 2.49761\n",
            "epoch no.0 train no.65030  loss = 3.06452 avg_loss = 2.46417\n",
            "epoch no.0 train no.65040  loss = 1.95844 avg_loss = 2.46437\n",
            "epoch no.0 train no.65050  loss = 1.63377 avg_loss = 2.46208\n",
            "epoch no.0 train no.65060  loss = 2.08816 avg_loss = 2.45544\n",
            "epoch no.0 train no.65070  loss = 2.72628 avg_loss = 2.45966\n",
            "epoch no.0 train no.65080  loss = 2.91357 avg_loss = 2.46887\n",
            "epoch no.0 train no.65090  loss = 2.12891 avg_loss = 2.46279\n",
            "epoch no.0 train no.65100  loss = 2.57915 avg_loss = 2.48469\n",
            "epoch no.0 train no.65110  loss = 3.12920 avg_loss = 2.48174\n",
            "epoch no.0 train no.65120  loss = 1.99803 avg_loss = 2.46497\n",
            "epoch no.0 train no.65130  loss = 2.65040 avg_loss = 2.46572\n",
            "epoch no.0 train no.65140  loss = 2.50959 avg_loss = 2.47141\n",
            "epoch no.0 train no.65150  loss = 2.69993 avg_loss = 2.46780\n",
            "epoch no.0 train no.65160  loss = 1.98574 avg_loss = 2.45264\n",
            "epoch no.0 train no.65170  loss = 2.85665 avg_loss = 2.44879\n",
            "epoch no.0 train no.65180  loss = 1.67021 avg_loss = 2.45235\n",
            "epoch no.0 train no.65190  loss = 2.84060 avg_loss = 2.43110\n",
            "epoch no.0 train no.65200  loss = 2.89697 avg_loss = 2.44978\n",
            "epoch no.0 train no.65210  loss = 2.12789 avg_loss = 2.47869\n",
            "epoch no.0 train no.65220  loss = 2.07959 avg_loss = 2.48492\n",
            "epoch no.0 train no.65230  loss = 2.46895 avg_loss = 2.49116\n",
            "epoch no.0 train no.65240  loss = 2.50425 avg_loss = 2.49526\n",
            "epoch no.0 train no.65250  loss = 2.67418 avg_loss = 2.50547\n",
            "epoch no.0 train no.65260  loss = 2.53736 avg_loss = 2.51666\n",
            "epoch no.0 train no.65270  loss = 3.34249 avg_loss = 2.51375\n",
            "epoch no.0 train no.65280  loss = 2.86395 avg_loss = 2.50523\n",
            "epoch no.0 train no.65290  loss = 1.71484 avg_loss = 2.49898\n",
            "epoch no.0 train no.65300  loss = 3.38317 avg_loss = 2.51494\n",
            "epoch no.0 train no.65310  loss = 2.64400 avg_loss = 2.50038\n",
            "epoch no.0 train no.65320  loss = 2.44893 avg_loss = 2.48210\n",
            "epoch no.0 train no.65330  loss = 2.79155 avg_loss = 2.49469\n",
            "epoch no.0 train no.65340  loss = 2.80623 avg_loss = 2.49514\n",
            "epoch no.0 train no.65350  loss = 2.05423 avg_loss = 2.48590\n",
            "epoch no.0 train no.65360  loss = 2.18848 avg_loss = 2.48241\n",
            "epoch no.0 train no.65370  loss = 2.05784 avg_loss = 2.47778\n",
            "epoch no.0 train no.65380  loss = 3.13412 avg_loss = 2.45773\n",
            "epoch no.0 train no.65390  loss = 2.42687 avg_loss = 2.46724\n",
            "epoch no.0 train no.65400  loss = 2.74765 avg_loss = 2.46938\n",
            "epoch no.0 train no.65410  loss = 2.91710 avg_loss = 2.51725\n",
            "epoch no.0 train no.65420  loss = 2.61959 avg_loss = 2.51515\n",
            "epoch no.0 train no.65430  loss = 1.90599 avg_loss = 2.52509\n",
            "epoch no.0 train no.65440  loss = 2.51113 avg_loss = 2.50176\n",
            "epoch no.0 train no.65450  loss = 3.06235 avg_loss = 2.52697\n",
            "epoch no.0 train no.65460  loss = 2.23140 avg_loss = 2.52383\n",
            "epoch no.0 train no.65470  loss = 1.75376 avg_loss = 2.52115\n",
            "epoch no.0 train no.65480  loss = 2.83368 avg_loss = 2.52144\n",
            "epoch no.0 train no.65490  loss = 2.43762 avg_loss = 2.50816\n",
            "epoch no.0 train no.65500  loss = 2.48251 avg_loss = 2.50835\n",
            "epoch no.0 train no.65510  loss = 2.65627 avg_loss = 2.50451\n",
            "epoch no.0 train no.65520  loss = 1.83439 avg_loss = 2.49299\n",
            "epoch no.0 train no.65530  loss = 2.57929 avg_loss = 2.49130\n",
            "epoch no.0 train no.65540  loss = 2.52952 avg_loss = 2.52539\n",
            "epoch no.0 train no.65550  loss = 3.17367 avg_loss = 2.50318\n",
            "epoch no.0 train no.65560  loss = 1.73816 avg_loss = 2.49655\n",
            "epoch no.0 train no.65570  loss = 2.34662 avg_loss = 2.49784\n",
            "epoch no.0 train no.65580  loss = 2.25988 avg_loss = 2.47971\n",
            "epoch no.0 train no.65590  loss = 1.78151 avg_loss = 2.48528\n",
            "epoch no.0 train no.65600  loss = 2.15495 avg_loss = 2.48717\n",
            "epoch no.0 train no.65610  loss = 2.55855 avg_loss = 2.48712\n",
            "epoch no.0 train no.65620  loss = 2.56722 avg_loss = 2.49058\n",
            "epoch no.0 train no.65630  loss = 2.00590 avg_loss = 2.48804\n",
            "epoch no.0 train no.65640  loss = 3.12817 avg_loss = 2.44960\n",
            "epoch no.0 train no.65650  loss = 2.89945 avg_loss = 2.44475\n",
            "epoch no.0 train no.65660  loss = 2.54887 avg_loss = 2.44224\n",
            "epoch no.0 train no.65670  loss = 2.07059 avg_loss = 2.41845\n",
            "epoch no.0 train no.65680  loss = 1.60492 avg_loss = 2.40808\n",
            "epoch no.0 train no.65690  loss = 2.70230 avg_loss = 2.42467\n",
            "epoch no.0 train no.65700  loss = 3.10738 avg_loss = 2.43108\n",
            "epoch no.0 train no.65710  loss = 2.24213 avg_loss = 2.40705\n",
            "epoch no.0 train no.65720  loss = 2.16492 avg_loss = 2.41403\n",
            "epoch no.0 train no.65730  loss = 2.94098 avg_loss = 2.44787\n",
            "epoch no.0 train no.65740  loss = 2.14557 avg_loss = 2.43436\n",
            "epoch no.0 train no.65750  loss = 2.90313 avg_loss = 2.45300\n",
            "epoch no.0 train no.65760  loss = 2.48949 avg_loss = 2.48220\n",
            "epoch no.0 train no.65770  loss = 2.43966 avg_loss = 2.51662\n",
            "epoch no.0 train no.65780  loss = 2.39115 avg_loss = 2.51538\n",
            "epoch no.0 train no.65790  loss = 2.36155 avg_loss = 2.51057\n",
            "epoch no.0 train no.65800  loss = 1.39986 avg_loss = 2.47966\n",
            "epoch no.0 train no.65810  loss = 1.32992 avg_loss = 2.45231\n",
            "epoch no.0 train no.65820  loss = 3.43808 avg_loss = 2.47356\n",
            "epoch no.0 train no.65830  loss = 3.45846 avg_loss = 2.47293\n",
            "epoch no.0 train no.65840  loss = 2.01136 avg_loss = 2.46859\n",
            "epoch no.0 train no.65850  loss = 2.80826 avg_loss = 2.49908\n",
            "epoch no.0 train no.65860  loss = 2.88810 avg_loss = 2.50640\n",
            "epoch no.0 train no.65870  loss = 2.41761 avg_loss = 2.50381\n",
            "epoch no.0 train no.65880  loss = 2.09552 avg_loss = 2.48668\n",
            "epoch no.0 train no.65890  loss = 2.99332 avg_loss = 2.47892\n",
            "epoch no.0 train no.65900  loss = 2.28194 avg_loss = 2.44413\n",
            "epoch no.0 train no.65910  loss = 2.61082 avg_loss = 2.43248\n",
            "epoch no.0 train no.65920  loss = 2.45520 avg_loss = 2.42497\n",
            "epoch no.0 train no.65930  loss = 2.74329 avg_loss = 2.42018\n",
            "epoch no.0 train no.65940  loss = 2.16466 avg_loss = 2.40017\n",
            "epoch no.0 train no.65950  loss = 2.43778 avg_loss = 2.40548\n",
            "epoch no.0 train no.65960  loss = 2.56146 avg_loss = 2.39954\n",
            "epoch no.0 train no.65970  loss = 2.38287 avg_loss = 2.43472\n",
            "epoch no.0 train no.65980  loss = 2.99399 avg_loss = 2.44347\n",
            "epoch no.0 train no.65990  loss = 2.35667 avg_loss = 2.45113\n",
            "epoch no.0 train no.66000  loss = 1.93667 avg_loss = 2.41231\n",
            "epoch no.0 train no.66010  loss = 3.23382 avg_loss = 2.41448\n",
            "epoch no.0 train no.66020  loss = 2.55752 avg_loss = 2.44476\n",
            "epoch no.0 train no.66030  loss = 3.05159 avg_loss = 2.44907\n",
            "epoch no.0 train no.66040  loss = 1.63460 avg_loss = 2.44697\n",
            "epoch no.0 train no.66050  loss = 1.58719 avg_loss = 2.44448\n",
            "epoch no.0 train no.66060  loss = 2.44348 avg_loss = 2.41607\n",
            "epoch no.0 train no.66070  loss = 3.12668 avg_loss = 2.44356\n",
            "epoch no.0 train no.66080  loss = 2.38521 avg_loss = 2.46741\n",
            "epoch no.0 train no.66090  loss = 2.56477 avg_loss = 2.48286\n",
            "epoch no.0 train no.66100  loss = 2.54849 avg_loss = 2.49785\n",
            "epoch no.0 train no.66110  loss = 3.45318 avg_loss = 2.48209\n",
            "epoch no.0 train no.66120  loss = 2.03225 avg_loss = 2.48080\n",
            "epoch no.0 train no.66130  loss = 2.49363 avg_loss = 2.47137\n",
            "epoch no.0 train no.66140  loss = 3.06204 avg_loss = 2.47311\n",
            "epoch no.0 train no.66150  loss = 2.77707 avg_loss = 2.47721\n",
            "epoch no.0 train no.66160  loss = 2.35066 avg_loss = 2.48572\n",
            "epoch no.0 train no.66170  loss = 2.62269 avg_loss = 2.49430\n",
            "epoch no.0 train no.66180  loss = 1.98595 avg_loss = 2.49207\n",
            "epoch no.0 train no.66190  loss = 2.83472 avg_loss = 2.52045\n",
            "epoch no.0 train no.66200  loss = 2.47037 avg_loss = 2.54523\n",
            "epoch no.0 train no.66210  loss = 3.00878 avg_loss = 2.54335\n",
            "epoch no.0 train no.66220  loss = 1.42850 avg_loss = 2.51892\n",
            "epoch no.0 train no.66230  loss = 1.85044 avg_loss = 2.50505\n",
            "epoch no.0 train no.66240  loss = 2.84149 avg_loss = 2.50105\n",
            "epoch no.0 train no.66250  loss = 1.58999 avg_loss = 2.51220\n",
            "epoch no.0 train no.66260  loss = 2.55599 avg_loss = 2.51832\n",
            "epoch no.0 train no.66270  loss = 3.13237 avg_loss = 2.53161\n",
            "epoch no.0 train no.66280  loss = 2.60909 avg_loss = 2.53216\n",
            "epoch no.0 train no.66290  loss = 1.97808 avg_loss = 2.51143\n",
            "epoch no.0 train no.66300  loss = 3.12683 avg_loss = 2.52300\n",
            "epoch no.0 train no.66310  loss = 2.48665 avg_loss = 2.52660\n",
            "epoch no.0 train no.66320  loss = 1.70091 avg_loss = 2.51663\n",
            "epoch no.0 train no.66330  loss = 2.22266 avg_loss = 2.49456\n",
            "epoch no.0 train no.66340  loss = 2.22635 avg_loss = 2.46619\n",
            "epoch no.0 train no.66350  loss = 1.38491 avg_loss = 2.47728\n",
            "epoch no.0 train no.66360  loss = 2.21373 avg_loss = 2.50133\n",
            "epoch no.0 train no.66370  loss = 2.70773 avg_loss = 2.47667\n",
            "epoch no.0 train no.66380  loss = 2.14750 avg_loss = 2.45994\n",
            "epoch no.0 train no.66390  loss = 2.26465 avg_loss = 2.43435\n",
            "epoch no.0 train no.66400  loss = 2.47058 avg_loss = 2.43267\n",
            "epoch no.0 train no.66410  loss = 2.57488 avg_loss = 2.44326\n",
            "epoch no.0 train no.66420  loss = 1.86808 avg_loss = 2.43604\n",
            "epoch no.0 train no.66430  loss = 2.68762 avg_loss = 2.45984\n",
            "epoch no.0 train no.66440  loss = 3.13718 avg_loss = 2.47606\n",
            "epoch no.0 train no.66450  loss = 2.22391 avg_loss = 2.49698\n",
            "epoch no.0 train no.66460  loss = 2.18984 avg_loss = 2.49405\n",
            "epoch no.0 train no.66470  loss = 2.66114 avg_loss = 2.49987\n",
            "epoch no.0 train no.66480  loss = 2.85296 avg_loss = 2.52880\n",
            "epoch no.0 train no.66490  loss = 2.37711 avg_loss = 2.51810\n",
            "epoch no.0 train no.66500  loss = 2.06633 avg_loss = 2.51611\n",
            "epoch no.0 train no.66510  loss = 3.10156 avg_loss = 2.50644\n",
            "epoch no.0 train no.66520  loss = 2.47081 avg_loss = 2.49665\n",
            "epoch no.0 train no.66530  loss = 2.76766 avg_loss = 2.50765\n",
            "epoch no.0 train no.66540  loss = 2.23050 avg_loss = 2.48173\n",
            "epoch no.0 train no.66550  loss = 3.05019 avg_loss = 2.46729\n",
            "epoch no.0 train no.66560  loss = 2.41958 avg_loss = 2.46151\n",
            "epoch no.0 train no.66570  loss = 3.54966 avg_loss = 2.49916\n",
            "epoch no.0 train no.66580  loss = 2.22458 avg_loss = 2.47499\n",
            "epoch no.0 train no.66590  loss = 3.35410 avg_loss = 2.51007\n",
            "epoch no.0 train no.66600  loss = 1.70868 avg_loss = 2.50988\n",
            "epoch no.0 train no.66610  loss = 1.93364 avg_loss = 2.50579\n",
            "epoch no.0 train no.66620  loss = 3.17694 avg_loss = 2.49612\n",
            "epoch no.0 train no.66630  loss = 2.78699 avg_loss = 2.46801\n",
            "epoch no.0 train no.66640  loss = 2.52141 avg_loss = 2.47217\n",
            "epoch no.0 train no.66650  loss = 2.82158 avg_loss = 2.48768\n",
            "epoch no.0 train no.66660  loss = 2.52700 avg_loss = 2.48621\n",
            "epoch no.0 train no.66670  loss = 2.37827 avg_loss = 2.47604\n",
            "epoch no.0 train no.66680  loss = 2.88467 avg_loss = 2.48769\n",
            "epoch no.0 train no.66690  loss = 3.47241 avg_loss = 2.49024\n",
            "epoch no.0 train no.66700  loss = 2.23752 avg_loss = 2.48878\n",
            "epoch no.0 train no.66710  loss = 2.45623 avg_loss = 2.48458\n",
            "epoch no.0 train no.66720  loss = 2.65951 avg_loss = 2.48302\n",
            "epoch no.0 train no.66730  loss = 2.54823 avg_loss = 2.50812\n",
            "epoch no.0 train no.66740  loss = 3.23957 avg_loss = 2.52115\n",
            "epoch no.0 train no.66750  loss = 2.26345 avg_loss = 2.52128\n",
            "epoch no.0 train no.66760  loss = 3.35522 avg_loss = 2.51266\n",
            "epoch no.0 train no.66770  loss = 3.22672 avg_loss = 2.52473\n",
            "epoch no.0 train no.66780  loss = 2.45514 avg_loss = 2.53152\n",
            "epoch no.0 train no.66790  loss = 3.05645 avg_loss = 2.52789\n",
            "epoch no.0 train no.66800  loss = 2.39269 avg_loss = 2.50548\n",
            "epoch no.0 train no.66810  loss = 1.60916 avg_loss = 2.48124\n",
            "epoch no.0 train no.66820  loss = 2.58250 avg_loss = 2.50023\n",
            "epoch no.0 train no.66830  loss = 2.48353 avg_loss = 2.49073\n",
            "epoch no.0 train no.66840  loss = 3.13718 avg_loss = 2.48629\n",
            "epoch no.0 train no.66850  loss = 2.63041 avg_loss = 2.52021\n",
            "epoch no.0 train no.66860  loss = 1.89396 avg_loss = 2.51755\n",
            "epoch no.0 train no.66870  loss = 2.24100 avg_loss = 2.49629\n",
            "epoch no.0 train no.66880  loss = 2.57006 avg_loss = 2.47071\n",
            "epoch no.0 train no.66890  loss = 2.92185 avg_loss = 2.47306\n",
            "epoch no.0 train no.66900  loss = 2.51891 avg_loss = 2.46446\n",
            "epoch no.0 train no.66910  loss = 3.26142 avg_loss = 2.47610\n",
            "epoch no.0 train no.66920  loss = 1.78078 avg_loss = 2.46804\n",
            "epoch no.0 train no.66930  loss = 2.64042 avg_loss = 2.46765\n",
            "epoch no.0 train no.66940  loss = 2.41387 avg_loss = 2.46746\n",
            "epoch no.0 train no.66950  loss = 2.63540 avg_loss = 2.45383\n",
            "epoch no.0 train no.66960  loss = 1.82323 avg_loss = 2.43709\n",
            "epoch no.0 train no.66970  loss = 2.79926 avg_loss = 2.46094\n",
            "epoch no.0 train no.66980  loss = 2.30953 avg_loss = 2.47208\n",
            "epoch no.0 train no.66990  loss = 3.15480 avg_loss = 2.50444\n",
            "epoch no.0 train no.67000  loss = 1.87164 avg_loss = 2.50265\n",
            "epoch no.0 train no.67010  loss = 1.95091 avg_loss = 2.50199\n",
            "epoch no.0 train no.67020  loss = 1.64075 avg_loss = 2.47060\n",
            "epoch no.0 train no.67030  loss = 2.59423 avg_loss = 2.47510\n",
            "epoch no.0 train no.67040  loss = 1.74047 avg_loss = 2.50962\n",
            "epoch no.0 train no.67050  loss = 3.08765 avg_loss = 2.55958\n",
            "epoch no.0 train no.67060  loss = 2.18650 avg_loss = 2.55328\n",
            "epoch no.0 train no.67070  loss = 2.21999 avg_loss = 2.55002\n",
            "epoch no.0 train no.67080  loss = 2.31726 avg_loss = 2.51805\n",
            "epoch no.0 train no.67090  loss = 2.60217 avg_loss = 2.50228\n",
            "epoch no.0 train no.67100  loss = 2.81197 avg_loss = 2.49842\n",
            "epoch no.0 train no.67110  loss = 2.06079 avg_loss = 2.48665\n",
            "epoch no.0 train no.67120  loss = 1.84214 avg_loss = 2.51021\n",
            "epoch no.0 train no.67130  loss = 2.19532 avg_loss = 2.49618\n",
            "epoch no.0 train no.67140  loss = 1.82944 avg_loss = 2.49358\n",
            "epoch no.0 train no.67150  loss = 2.62760 avg_loss = 2.50775\n",
            "epoch no.0 train no.67160  loss = 2.33196 avg_loss = 2.50315\n",
            "epoch no.0 train no.67170  loss = 3.16635 avg_loss = 2.51450\n",
            "epoch no.0 train no.67180  loss = 2.45209 avg_loss = 2.51748\n",
            "epoch no.0 train no.67190  loss = 2.06685 avg_loss = 2.51093\n",
            "epoch no.0 train no.67200  loss = 2.23376 avg_loss = 2.50764\n",
            "epoch no.0 train no.67210  loss = 2.76308 avg_loss = 2.51211\n",
            "epoch no.0 train no.67220  loss = 2.25470 avg_loss = 2.49437\n",
            "epoch no.0 train no.67230  loss = 2.15032 avg_loss = 2.48255\n",
            "epoch no.0 train no.67240  loss = 2.93034 avg_loss = 2.48457\n",
            "epoch no.0 train no.67250  loss = 2.66833 avg_loss = 2.49974\n",
            "epoch no.0 train no.67260  loss = 2.41946 avg_loss = 2.51795\n",
            "epoch no.0 train no.67270  loss = 3.49049 avg_loss = 2.52331\n",
            "epoch no.0 train no.67280  loss = 2.85321 avg_loss = 2.55979\n",
            "epoch no.0 train no.67290  loss = 2.51639 avg_loss = 2.54758\n",
            "epoch no.0 train no.67300  loss = 1.94765 avg_loss = 2.52584\n",
            "epoch no.0 train no.67310  loss = 2.66654 avg_loss = 2.53631\n",
            "epoch no.0 train no.67320  loss = 3.10864 avg_loss = 2.52597\n",
            "epoch no.0 train no.67330  loss = 2.91624 avg_loss = 2.53661\n",
            "epoch no.0 train no.67340  loss = 2.30779 avg_loss = 2.52246\n",
            "epoch no.0 train no.67350  loss = 2.14877 avg_loss = 2.50058\n",
            "epoch no.0 train no.67360  loss = 2.63159 avg_loss = 2.50537\n",
            "epoch no.0 train no.67370  loss = 2.51494 avg_loss = 2.48746\n",
            "epoch no.0 train no.67380  loss = 2.12742 avg_loss = 2.47921\n",
            "epoch no.0 train no.67390  loss = 3.21763 avg_loss = 2.50292\n",
            "epoch no.0 train no.67400  loss = 2.06983 avg_loss = 2.45588\n",
            "epoch no.0 train no.67410  loss = 3.17495 avg_loss = 2.44878\n",
            "epoch no.0 train no.67420  loss = 2.61217 avg_loss = 2.45278\n",
            "epoch no.0 train no.67430  loss = 1.84725 avg_loss = 2.44674\n",
            "epoch no.0 train no.67440  loss = 2.54439 avg_loss = 2.46566\n",
            "epoch no.0 train no.67450  loss = 2.14140 avg_loss = 2.46292\n",
            "epoch no.0 train no.67460  loss = 1.92415 avg_loss = 2.44467\n",
            "epoch no.0 train no.67470  loss = 2.98091 avg_loss = 2.43263\n",
            "epoch no.0 train no.67480  loss = 1.80871 avg_loss = 2.42814\n",
            "epoch no.0 train no.67490  loss = 2.89109 avg_loss = 2.41829\n",
            "epoch no.0 train no.67500  loss = 2.15294 avg_loss = 2.41462\n",
            "epoch no.0 train no.67510  loss = 2.41998 avg_loss = 2.41849\n",
            "epoch no.0 train no.67520  loss = 2.80518 avg_loss = 2.44584\n",
            "epoch no.0 train no.67530  loss = 2.20956 avg_loss = 2.42511\n",
            "epoch no.0 train no.67540  loss = 2.02189 avg_loss = 2.43150\n",
            "epoch no.0 train no.67550  loss = 2.47641 avg_loss = 2.42658\n",
            "epoch no.0 train no.67560  loss = 2.28429 avg_loss = 2.42639\n",
            "epoch no.0 train no.67570  loss = 2.66354 avg_loss = 2.43121\n",
            "epoch no.0 train no.67580  loss = 2.13149 avg_loss = 2.42282\n",
            "epoch no.0 train no.67590  loss = 2.54324 avg_loss = 2.42715\n",
            "epoch no.0 train no.67600  loss = 2.13070 avg_loss = 2.45118\n",
            "epoch no.0 train no.67610  loss = 3.00126 avg_loss = 2.45432\n",
            "epoch no.0 train no.67620  loss = 2.09668 avg_loss = 2.45295\n",
            "epoch no.0 train no.67630  loss = 1.47227 avg_loss = 2.43039\n",
            "epoch no.0 train no.67640  loss = 2.84757 avg_loss = 2.43463\n",
            "epoch no.0 train no.67650  loss = 2.59528 avg_loss = 2.45133\n",
            "epoch no.0 train no.67660  loss = 2.92057 avg_loss = 2.45899\n",
            "epoch no.0 train no.67670  loss = 2.15626 avg_loss = 2.45341\n",
            "epoch no.0 train no.67680  loss = 1.72826 avg_loss = 2.46457\n",
            "epoch no.0 train no.67690  loss = 2.03992 avg_loss = 2.47022\n",
            "epoch no.0 train no.67700  loss = 2.54621 avg_loss = 2.47116\n",
            "epoch no.0 train no.67710  loss = 2.71676 avg_loss = 2.45566\n",
            "epoch no.0 train no.67720  loss = 2.70040 avg_loss = 2.42951\n",
            "epoch no.0 train no.67730  loss = 2.18367 avg_loss = 2.43958\n",
            "epoch no.0 train no.67740  loss = 2.39307 avg_loss = 2.45102\n",
            "epoch no.0 train no.67750  loss = 2.11098 avg_loss = 2.44712\n",
            "epoch no.0 train no.67760  loss = 2.07264 avg_loss = 2.43231\n",
            "epoch no.0 train no.67770  loss = 2.77911 avg_loss = 2.44738\n",
            "epoch no.0 train no.67780  loss = 2.35667 avg_loss = 2.45747\n",
            "epoch no.0 train no.67790  loss = 2.17011 avg_loss = 2.45519\n",
            "epoch no.0 train no.67800  loss = 2.21758 avg_loss = 2.46433\n",
            "epoch no.0 train no.67810  loss = 3.18622 avg_loss = 2.46516\n",
            "epoch no.0 train no.67820  loss = 2.64165 avg_loss = 2.45411\n",
            "epoch no.0 train no.67830  loss = 3.17212 avg_loss = 2.45004\n",
            "epoch no.0 train no.67840  loss = 3.26935 avg_loss = 2.45780\n",
            "epoch no.0 train no.67850  loss = 1.94660 avg_loss = 2.42664\n",
            "epoch no.0 train no.67860  loss = 3.00666 avg_loss = 2.43853\n",
            "epoch no.0 train no.67870  loss = 2.73969 avg_loss = 2.42138\n",
            "epoch no.0 train no.67880  loss = 3.12191 avg_loss = 2.42151\n",
            "epoch no.0 train no.67890  loss = 2.64440 avg_loss = 2.46363\n",
            "epoch no.0 train no.67900  loss = 2.49894 avg_loss = 2.45280\n",
            "epoch no.0 train no.67910  loss = 2.64337 avg_loss = 2.43277\n",
            "epoch no.0 train no.67920  loss = 2.89240 avg_loss = 2.39818\n",
            "epoch no.0 train no.67930  loss = 2.08353 avg_loss = 2.39718\n",
            "epoch no.0 train no.67940  loss = 3.43475 avg_loss = 2.39978\n",
            "epoch no.0 train no.67950  loss = 2.46383 avg_loss = 2.39990\n",
            "epoch no.0 train no.67960  loss = 2.13917 avg_loss = 2.39528\n",
            "epoch no.0 train no.67970  loss = 2.73386 avg_loss = 2.40978\n",
            "epoch no.0 train no.67980  loss = 2.26457 avg_loss = 2.43444\n",
            "epoch no.0 train no.67990  loss = 2.60884 avg_loss = 2.43349\n",
            "epoch no.0 train no.68000  loss = 2.66675 avg_loss = 2.41431\n",
            "epoch no.0 train no.68010  loss = 2.82292 avg_loss = 2.40752\n",
            "epoch no.0 train no.68020  loss = 2.41778 avg_loss = 2.43142\n",
            "epoch no.0 train no.68030  loss = 2.87295 avg_loss = 2.44958\n",
            "epoch no.0 train no.68040  loss = 1.41855 avg_loss = 2.44954\n",
            "epoch no.0 train no.68050  loss = 2.59688 avg_loss = 2.46551\n",
            "epoch no.0 train no.68060  loss = 2.17760 avg_loss = 2.47496\n",
            "epoch no.0 train no.68070  loss = 2.25502 avg_loss = 2.46816\n",
            "epoch no.0 train no.68080  loss = 2.76447 avg_loss = 2.46502\n",
            "epoch no.0 train no.68090  loss = 2.03191 avg_loss = 2.44842\n",
            "epoch no.0 train no.68100  loss = 3.43402 avg_loss = 2.48452\n",
            "epoch no.0 train no.68110  loss = 2.65082 avg_loss = 2.45776\n",
            "epoch no.0 train no.68120  loss = 1.96965 avg_loss = 2.46724\n",
            "epoch no.0 train no.68130  loss = 1.48359 avg_loss = 2.46055\n",
            "epoch no.0 train no.68140  loss = 3.10974 avg_loss = 2.45894\n",
            "epoch no.0 train no.68150  loss = 2.15750 avg_loss = 2.46740\n",
            "epoch no.0 train no.68160  loss = 2.12444 avg_loss = 2.46095\n",
            "epoch no.0 train no.68170  loss = 2.35391 avg_loss = 2.47293\n",
            "epoch no.0 train no.68180  loss = 3.58693 avg_loss = 2.47501\n",
            "epoch no.0 train no.68190  loss = 2.05844 avg_loss = 2.46974\n",
            "epoch no.0 train no.68200  loss = 3.18294 avg_loss = 2.46243\n",
            "epoch no.0 train no.68210  loss = 1.74229 avg_loss = 2.47593\n",
            "epoch no.0 train no.68220  loss = 1.95163 avg_loss = 2.47898\n",
            "epoch no.0 train no.68230  loss = 2.06904 avg_loss = 2.44807\n",
            "epoch no.0 train no.68240  loss = 2.82590 avg_loss = 2.46418\n",
            "epoch no.0 train no.68250  loss = 1.80962 avg_loss = 2.48014\n",
            "epoch no.0 train no.68260  loss = 2.28207 avg_loss = 2.46558\n",
            "epoch no.0 train no.68270  loss = 3.20428 avg_loss = 2.47314\n",
            "epoch no.0 train no.68280  loss = 2.41889 avg_loss = 2.49368\n",
            "epoch no.0 train no.68290  loss = 1.72508 avg_loss = 2.47995\n",
            "epoch no.0 train no.68300  loss = 2.62480 avg_loss = 2.48769\n",
            "epoch no.0 train no.68310  loss = 3.11616 avg_loss = 2.50585\n",
            "epoch no.0 train no.68320  loss = 3.02823 avg_loss = 2.49503\n",
            "epoch no.0 train no.68330  loss = 3.05841 avg_loss = 2.47226\n",
            "epoch no.0 train no.68340  loss = 2.13681 avg_loss = 2.47642\n",
            "epoch no.0 train no.68350  loss = 2.02833 avg_loss = 2.47323\n",
            "epoch no.0 train no.68360  loss = 2.61917 avg_loss = 2.49117\n",
            "epoch no.0 train no.68370  loss = 2.00036 avg_loss = 2.49348\n",
            "epoch no.0 train no.68380  loss = 1.90379 avg_loss = 2.49518\n",
            "epoch no.0 train no.68390  loss = 2.05173 avg_loss = 2.45763\n",
            "epoch no.0 train no.68400  loss = 2.42323 avg_loss = 2.43603\n",
            "epoch no.0 train no.68410  loss = 2.28035 avg_loss = 2.44318\n",
            "epoch no.0 train no.68420  loss = 2.79742 avg_loss = 2.43376\n",
            "epoch no.0 train no.68430  loss = 3.07422 avg_loss = 2.44539\n",
            "epoch no.0 train no.68440  loss = 2.09579 avg_loss = 2.43508\n",
            "epoch no.0 train no.68450  loss = 2.10912 avg_loss = 2.45132\n",
            "epoch no.0 train no.68460  loss = 2.44751 avg_loss = 2.45041\n",
            "epoch no.0 train no.68470  loss = 2.51236 avg_loss = 2.45894\n",
            "epoch no.0 train no.68480  loss = 2.56390 avg_loss = 2.45160\n",
            "epoch no.0 train no.68490  loss = 3.15227 avg_loss = 2.45245\n",
            "epoch no.0 train no.68500  loss = 2.13705 avg_loss = 2.45636\n",
            "epoch no.0 train no.68510  loss = 1.46910 avg_loss = 2.44931\n",
            "epoch no.0 train no.68520  loss = 2.21924 avg_loss = 2.44149\n",
            "epoch no.0 train no.68530  loss = 2.19336 avg_loss = 2.42628\n",
            "epoch no.0 train no.68540  loss = 1.79653 avg_loss = 2.41391\n",
            "epoch no.0 train no.68550  loss = 2.83251 avg_loss = 2.42435\n",
            "epoch no.0 train no.68560  loss = 3.21361 avg_loss = 2.42302\n",
            "epoch no.0 train no.68570  loss = 2.83189 avg_loss = 2.43825\n",
            "epoch no.0 train no.68580  loss = 1.73843 avg_loss = 2.43543\n",
            "epoch no.0 train no.68590  loss = 3.49936 avg_loss = 2.44201\n",
            "epoch no.0 train no.68600  loss = 2.30801 avg_loss = 2.44316\n",
            "epoch no.0 train no.68610  loss = 2.47481 avg_loss = 2.45529\n",
            "epoch no.0 train no.68620  loss = 2.82130 avg_loss = 2.46213\n",
            "epoch no.0 train no.68630  loss = 2.53844 avg_loss = 2.45886\n",
            "epoch no.0 train no.68640  loss = 3.99522 avg_loss = 2.47896\n",
            "epoch no.0 train no.68650  loss = 1.84388 avg_loss = 2.47534\n",
            "epoch no.0 train no.68660  loss = 1.40760 avg_loss = 2.46992\n",
            "epoch no.0 train no.68670  loss = 2.39230 avg_loss = 2.46059\n",
            "epoch no.0 train no.68680  loss = 2.21480 avg_loss = 2.47140\n",
            "epoch no.0 train no.68690  loss = 1.95266 avg_loss = 2.48767\n",
            "epoch no.0 train no.68700  loss = 2.01063 avg_loss = 2.47785\n",
            "epoch no.0 train no.68710  loss = 3.00968 avg_loss = 2.47263\n",
            "epoch no.0 train no.68720  loss = 2.99882 avg_loss = 2.47401\n",
            "epoch no.0 train no.68730  loss = 1.34955 avg_loss = 2.45919\n",
            "epoch no.0 train no.68740  loss = 2.80625 avg_loss = 2.44503\n",
            "epoch no.0 train no.68750  loss = 2.85313 avg_loss = 2.42583\n",
            "epoch no.0 train no.68760  loss = 2.65905 avg_loss = 2.43475\n",
            "epoch no.0 train no.68770  loss = 2.76795 avg_loss = 2.42545\n",
            "epoch no.0 train no.68780  loss = 1.44067 avg_loss = 2.43186\n",
            "epoch no.0 train no.68790  loss = 3.26689 avg_loss = 2.45161\n",
            "epoch no.0 train no.68800  loss = 2.32157 avg_loss = 2.44938\n",
            "epoch no.0 train no.68810  loss = 2.86281 avg_loss = 2.45008\n",
            "epoch no.0 train no.68820  loss = 2.69582 avg_loss = 2.47350\n",
            "epoch no.0 train no.68830  loss = 2.56374 avg_loss = 2.47782\n",
            "epoch no.0 train no.68840  loss = 2.88796 avg_loss = 2.50020\n",
            "epoch no.0 train no.68850  loss = 1.87804 avg_loss = 2.48017\n",
            "epoch no.0 train no.68860  loss = 1.82792 avg_loss = 2.47297\n",
            "epoch no.0 train no.68870  loss = 2.02413 avg_loss = 2.46714\n",
            "epoch no.0 train no.68880  loss = 1.86380 avg_loss = 2.46724\n",
            "epoch no.0 train no.68890  loss = 2.33665 avg_loss = 2.47940\n",
            "epoch no.0 train no.68900  loss = 2.64877 avg_loss = 2.46788\n",
            "epoch no.0 train no.68910  loss = 3.52562 avg_loss = 2.45308\n",
            "epoch no.0 train no.68920  loss = 2.58933 avg_loss = 2.43309\n",
            "epoch no.0 train no.68930  loss = 2.11990 avg_loss = 2.42838\n",
            "epoch no.0 train no.68940  loss = 1.76536 avg_loss = 2.42636\n",
            "epoch no.0 train no.68950  loss = 1.78852 avg_loss = 2.43490\n",
            "epoch no.0 train no.68960  loss = 2.63266 avg_loss = 2.42997\n",
            "epoch no.0 train no.68970  loss = 2.33520 avg_loss = 2.43356\n",
            "epoch no.0 train no.68980  loss = 2.27196 avg_loss = 2.43674\n",
            "epoch no.0 train no.68990  loss = 2.43460 avg_loss = 2.45323\n",
            "epoch no.0 train no.69000  loss = 1.85608 avg_loss = 2.46526\n",
            "epoch no.0 train no.69010  loss = 2.57100 avg_loss = 2.44991\n",
            "epoch no.0 train no.69020  loss = 3.11449 avg_loss = 2.45767\n",
            "epoch no.0 train no.69030  loss = 2.53627 avg_loss = 2.45563\n",
            "epoch no.0 train no.69040  loss = 3.48835 avg_loss = 2.47669\n",
            "epoch no.0 train no.69050  loss = 1.50488 avg_loss = 2.46736\n",
            "epoch no.0 train no.69060  loss = 2.34549 avg_loss = 2.46177\n",
            "epoch no.0 train no.69070  loss = 1.96428 avg_loss = 2.43027\n",
            "epoch no.0 train no.69080  loss = 2.28553 avg_loss = 2.40388\n",
            "epoch no.0 train no.69090  loss = 2.57546 avg_loss = 2.42060\n",
            "epoch no.0 train no.69100  loss = 2.76415 avg_loss = 2.41769\n",
            "epoch no.0 train no.69110  loss = 2.26648 avg_loss = 2.42624\n",
            "epoch no.0 train no.69120  loss = 2.69520 avg_loss = 2.42057\n",
            "epoch no.0 train no.69130  loss = 2.18773 avg_loss = 2.42633\n",
            "epoch no.0 train no.69140  loss = 2.99367 avg_loss = 2.43849\n",
            "epoch no.0 train no.69150  loss = 2.47468 avg_loss = 2.43773\n",
            "epoch no.0 train no.69160  loss = 2.67406 avg_loss = 2.46094\n",
            "epoch no.0 train no.69170  loss = 3.27055 avg_loss = 2.46218\n",
            "epoch no.0 train no.69180  loss = 2.00539 avg_loss = 2.45798\n",
            "epoch no.0 train no.69190  loss = 2.06693 avg_loss = 2.46148\n",
            "epoch no.0 train no.69200  loss = 3.19309 avg_loss = 2.46823\n",
            "epoch no.0 train no.69210  loss = 1.95750 avg_loss = 2.46044\n",
            "epoch no.0 train no.69220  loss = 2.86894 avg_loss = 2.43625\n",
            "epoch no.0 train no.69230  loss = 2.21031 avg_loss = 2.43076\n",
            "epoch no.0 train no.69240  loss = 2.07954 avg_loss = 2.44282\n",
            "epoch no.0 train no.69250  loss = 2.38699 avg_loss = 2.44379\n",
            "epoch no.0 train no.69260  loss = 1.95330 avg_loss = 2.46134\n",
            "epoch no.0 train no.69270  loss = 2.62911 avg_loss = 2.44757\n",
            "epoch no.0 train no.69280  loss = 2.09301 avg_loss = 2.42562\n",
            "epoch no.0 train no.69290  loss = 2.94306 avg_loss = 2.43139\n",
            "epoch no.0 train no.69300  loss = 1.75174 avg_loss = 2.42521\n",
            "epoch no.0 train no.69310  loss = 1.93633 avg_loss = 2.42069\n",
            "epoch no.0 train no.69320  loss = 2.54480 avg_loss = 2.42059\n",
            "epoch no.0 train no.69330  loss = 2.67619 avg_loss = 2.41435\n",
            "epoch no.0 train no.69340  loss = 2.03028 avg_loss = 2.41154\n",
            "epoch no.0 train no.69350  loss = 1.54151 avg_loss = 2.40673\n",
            "epoch no.0 train no.69360  loss = 2.46624 avg_loss = 2.43169\n",
            "epoch no.0 train no.69370  loss = 2.34219 avg_loss = 2.43513\n",
            "epoch no.0 train no.69380  loss = 2.85289 avg_loss = 2.44961\n",
            "epoch no.0 train no.69390  loss = 2.28022 avg_loss = 2.43913\n",
            "epoch no.0 train no.69400  loss = 2.42330 avg_loss = 2.44741\n",
            "epoch no.0 train no.69410  loss = 1.72538 avg_loss = 2.46625\n",
            "epoch no.0 train no.69420  loss = 2.92918 avg_loss = 2.45348\n",
            "epoch no.0 train no.69430  loss = 2.34291 avg_loss = 2.45436\n",
            "epoch no.0 train no.69440  loss = 2.57298 avg_loss = 2.45598\n",
            "epoch no.0 train no.69450  loss = 1.92832 avg_loss = 2.46457\n",
            "epoch no.0 train no.69460  loss = 3.12865 avg_loss = 2.47721\n",
            "epoch no.0 train no.69470  loss = 1.92727 avg_loss = 2.46724\n",
            "epoch no.0 train no.69480  loss = 2.25574 avg_loss = 2.48289\n",
            "epoch no.0 train no.69490  loss = 3.18021 avg_loss = 2.49099\n",
            "epoch no.0 train no.69500  loss = 3.14570 avg_loss = 2.47592\n",
            "epoch no.0 train no.69510  loss = 2.13715 avg_loss = 2.47308\n",
            "epoch no.0 train no.69520  loss = 2.30837 avg_loss = 2.44677\n",
            "epoch no.0 train no.69530  loss = 3.39258 avg_loss = 2.46107\n",
            "epoch no.0 train no.69540  loss = 1.81996 avg_loss = 2.45659\n",
            "epoch no.0 train no.69550  loss = 2.44487 avg_loss = 2.44486\n",
            "epoch no.0 train no.69560  loss = 2.09958 avg_loss = 2.45643\n",
            "epoch no.0 train no.69570  loss = 3.05719 avg_loss = 2.46033\n",
            "epoch no.0 train no.69580  loss = 2.43550 avg_loss = 2.45528\n",
            "epoch no.0 train no.69590  loss = 2.81131 avg_loss = 2.43926\n",
            "epoch no.0 train no.69600  loss = 1.77076 avg_loss = 2.44548\n",
            "epoch no.0 train no.69610  loss = 2.20013 avg_loss = 2.43796\n",
            "epoch no.0 train no.69620  loss = 1.51307 avg_loss = 2.43637\n",
            "epoch no.0 train no.69630  loss = 2.09135 avg_loss = 2.43246\n",
            "epoch no.0 train no.69640  loss = 2.29696 avg_loss = 2.40754\n",
            "epoch no.0 train no.69650  loss = 2.66209 avg_loss = 2.41053\n",
            "epoch no.0 train no.69660  loss = 1.65720 avg_loss = 2.39796\n",
            "epoch no.0 train no.69670  loss = 2.46023 avg_loss = 2.39860\n",
            "epoch no.0 train no.69680  loss = 3.12417 avg_loss = 2.44716\n",
            "epoch no.0 train no.69690  loss = 3.06755 avg_loss = 2.44563\n",
            "epoch no.0 train no.69700  loss = 2.83828 avg_loss = 2.46466\n",
            "epoch no.0 train no.69710  loss = 2.13534 avg_loss = 2.44355\n",
            "epoch no.0 train no.69720  loss = 2.69951 avg_loss = 2.44582\n",
            "epoch no.0 train no.69730  loss = 1.74344 avg_loss = 2.44045\n",
            "epoch no.0 train no.69740  loss = 2.74774 avg_loss = 2.43703\n",
            "epoch no.0 train no.69750  loss = 2.69510 avg_loss = 2.41041\n",
            "epoch no.0 train no.69760  loss = 2.29903 avg_loss = 2.40120\n",
            "epoch no.0 train no.69770  loss = 2.31708 avg_loss = 2.40408\n",
            "epoch no.0 train no.69780  loss = 3.38239 avg_loss = 2.41092\n",
            "epoch no.0 train no.69790  loss = 2.76733 avg_loss = 2.42439\n",
            "epoch no.0 train no.69800  loss = 2.77176 avg_loss = 2.44520\n",
            "epoch no.0 train no.69810  loss = 2.41520 avg_loss = 2.43548\n",
            "epoch no.0 train no.69820  loss = 2.27650 avg_loss = 2.45329\n",
            "epoch no.0 train no.69830  loss = 2.67093 avg_loss = 2.47043\n",
            "epoch no.0 train no.69840  loss = 3.03074 avg_loss = 2.47923\n",
            "epoch no.0 train no.69850  loss = 2.64490 avg_loss = 2.46451\n",
            "epoch no.0 train no.69860  loss = 1.97101 avg_loss = 2.45322\n",
            "epoch no.0 train no.69870  loss = 2.74783 avg_loss = 2.43587\n",
            "epoch no.0 train no.69880  loss = 2.41338 avg_loss = 2.41168\n",
            "epoch no.0 train no.69890  loss = 1.76692 avg_loss = 2.40083\n",
            "epoch no.0 train no.69900  loss = 1.29685 avg_loss = 2.38548\n",
            "epoch no.0 train no.69910  loss = 2.99441 avg_loss = 2.40205\n",
            "epoch no.0 train no.69920  loss = 2.07281 avg_loss = 2.38774\n",
            "epoch no.0 train no.69930  loss = 2.96938 avg_loss = 2.40849\n",
            "epoch no.0 train no.69940  loss = 2.99146 avg_loss = 2.44215\n",
            "epoch no.0 train no.69950  loss = 2.88029 avg_loss = 2.45078\n",
            "epoch no.0 train no.69960  loss = 2.19811 avg_loss = 2.45165\n",
            "epoch no.0 train no.69970  loss = 2.48428 avg_loss = 2.46650\n",
            "epoch no.0 train no.69980  loss = 1.91959 avg_loss = 2.46326\n",
            "epoch no.0 train no.69990  loss = 2.24624 avg_loss = 2.42022\n",
            "epoch no.0 train no.70000  loss = 3.22260 avg_loss = 2.42585\n",
            "epoch no.0 train no.70010  loss = 1.91494 avg_loss = 2.42184\n",
            "epoch no.0 train no.70020  loss = 2.12424 avg_loss = 2.41372\n",
            "epoch no.0 train no.70030  loss = 2.04712 avg_loss = 2.40148\n",
            "epoch no.0 train no.70040  loss = 2.37266 avg_loss = 2.39415\n",
            "epoch no.0 train no.70050  loss = 2.01081 avg_loss = 2.41679\n",
            "epoch no.0 train no.70060  loss = 2.35659 avg_loss = 2.42018\n",
            "epoch no.0 train no.70070  loss = 2.28635 avg_loss = 2.42515\n",
            "epoch no.0 train no.70080  loss = 2.29850 avg_loss = 2.43039\n",
            "epoch no.0 train no.70090  loss = 2.97905 avg_loss = 2.44817\n",
            "epoch no.0 train no.70100  loss = 3.20106 avg_loss = 2.43629\n",
            "epoch no.0 train no.70110  loss = 2.44207 avg_loss = 2.42860\n",
            "epoch no.0 train no.70120  loss = 1.94870 avg_loss = 2.42767\n",
            "epoch no.0 train no.70130  loss = 3.79839 avg_loss = 2.43472\n",
            "epoch no.0 train no.70140  loss = 2.34485 avg_loss = 2.41572\n",
            "epoch no.0 train no.70150  loss = 2.68984 avg_loss = 2.42609\n",
            "epoch no.0 train no.70160  loss = 2.36741 avg_loss = 2.40746\n",
            "epoch no.0 train no.70170  loss = 2.39001 avg_loss = 2.38595\n",
            "epoch no.0 train no.70180  loss = 3.68862 avg_loss = 2.42244\n",
            "epoch no.0 train no.70190  loss = 2.56231 avg_loss = 2.42780\n",
            "epoch no.0 train no.70200  loss = 1.96992 avg_loss = 2.39870\n",
            "epoch no.0 train no.70210  loss = 2.26325 avg_loss = 2.40573\n",
            "epoch no.0 train no.70220  loss = 2.20382 avg_loss = 2.41694\n",
            "epoch no.0 train no.70230  loss = 2.21714 avg_loss = 2.39786\n",
            "epoch no.0 train no.70240  loss = 2.55673 avg_loss = 2.44123\n",
            "epoch no.0 train no.70250  loss = 1.48910 avg_loss = 2.43335\n",
            "epoch no.0 train no.70260  loss = 1.79371 avg_loss = 2.42515\n",
            "epoch no.0 train no.70270  loss = 1.78222 avg_loss = 2.42663\n",
            "epoch no.0 train no.70280  loss = 1.86776 avg_loss = 2.40868\n",
            "epoch no.0 train no.70290  loss = 2.42789 avg_loss = 2.41960\n",
            "epoch no.0 train no.70300  loss = 2.27900 avg_loss = 2.41743\n",
            "epoch no.0 train no.70310  loss = 2.08123 avg_loss = 2.46741\n",
            "epoch no.0 train no.70320  loss = 1.99760 avg_loss = 2.46238\n",
            "epoch no.0 train no.70330  loss = 2.09599 avg_loss = 2.46779\n",
            "epoch no.0 train no.70340  loss = 1.91515 avg_loss = 2.43617\n",
            "epoch no.0 train no.70350  loss = 2.53752 avg_loss = 2.42966\n",
            "epoch no.0 train no.70360  loss = 3.17053 avg_loss = 2.43360\n",
            "epoch no.0 train no.70370  loss = 3.09152 avg_loss = 2.43106\n",
            "epoch no.0 train no.70380  loss = 2.35347 avg_loss = 2.43176\n",
            "epoch no.0 train no.70390  loss = 1.98824 avg_loss = 2.44078\n",
            "epoch no.0 train no.70400  loss = 1.86477 avg_loss = 2.45004\n",
            "epoch no.0 train no.70410  loss = 2.39461 avg_loss = 2.43019\n",
            "epoch no.0 train no.70420  loss = 3.56993 avg_loss = 2.46605\n",
            "epoch no.0 train no.70430  loss = 2.28009 avg_loss = 2.45909\n",
            "epoch no.0 train no.70440  loss = 1.89134 avg_loss = 2.43822\n",
            "epoch no.0 train no.70450  loss = 2.41911 avg_loss = 2.40050\n",
            "epoch no.0 train no.70460  loss = 2.55467 avg_loss = 2.41957\n",
            "epoch no.0 train no.70470  loss = 1.74929 avg_loss = 2.44121\n",
            "epoch no.0 train no.70480  loss = 2.96225 avg_loss = 2.42246\n",
            "epoch no.0 train no.70490  loss = 2.58115 avg_loss = 2.40416\n",
            "epoch no.0 train no.70500  loss = 2.84460 avg_loss = 2.43708\n",
            "epoch no.0 train no.70510  loss = 3.14336 avg_loss = 2.43903\n",
            "epoch no.0 train no.70520  loss = 3.23611 avg_loss = 2.41197\n",
            "epoch no.0 train no.70530  loss = 2.42933 avg_loss = 2.44991\n",
            "epoch no.0 train no.70540  loss = 2.03586 avg_loss = 2.45294\n",
            "epoch no.0 train no.70550  loss = 2.73516 avg_loss = 2.46954\n",
            "epoch no.0 train no.70560  loss = 2.22154 avg_loss = 2.47550\n",
            "epoch no.0 train no.70570  loss = 2.37053 avg_loss = 2.46899\n",
            "epoch no.0 train no.70580  loss = 1.75654 avg_loss = 2.46195\n",
            "epoch no.0 train no.70590  loss = 2.56290 avg_loss = 2.47818\n",
            "epoch no.0 train no.70600  loss = 2.16853 avg_loss = 2.48328\n",
            "epoch no.0 train no.70610  loss = 3.06412 avg_loss = 2.50315\n",
            "epoch no.0 train no.70620  loss = 2.99635 avg_loss = 2.50671\n",
            "epoch no.0 train no.70630  loss = 2.88949 avg_loss = 2.51331\n",
            "epoch no.0 train no.70640  loss = 2.18110 avg_loss = 2.48494\n",
            "epoch no.0 train no.70650  loss = 1.47635 avg_loss = 2.45688\n",
            "epoch no.0 train no.70660  loss = 2.38212 avg_loss = 2.44631\n",
            "epoch no.0 train no.70670  loss = 3.45896 avg_loss = 2.46379\n",
            "epoch no.0 train no.70680  loss = 2.80641 avg_loss = 2.46621\n",
            "epoch no.0 train no.70690  loss = 1.75340 avg_loss = 2.46512\n",
            "epoch no.0 train no.70700  loss = 1.74897 avg_loss = 2.46320\n",
            "epoch no.0 train no.70710  loss = 1.62844 avg_loss = 2.44878\n",
            "epoch no.0 train no.70720  loss = 2.07272 avg_loss = 2.44326\n",
            "epoch no.0 train no.70730  loss = 2.97322 avg_loss = 2.42088\n",
            "epoch no.0 train no.70740  loss = 1.77391 avg_loss = 2.41542\n",
            "epoch no.0 train no.70750  loss = 2.64762 avg_loss = 2.41265\n",
            "epoch no.0 train no.70760  loss = 1.97540 avg_loss = 2.39742\n",
            "epoch no.0 train no.70770  loss = 2.65468 avg_loss = 2.41060\n",
            "epoch no.0 train no.70780  loss = 2.26719 avg_loss = 2.40337\n",
            "epoch no.0 train no.70790  loss = 1.63416 avg_loss = 2.40871\n",
            "epoch no.0 train no.70800  loss = 2.71521 avg_loss = 2.41596\n",
            "epoch no.0 train no.70810  loss = 1.94583 avg_loss = 2.42366\n",
            "epoch no.0 train no.70820  loss = 3.08459 avg_loss = 2.43056\n",
            "epoch no.0 train no.70830  loss = 2.14856 avg_loss = 2.41782\n",
            "epoch no.0 train no.70840  loss = 1.77869 avg_loss = 2.41180\n",
            "epoch no.0 train no.70850  loss = 2.75889 avg_loss = 2.42386\n",
            "epoch no.0 train no.70860  loss = 2.10774 avg_loss = 2.40398\n",
            "epoch no.0 train no.70870  loss = 1.63129 avg_loss = 2.39764\n",
            "epoch no.0 train no.70880  loss = 3.78859 avg_loss = 2.41147\n",
            "epoch no.0 train no.70890  loss = 1.30136 avg_loss = 2.40012\n",
            "epoch no.0 train no.70900  loss = 1.60459 avg_loss = 2.41655\n",
            "epoch no.0 train no.70910  loss = 2.64177 avg_loss = 2.43417\n",
            "epoch no.0 train no.70920  loss = 1.62293 avg_loss = 2.40970\n",
            "epoch no.0 train no.70930  loss = 2.88707 avg_loss = 2.41145\n",
            "epoch no.0 train no.70940  loss = 1.42683 avg_loss = 2.42558\n",
            "epoch no.0 train no.70950  loss = 2.26433 avg_loss = 2.41962\n",
            "epoch no.0 train no.70960  loss = 2.06490 avg_loss = 2.44110\n",
            "epoch no.0 train no.70970  loss = 3.06991 avg_loss = 2.41638\n",
            "epoch no.0 train no.70980  loss = 2.78787 avg_loss = 2.40118\n",
            "epoch no.0 train no.70990  loss = 2.20896 avg_loss = 2.42754\n",
            "epoch no.0 train no.71000  loss = 3.46327 avg_loss = 2.44262\n",
            "epoch no.0 train no.71010  loss = 2.31844 avg_loss = 2.44772\n",
            "epoch no.0 train no.71020  loss = 3.04688 avg_loss = 2.46758\n",
            "epoch no.0 train no.71030  loss = 2.24101 avg_loss = 2.44379\n",
            "epoch no.0 train no.71040  loss = 1.66742 avg_loss = 2.44957\n",
            "epoch no.0 train no.71050  loss = 1.59759 avg_loss = 2.44547\n",
            "epoch no.0 train no.71060  loss = 2.26496 avg_loss = 2.44311\n",
            "epoch no.0 train no.71070  loss = 3.53893 avg_loss = 2.46087\n",
            "epoch no.0 train no.71080  loss = 2.35908 avg_loss = 2.46387\n",
            "epoch no.0 train no.71090  loss = 3.21186 avg_loss = 2.45284\n",
            "epoch no.0 train no.71100  loss = 2.25120 avg_loss = 2.43874\n",
            "epoch no.0 train no.71110  loss = 2.81592 avg_loss = 2.44659\n",
            "epoch no.0 train no.71120  loss = 2.52474 avg_loss = 2.45370\n",
            "epoch no.0 train no.71130  loss = 2.61272 avg_loss = 2.46084\n",
            "epoch no.0 train no.71140  loss = 2.37457 avg_loss = 2.47537\n",
            "epoch no.0 train no.71150  loss = 2.79960 avg_loss = 2.48218\n",
            "epoch no.0 train no.71160  loss = 2.51604 avg_loss = 2.49819\n",
            "epoch no.0 train no.71170  loss = 2.01206 avg_loss = 2.47953\n",
            "epoch no.1 train no.71180  loss = 2.00797 avg_loss = 2.46913\n",
            "epoch no.1 train no.71190  loss = 2.26084 avg_loss = 2.47123\n",
            "epoch no.1 train no.71200  loss = 3.04183 avg_loss = 2.46825\n",
            "epoch no.1 train no.71210  loss = 2.70286 avg_loss = 2.46706\n",
            "epoch no.1 train no.71220  loss = 2.38311 avg_loss = 2.46089\n",
            "epoch no.1 train no.71230  loss = 2.41469 avg_loss = 2.43025\n",
            "epoch no.1 train no.71240  loss = 2.30521 avg_loss = 2.41238\n",
            "epoch no.1 train no.71250  loss = 3.24635 avg_loss = 2.41701\n",
            "epoch no.1 train no.71260  loss = 2.28555 avg_loss = 2.39138\n",
            "epoch no.1 train no.71270  loss = 1.85822 avg_loss = 2.38664\n",
            "epoch no.1 train no.71280  loss = 2.26436 avg_loss = 2.38233\n",
            "epoch no.1 train no.71290  loss = 2.03715 avg_loss = 2.35863\n",
            "epoch no.1 train no.71300  loss = 1.62776 avg_loss = 2.33711\n",
            "epoch no.1 train no.71310  loss = 2.47155 avg_loss = 2.32476\n",
            "epoch no.1 train no.71320  loss = 2.72723 avg_loss = 2.33443\n",
            "epoch no.1 train no.71330  loss = 1.98560 avg_loss = 2.31934\n",
            "epoch no.1 train no.71340  loss = 1.56174 avg_loss = 2.30484\n",
            "epoch no.1 train no.71350  loss = 2.05448 avg_loss = 2.29974\n",
            "epoch no.1 train no.71360  loss = 3.44686 avg_loss = 2.32272\n",
            "epoch no.1 train no.71370  loss = 1.79530 avg_loss = 2.32284\n",
            "epoch no.1 train no.71380  loss = 2.74955 avg_loss = 2.32539\n",
            "epoch no.1 train no.71390  loss = 2.60435 avg_loss = 2.33798\n",
            "epoch no.1 train no.71400  loss = 2.12469 avg_loss = 2.31578\n",
            "epoch no.1 train no.71410  loss = 2.88918 avg_loss = 2.33654\n",
            "epoch no.1 train no.71420  loss = 2.44219 avg_loss = 2.34673\n",
            "epoch no.1 train no.71430  loss = 1.83617 avg_loss = 2.33992\n",
            "epoch no.1 train no.71440  loss = 2.78515 avg_loss = 2.34291\n",
            "epoch no.1 train no.71450  loss = 1.69215 avg_loss = 2.35377\n",
            "epoch no.1 train no.71460  loss = 2.16302 avg_loss = 2.37386\n",
            "epoch no.1 train no.71470  loss = 2.43581 avg_loss = 2.37957\n",
            "epoch no.1 train no.71480  loss = 2.29365 avg_loss = 2.36685\n",
            "epoch no.1 train no.71490  loss = 2.15817 avg_loss = 2.35202\n",
            "epoch no.1 train no.71500  loss = 2.71408 avg_loss = 2.35118\n",
            "epoch no.1 train no.71510  loss = 2.41870 avg_loss = 2.35571\n",
            "epoch no.1 train no.71520  loss = 2.52391 avg_loss = 2.36188\n",
            "epoch no.1 train no.71530  loss = 2.18980 avg_loss = 2.35613\n",
            "epoch no.1 train no.71540  loss = 3.02878 avg_loss = 2.37475\n",
            "epoch no.1 train no.71550  loss = 2.24979 avg_loss = 2.36319\n",
            "epoch no.1 train no.71560  loss = 2.16861 avg_loss = 2.35690\n",
            "epoch no.1 train no.71570  loss = 3.42222 avg_loss = 2.35941\n",
            "epoch no.1 train no.71580  loss = 2.77793 avg_loss = 2.38235\n",
            "epoch no.1 train no.71590  loss = 2.51912 avg_loss = 2.37325\n",
            "epoch no.1 train no.71600  loss = 2.19235 avg_loss = 2.39103\n",
            "epoch no.1 train no.71610  loss = 2.78429 avg_loss = 2.36135\n",
            "epoch no.1 train no.71620  loss = 2.23607 avg_loss = 2.36229\n",
            "epoch no.1 train no.71630  loss = 3.12748 avg_loss = 2.37512\n",
            "epoch no.1 train no.71640  loss = 2.73718 avg_loss = 2.38380\n",
            "epoch no.1 train no.71650  loss = 1.77943 avg_loss = 2.35790\n",
            "epoch no.1 train no.71660  loss = 0.96436 avg_loss = 2.34956\n",
            "epoch no.1 train no.71670  loss = 3.15207 avg_loss = 2.35041\n",
            "epoch no.1 train no.71680  loss = 1.92846 avg_loss = 2.37594\n",
            "epoch no.1 train no.71690  loss = 1.85740 avg_loss = 2.39709\n",
            "epoch no.1 train no.71700  loss = 2.28267 avg_loss = 2.39700\n",
            "epoch no.1 train no.71710  loss = 1.86647 avg_loss = 2.37463\n",
            "epoch no.1 train no.71720  loss = 2.69581 avg_loss = 2.36977\n",
            "epoch no.1 train no.71730  loss = 1.38693 avg_loss = 2.36398\n",
            "epoch no.1 train no.71740  loss = 1.99708 avg_loss = 2.34600\n",
            "epoch no.1 train no.71750  loss = 2.29400 avg_loss = 2.31466\n",
            "epoch no.1 train no.71760  loss = 3.44692 avg_loss = 2.34409\n",
            "epoch no.1 train no.71770  loss = 2.48774 avg_loss = 2.34156\n",
            "epoch no.1 train no.71780  loss = 1.78408 avg_loss = 2.35493\n",
            "epoch no.1 train no.71790  loss = 1.89567 avg_loss = 2.33803\n",
            "epoch no.1 train no.71800  loss = 1.43885 avg_loss = 2.32694\n",
            "epoch no.1 train no.71810  loss = 1.81728 avg_loss = 2.34902\n",
            "epoch no.1 train no.71820  loss = 2.32059 avg_loss = 2.35079\n",
            "epoch no.1 train no.71830  loss = 2.95789 avg_loss = 2.34913\n",
            "epoch no.1 train no.71840  loss = 2.54478 avg_loss = 2.34042\n",
            "epoch no.1 train no.71850  loss = 2.25803 avg_loss = 2.34483\n",
            "epoch no.1 train no.71860  loss = 2.26685 avg_loss = 2.34703\n",
            "epoch no.1 train no.71870  loss = 1.77562 avg_loss = 2.32635\n",
            "epoch no.1 train no.71880  loss = 3.63999 avg_loss = 2.33331\n",
            "epoch no.1 train no.71890  loss = 2.33026 avg_loss = 2.33089\n",
            "epoch no.1 train no.71900  loss = 2.75181 avg_loss = 2.35452\n",
            "epoch no.1 train no.71910  loss = 2.55131 avg_loss = 2.33471\n",
            "epoch no.1 train no.71920  loss = 2.00014 avg_loss = 2.33659\n",
            "epoch no.1 train no.71930  loss = 2.29116 avg_loss = 2.34532\n",
            "epoch no.1 train no.71940  loss = 2.22723 avg_loss = 2.36810\n",
            "epoch no.1 train no.71950  loss = 2.91365 avg_loss = 2.36768\n",
            "epoch no.1 train no.71960  loss = 2.67449 avg_loss = 2.38194\n",
            "epoch no.1 train no.71970  loss = 2.64521 avg_loss = 2.38772\n",
            "epoch no.1 train no.71980  loss = 1.86178 avg_loss = 2.38453\n",
            "epoch no.1 train no.71990  loss = 1.63847 avg_loss = 2.36961\n",
            "epoch no.1 train no.72000  loss = 3.36436 avg_loss = 2.37859\n",
            "epoch no.1 train no.72010  loss = 2.42116 avg_loss = 2.36435\n",
            "epoch no.1 train no.72020  loss = 2.41547 avg_loss = 2.35801\n",
            "epoch no.1 train no.72030  loss = 2.77806 avg_loss = 2.36697\n",
            "epoch no.1 train no.72040  loss = 2.96883 avg_loss = 2.38215\n",
            "epoch no.1 train no.72050  loss = 1.92909 avg_loss = 2.40703\n",
            "epoch no.1 train no.72060  loss = 2.83538 avg_loss = 2.37198\n",
            "epoch no.1 train no.72070  loss = 2.56464 avg_loss = 2.39223\n",
            "epoch no.1 train no.72080  loss = 1.39653 avg_loss = 2.38849\n",
            "epoch no.1 train no.72090  loss = 1.68854 avg_loss = 2.38396\n",
            "epoch no.1 train no.72100  loss = 2.73702 avg_loss = 2.38925\n",
            "epoch no.1 train no.72110  loss = 1.56092 avg_loss = 2.38999\n",
            "epoch no.1 train no.72120  loss = 2.89412 avg_loss = 2.38634\n",
            "epoch no.1 train no.72130  loss = 2.49347 avg_loss = 2.36468\n",
            "epoch no.1 train no.72140  loss = 3.09391 avg_loss = 2.37000\n",
            "epoch no.1 train no.72150  loss = 1.91341 avg_loss = 2.35986\n",
            "epoch no.1 train no.72160  loss = 1.67742 avg_loss = 2.37002\n",
            "epoch no.1 train no.72170  loss = 1.54618 avg_loss = 2.36930\n",
            "epoch no.1 train no.72180  loss = 2.14228 avg_loss = 2.33588\n",
            "epoch no.1 train no.72190  loss = 0.98797 avg_loss = 2.34940\n",
            "epoch no.1 train no.72200  loss = 2.20788 avg_loss = 2.37728\n",
            "epoch no.1 train no.72210  loss = 2.57598 avg_loss = 2.36589\n",
            "epoch no.1 train no.72220  loss = 2.07616 avg_loss = 2.35885\n",
            "epoch no.1 train no.72230  loss = 2.03191 avg_loss = 2.35811\n",
            "epoch no.1 train no.72240  loss = 3.25166 avg_loss = 2.35315\n",
            "epoch no.1 train no.72250  loss = 2.62623 avg_loss = 2.35706\n",
            "epoch no.1 train no.72260  loss = 2.29305 avg_loss = 2.33550\n",
            "epoch no.1 train no.72270  loss = 2.58938 avg_loss = 2.33918\n",
            "epoch no.1 train no.72280  loss = 2.49832 avg_loss = 2.35362\n",
            "epoch no.1 train no.72290  loss = 2.31581 avg_loss = 2.36229\n",
            "epoch no.1 train no.72300  loss = 1.69375 avg_loss = 2.34953\n",
            "epoch no.1 train no.72310  loss = 2.18291 avg_loss = 2.34493\n",
            "epoch no.1 train no.72320  loss = 1.92136 avg_loss = 2.33691\n",
            "epoch no.1 train no.72330  loss = 2.59315 avg_loss = 2.33745\n",
            "epoch no.1 train no.72340  loss = 2.87055 avg_loss = 2.33729\n",
            "epoch no.1 train no.72350  loss = 2.53187 avg_loss = 2.34905\n",
            "epoch no.1 train no.72360  loss = 2.62108 avg_loss = 2.34083\n",
            "epoch no.1 train no.72370  loss = 2.95782 avg_loss = 2.37083\n",
            "epoch no.1 train no.72380  loss = 2.44238 avg_loss = 2.37556\n",
            "epoch no.1 train no.72390  loss = 2.63315 avg_loss = 2.37356\n",
            "epoch no.1 train no.72400  loss = 2.23364 avg_loss = 2.40591\n",
            "epoch no.1 train no.72410  loss = 2.61714 avg_loss = 2.40282\n",
            "epoch no.1 train no.72420  loss = 2.07567 avg_loss = 2.41410\n",
            "epoch no.1 train no.72430  loss = 2.05267 avg_loss = 2.42125\n",
            "epoch no.1 train no.72440  loss = 2.03766 avg_loss = 2.42201\n",
            "epoch no.1 train no.72450  loss = 2.52264 avg_loss = 2.42692\n",
            "epoch no.1 train no.72460  loss = 1.87410 avg_loss = 2.43271\n",
            "epoch no.1 train no.72470  loss = 2.29995 avg_loss = 2.42653\n",
            "epoch no.1 train no.72480  loss = 1.53243 avg_loss = 2.43050\n",
            "epoch no.1 train no.72490  loss = 2.27766 avg_loss = 2.44626\n",
            "epoch no.1 train no.72500  loss = 2.55362 avg_loss = 2.44261\n",
            "epoch no.1 train no.72510  loss = 2.89475 avg_loss = 2.43699\n",
            "epoch no.1 train no.72520  loss = 2.35750 avg_loss = 2.43632\n",
            "epoch no.1 train no.72530  loss = 2.71109 avg_loss = 2.45872\n",
            "epoch no.1 train no.72540  loss = 2.44539 avg_loss = 2.43009\n",
            "epoch no.1 train no.72550  loss = 3.02965 avg_loss = 2.45091\n",
            "epoch no.1 train no.72560  loss = 2.56410 avg_loss = 2.44909\n",
            "epoch no.1 train no.72570  loss = 1.79971 avg_loss = 2.44840\n",
            "epoch no.1 train no.72580  loss = 1.91746 avg_loss = 2.47153\n",
            "epoch no.1 train no.72590  loss = 2.01144 avg_loss = 2.45511\n",
            "epoch no.1 train no.72600  loss = 1.94348 avg_loss = 2.44802\n",
            "epoch no.1 train no.72610  loss = 2.33162 avg_loss = 2.43242\n",
            "epoch no.1 train no.72620  loss = 2.12700 avg_loss = 2.42983\n",
            "epoch no.1 train no.72630  loss = 3.08465 avg_loss = 2.46542\n",
            "epoch no.1 train no.72640  loss = 2.55886 avg_loss = 2.46369\n",
            "epoch no.1 train no.72650  loss = 2.69328 avg_loss = 2.44828\n",
            "epoch no.1 train no.72660  loss = 1.49505 avg_loss = 2.43394\n",
            "epoch no.1 train no.72670  loss = 1.91282 avg_loss = 2.42427\n",
            "epoch no.1 train no.72680  loss = 2.02141 avg_loss = 2.41957\n",
            "epoch no.1 train no.72690  loss = 2.61637 avg_loss = 2.40507\n",
            "epoch no.1 train no.72700  loss = 2.14726 avg_loss = 2.38892\n",
            "epoch no.1 train no.72710  loss = 3.16571 avg_loss = 2.38476\n",
            "epoch no.1 train no.72720  loss = 2.01666 avg_loss = 2.40649\n",
            "epoch no.1 train no.72730  loss = 1.75791 avg_loss = 2.39317\n",
            "epoch no.1 train no.72740  loss = 2.51024 avg_loss = 2.40278\n",
            "epoch no.1 train no.72750  loss = 2.18752 avg_loss = 2.41790\n",
            "epoch no.1 train no.72760  loss = 3.06860 avg_loss = 2.42857\n",
            "epoch no.1 train no.72770  loss = 3.23313 avg_loss = 2.42420\n",
            "epoch no.1 train no.72780  loss = 2.52689 avg_loss = 2.42155\n",
            "epoch no.1 train no.72790  loss = 2.33164 avg_loss = 2.40946\n",
            "epoch no.1 train no.72800  loss = 1.43762 avg_loss = 2.39837\n",
            "epoch no.1 train no.72810  loss = 2.66884 avg_loss = 2.39614\n",
            "epoch no.1 train no.72820  loss = 3.12328 avg_loss = 2.44287\n",
            "epoch no.1 train no.72830  loss = 2.42667 avg_loss = 2.42536\n",
            "epoch no.1 train no.72840  loss = 2.26573 avg_loss = 2.40759\n",
            "epoch no.1 train no.72850  loss = 2.10280 avg_loss = 2.43871\n",
            "epoch no.1 train no.72860  loss = 3.05051 avg_loss = 2.43192\n",
            "epoch no.1 train no.72870  loss = 2.18167 avg_loss = 2.42114\n",
            "epoch no.1 train no.72880  loss = 2.40556 avg_loss = 2.42070\n",
            "epoch no.1 train no.72890  loss = 1.76027 avg_loss = 2.39686\n",
            "epoch no.1 train no.72900  loss = 2.12359 avg_loss = 2.40381\n",
            "epoch no.1 train no.72910  loss = 2.12401 avg_loss = 2.40197\n",
            "epoch no.1 train no.72920  loss = 1.58547 avg_loss = 2.40219\n",
            "epoch no.1 train no.72930  loss = 3.00748 avg_loss = 2.37345\n",
            "epoch no.1 train no.72940  loss = 3.22399 avg_loss = 2.41553\n",
            "epoch no.1 train no.72950  loss = 1.73641 avg_loss = 2.39136\n",
            "epoch no.1 train no.72960  loss = 2.48134 avg_loss = 2.39964\n",
            "epoch no.1 train no.72970  loss = 1.93265 avg_loss = 2.40824\n",
            "epoch no.1 train no.72980  loss = 2.39655 avg_loss = 2.42964\n",
            "epoch no.1 train no.72990  loss = 2.38027 avg_loss = 2.43183\n",
            "epoch no.1 train no.73000  loss = 2.88966 avg_loss = 2.43217\n",
            "epoch no.1 train no.73010  loss = 1.53288 avg_loss = 2.41857\n",
            "epoch no.1 train no.73020  loss = 1.48932 avg_loss = 2.41303\n",
            "epoch no.1 train no.73030  loss = 2.32968 avg_loss = 2.39067\n",
            "epoch no.1 train no.73040  loss = 3.25160 avg_loss = 2.40820\n",
            "epoch no.1 train no.73050  loss = 2.54316 avg_loss = 2.40649\n",
            "epoch no.1 train no.73060  loss = 2.18637 avg_loss = 2.41258\n",
            "epoch no.1 train no.73070  loss = 2.90538 avg_loss = 2.43400\n",
            "epoch no.1 train no.73080  loss = 1.92802 avg_loss = 2.43029\n",
            "epoch no.1 train no.73090  loss = 1.91389 avg_loss = 2.41191\n",
            "epoch no.1 train no.73100  loss = 1.92582 avg_loss = 2.39868\n",
            "epoch no.1 train no.73110  loss = 2.18917 avg_loss = 2.40508\n",
            "epoch no.1 train no.73120  loss = 2.00490 avg_loss = 2.40929\n",
            "epoch no.1 train no.73130  loss = 1.79201 avg_loss = 2.41424\n",
            "epoch no.1 train no.73140  loss = 2.47601 avg_loss = 2.40533\n",
            "epoch no.1 train no.73150  loss = 2.44488 avg_loss = 2.41648\n",
            "epoch no.1 train no.73160  loss = 2.26140 avg_loss = 2.41606\n",
            "epoch no.1 train no.73170  loss = 1.95585 avg_loss = 2.40119\n",
            "epoch no.1 train no.73180  loss = 2.40042 avg_loss = 2.39423\n",
            "epoch no.1 train no.73190  loss = 2.63012 avg_loss = 2.40039\n",
            "epoch no.1 train no.73200  loss = 2.07490 avg_loss = 2.36521\n",
            "epoch no.1 train no.73210  loss = 3.66043 avg_loss = 2.37060\n",
            "epoch no.1 train no.73220  loss = 2.52359 avg_loss = 2.37823\n",
            "epoch no.1 train no.73230  loss = 2.01991 avg_loss = 2.37038\n",
            "epoch no.1 train no.73240  loss = 2.23794 avg_loss = 2.35189\n",
            "epoch no.1 train no.73250  loss = 2.93228 avg_loss = 2.32499\n",
            "epoch no.1 train no.73260  loss = 2.63229 avg_loss = 2.35104\n",
            "epoch no.1 train no.73270  loss = 2.51876 avg_loss = 2.37499\n",
            "epoch no.1 train no.73280  loss = 3.29858 avg_loss = 2.38077\n",
            "epoch no.1 train no.73290  loss = 2.00291 avg_loss = 2.37121\n",
            "epoch no.1 train no.73300  loss = 3.21675 avg_loss = 2.40018\n",
            "epoch no.1 train no.73310  loss = 2.77312 avg_loss = 2.40398\n",
            "epoch no.1 train no.73320  loss = 2.27078 avg_loss = 2.39626\n",
            "epoch no.1 train no.73330  loss = 2.75608 avg_loss = 2.38877\n",
            "epoch no.1 train no.73340  loss = 2.41162 avg_loss = 2.35612\n",
            "epoch no.1 train no.73350  loss = 2.85334 avg_loss = 2.36688\n",
            "epoch no.1 train no.73360  loss = 2.08533 avg_loss = 2.38536\n",
            "epoch no.1 train no.73370  loss = 2.86589 avg_loss = 2.39035\n",
            "epoch no.1 train no.73380  loss = 2.18195 avg_loss = 2.35247\n",
            "epoch no.1 train no.73390  loss = 2.35097 avg_loss = 2.35421\n",
            "epoch no.1 train no.73400  loss = 2.31795 avg_loss = 2.34800\n",
            "epoch no.1 train no.73410  loss = 3.07666 avg_loss = 2.38164\n",
            "epoch no.1 train no.73420  loss = 2.39707 avg_loss = 2.38414\n",
            "epoch no.1 train no.73430  loss = 3.05489 avg_loss = 2.38804\n",
            "epoch no.1 train no.73440  loss = 2.34970 avg_loss = 2.38888\n",
            "epoch no.1 train no.73450  loss = 3.20135 avg_loss = 2.41226\n",
            "epoch no.1 train no.73460  loss = 2.32780 avg_loss = 2.42114\n",
            "epoch no.1 train no.73470  loss = 2.43938 avg_loss = 2.39581\n",
            "epoch no.1 train no.73480  loss = 2.73839 avg_loss = 2.41857\n",
            "epoch no.1 train no.73490  loss = 1.82525 avg_loss = 2.40196\n",
            "epoch no.1 train no.73500  loss = 2.42695 avg_loss = 2.40428\n",
            "epoch no.1 train no.73510  loss = 2.55486 avg_loss = 2.38554\n",
            "epoch no.1 train no.73520  loss = 2.42993 avg_loss = 2.37371\n",
            "epoch no.1 train no.73530  loss = 2.91014 avg_loss = 2.36969\n",
            "epoch no.1 train no.73540  loss = 2.38812 avg_loss = 2.36231\n",
            "epoch no.1 train no.73550  loss = 2.54350 avg_loss = 2.34720\n",
            "epoch no.1 train no.73560  loss = 1.66041 avg_loss = 2.34662\n",
            "epoch no.1 train no.73570  loss = 2.37930 avg_loss = 2.34361\n",
            "epoch no.1 train no.73580  loss = 2.67548 avg_loss = 2.37049\n",
            "epoch no.1 train no.73590  loss = 1.73106 avg_loss = 2.35908\n",
            "epoch no.1 train no.73600  loss = 2.33300 avg_loss = 2.36031\n",
            "epoch no.1 train no.73610  loss = 2.12200 avg_loss = 2.37518\n",
            "epoch no.1 train no.73620  loss = 2.63993 avg_loss = 2.39327\n",
            "epoch no.1 train no.73630  loss = 2.47968 avg_loss = 2.36296\n",
            "epoch no.1 train no.73640  loss = 3.06502 avg_loss = 2.37172\n",
            "epoch no.1 train no.73650  loss = 3.28154 avg_loss = 2.39372\n",
            "epoch no.1 train no.73660  loss = 2.78422 avg_loss = 2.40250\n",
            "epoch no.1 train no.73670  loss = 2.73683 avg_loss = 2.40407\n",
            "epoch no.1 train no.73680  loss = 2.51922 avg_loss = 2.40851\n",
            "epoch no.1 train no.73690  loss = 2.18588 avg_loss = 2.39724\n",
            "epoch no.1 train no.73700  loss = 1.71837 avg_loss = 2.40153\n",
            "epoch no.1 train no.73710  loss = 2.05387 avg_loss = 2.36480\n",
            "epoch no.1 train no.73720  loss = 2.79162 avg_loss = 2.38873\n",
            "epoch no.1 train no.73730  loss = 2.56547 avg_loss = 2.37944\n",
            "epoch no.1 train no.73740  loss = 2.24029 avg_loss = 2.35365\n",
            "epoch no.1 train no.73750  loss = 2.68344 avg_loss = 2.34414\n",
            "epoch no.1 train no.73760  loss = 2.37082 avg_loss = 2.33906\n",
            "epoch no.1 train no.73770  loss = 1.83722 avg_loss = 2.33953\n",
            "epoch no.1 train no.73780  loss = 2.51633 avg_loss = 2.33186\n",
            "epoch no.1 train no.73790  loss = 2.48847 avg_loss = 2.33367\n",
            "epoch no.1 train no.73800  loss = 3.28855 avg_loss = 2.34088\n",
            "epoch no.1 train no.73810  loss = 2.91841 avg_loss = 2.33972\n",
            "epoch no.1 train no.73820  loss = 1.54058 avg_loss = 2.33932\n",
            "epoch no.1 train no.73830  loss = 2.32698 avg_loss = 2.33898\n",
            "epoch no.1 train no.73840  loss = 2.64309 avg_loss = 2.35542\n",
            "epoch no.1 train no.73850  loss = 2.50079 avg_loss = 2.34273\n",
            "epoch no.1 train no.73860  loss = 2.51710 avg_loss = 2.36954\n",
            "epoch no.1 train no.73870  loss = 1.51423 avg_loss = 2.35550\n",
            "epoch no.1 train no.73880  loss = 2.57789 avg_loss = 2.34089\n",
            "epoch no.1 train no.73890  loss = 1.72537 avg_loss = 2.32520\n",
            "epoch no.1 train no.73900  loss = 2.52116 avg_loss = 2.31840\n",
            "epoch no.1 train no.73910  loss = 1.88638 avg_loss = 2.32924\n",
            "epoch no.1 train no.73920  loss = 2.08671 avg_loss = 2.31894\n",
            "epoch no.1 train no.73930  loss = 2.27541 avg_loss = 2.31489\n",
            "epoch no.1 train no.73940  loss = 1.13593 avg_loss = 2.32230\n",
            "epoch no.1 train no.73950  loss = 2.18546 avg_loss = 2.32979\n",
            "epoch no.1 train no.73960  loss = 3.28082 avg_loss = 2.32691\n",
            "epoch no.1 train no.73970  loss = 2.11712 avg_loss = 2.30103\n",
            "epoch no.1 train no.73980  loss = 2.32441 avg_loss = 2.31212\n",
            "epoch no.1 train no.73990  loss = 2.33436 avg_loss = 2.32440\n",
            "epoch no.1 train no.74000  loss = 3.20947 avg_loss = 2.34868\n",
            "epoch no.1 train no.74010  loss = 2.88189 avg_loss = 2.35995\n",
            "epoch no.1 train no.74020  loss = 2.82593 avg_loss = 2.36910\n",
            "epoch no.1 train no.74030  loss = 2.01996 avg_loss = 2.39380\n",
            "epoch no.1 train no.74040  loss = 3.31551 avg_loss = 2.39221\n",
            "epoch no.1 train no.74050  loss = 1.03990 avg_loss = 2.39131\n",
            "epoch no.1 train no.74060  loss = 2.86778 avg_loss = 2.39228\n",
            "epoch no.1 train no.74070  loss = 2.80400 avg_loss = 2.40913\n",
            "epoch no.1 train no.74080  loss = 1.70504 avg_loss = 2.39967\n",
            "epoch no.1 train no.74090  loss = 3.34049 avg_loss = 2.41554\n",
            "epoch no.1 train no.74100  loss = 2.63890 avg_loss = 2.40867\n",
            "epoch no.1 train no.74110  loss = 1.26229 avg_loss = 2.40214\n",
            "epoch no.1 train no.74120  loss = 2.57289 avg_loss = 2.41443\n",
            "epoch no.1 train no.74130  loss = 2.69526 avg_loss = 2.40363\n",
            "epoch no.1 train no.74140  loss = 1.84424 avg_loss = 2.40026\n",
            "epoch no.1 train no.74150  loss = 1.86946 avg_loss = 2.38903\n",
            "epoch no.1 train no.74160  loss = 3.08598 avg_loss = 2.39810\n",
            "epoch no.1 train no.74170  loss = 2.04559 avg_loss = 2.38128\n",
            "epoch no.1 train no.74180  loss = 2.61922 avg_loss = 2.39738\n",
            "epoch no.1 train no.74190  loss = 2.61224 avg_loss = 2.39207\n",
            "epoch no.1 train no.74200  loss = 2.08167 avg_loss = 2.37246\n",
            "epoch no.1 train no.74210  loss = 2.74745 avg_loss = 2.36171\n",
            "epoch no.1 train no.74220  loss = 2.29075 avg_loss = 2.36926\n",
            "epoch no.1 train no.74230  loss = 2.63347 avg_loss = 2.36061\n",
            "epoch no.1 train no.74240  loss = 3.25614 avg_loss = 2.37668\n",
            "epoch no.1 train no.74250  loss = 2.71862 avg_loss = 2.37349\n",
            "epoch no.1 train no.74260  loss = 2.23268 avg_loss = 2.41777\n",
            "epoch no.1 train no.74270  loss = 3.09092 avg_loss = 2.42913\n",
            "epoch no.1 train no.74280  loss = 2.85411 avg_loss = 2.41585\n",
            "epoch no.1 train no.74290  loss = 2.17288 avg_loss = 2.39977\n",
            "epoch no.1 train no.74300  loss = 2.66462 avg_loss = 2.38553\n",
            "epoch no.1 train no.74310  loss = 1.79545 avg_loss = 2.38352\n",
            "epoch no.1 train no.74320  loss = 1.72917 avg_loss = 2.37587\n",
            "epoch no.1 train no.74330  loss = 2.34188 avg_loss = 2.37485\n",
            "epoch no.1 train no.74340  loss = 2.85623 avg_loss = 2.38529\n",
            "epoch no.1 train no.74350  loss = 1.92103 avg_loss = 2.36710\n",
            "epoch no.1 train no.74360  loss = 2.97607 avg_loss = 2.35672\n",
            "epoch no.1 train no.74370  loss = 1.75972 avg_loss = 2.34853\n",
            "epoch no.1 train no.74380  loss = 3.83182 avg_loss = 2.37090\n",
            "epoch no.1 train no.74390  loss = 2.25188 avg_loss = 2.37822\n",
            "epoch no.1 train no.74400  loss = 2.70806 avg_loss = 2.35860\n",
            "epoch no.1 train no.74410  loss = 3.01217 avg_loss = 2.38082\n",
            "epoch no.1 train no.74420  loss = 2.31042 avg_loss = 2.40184\n",
            "epoch no.1 train no.74430  loss = 1.46752 avg_loss = 2.41558\n",
            "epoch no.1 train no.74440  loss = 2.25573 avg_loss = 2.44462\n",
            "epoch no.1 train no.74450  loss = 2.18492 avg_loss = 2.44108\n",
            "epoch no.1 train no.74460  loss = 3.19522 avg_loss = 2.43375\n",
            "epoch no.1 train no.74470  loss = 2.19057 avg_loss = 2.43593\n",
            "epoch no.1 train no.74480  loss = 2.58096 avg_loss = 2.43504\n",
            "epoch no.1 train no.74490  loss = 2.37049 avg_loss = 2.42772\n",
            "epoch no.1 train no.74500  loss = 2.08926 avg_loss = 2.41872\n",
            "epoch no.1 train no.74510  loss = 2.25563 avg_loss = 2.42418\n",
            "epoch no.1 train no.74520  loss = 1.46147 avg_loss = 2.42050\n",
            "epoch no.1 train no.74530  loss = 2.13003 avg_loss = 2.39203\n",
            "epoch no.1 train no.74540  loss = 2.19135 avg_loss = 2.37785\n",
            "epoch no.1 train no.74550  loss = 1.93614 avg_loss = 2.38895\n",
            "epoch no.1 train no.74560  loss = 2.52970 avg_loss = 2.37837\n",
            "epoch no.1 train no.74570  loss = 4.21672 avg_loss = 2.39820\n",
            "epoch no.1 train no.74580  loss = 2.89230 avg_loss = 2.40113\n",
            "epoch no.1 train no.74590  loss = 2.84194 avg_loss = 2.41390\n",
            "epoch no.1 train no.74600  loss = 1.69752 avg_loss = 2.42260\n",
            "epoch no.1 train no.74610  loss = 3.21398 avg_loss = 2.43950\n",
            "epoch no.1 train no.74620  loss = 2.70794 avg_loss = 2.44974\n",
            "epoch no.1 train no.74630  loss = 2.67396 avg_loss = 2.44206\n",
            "epoch no.1 train no.74640  loss = 1.92296 avg_loss = 2.45153\n",
            "epoch no.1 train no.74650  loss = 1.96972 avg_loss = 2.41695\n",
            "epoch no.1 train no.74660  loss = 1.81167 avg_loss = 2.42198\n",
            "epoch no.1 train no.74670  loss = 1.66495 avg_loss = 2.41098\n",
            "epoch no.1 train no.74680  loss = 2.38998 avg_loss = 2.40784\n",
            "epoch no.1 train no.74690  loss = 2.22686 avg_loss = 2.43742\n",
            "epoch no.1 train no.74700  loss = 3.28283 avg_loss = 2.42502\n",
            "epoch no.1 train no.74710  loss = 2.01325 avg_loss = 2.41925\n",
            "epoch no.1 train no.74720  loss = 2.27435 avg_loss = 2.41960\n",
            "epoch no.1 train no.74730  loss = 2.40029 avg_loss = 2.41980\n",
            "epoch no.1 train no.74740  loss = 1.96246 avg_loss = 2.41769\n",
            "epoch no.1 train no.74750  loss = 2.71783 avg_loss = 2.40897\n",
            "epoch no.1 train no.74760  loss = 2.41049 avg_loss = 2.42306\n",
            "epoch no.1 train no.74770  loss = 1.64197 avg_loss = 2.40084\n",
            "epoch no.1 train no.74780  loss = 2.59923 avg_loss = 2.42676\n",
            "epoch no.1 train no.74790  loss = 2.25059 avg_loss = 2.43998\n",
            "epoch no.1 train no.74800  loss = 1.61315 avg_loss = 2.43437\n",
            "epoch no.1 train no.74810  loss = 2.19679 avg_loss = 2.39547\n",
            "epoch no.1 train no.74820  loss = 2.66341 avg_loss = 2.40005\n",
            "epoch no.1 train no.74830  loss = 2.02609 avg_loss = 2.37888\n",
            "epoch no.1 train no.74840  loss = 2.01781 avg_loss = 2.34979\n",
            "epoch no.1 train no.74850  loss = 2.65036 avg_loss = 2.34960\n",
            "epoch no.1 train no.74860  loss = 2.31842 avg_loss = 2.35738\n",
            "epoch no.1 train no.74870  loss = 3.43471 avg_loss = 2.37783\n",
            "epoch no.1 train no.74880  loss = 2.21813 avg_loss = 2.39425\n",
            "epoch no.1 train no.74890  loss = 3.51192 avg_loss = 2.42175\n",
            "epoch no.1 train no.74900  loss = 2.75591 avg_loss = 2.41124\n",
            "epoch no.1 train no.74910  loss = 2.54500 avg_loss = 2.42026\n",
            "epoch no.1 train no.74920  loss = 2.74835 avg_loss = 2.42962\n",
            "epoch no.1 train no.74930  loss = 1.92193 avg_loss = 2.41673\n",
            "epoch no.1 train no.74940  loss = 2.31514 avg_loss = 2.40832\n",
            "epoch no.1 train no.74950  loss = 3.01237 avg_loss = 2.40384\n",
            "epoch no.1 train no.74960  loss = 1.81636 avg_loss = 2.41009\n",
            "epoch no.1 train no.74970  loss = 2.45967 avg_loss = 2.40742\n",
            "epoch no.1 train no.74980  loss = 2.84579 avg_loss = 2.39373\n",
            "epoch no.1 train no.74990  loss = 2.78491 avg_loss = 2.42413\n",
            "epoch no.1 train no.75000  loss = 2.25926 avg_loss = 2.41207\n",
            "epoch no.1 train no.75010  loss = 2.74730 avg_loss = 2.42796\n",
            "epoch no.1 train no.75020  loss = 1.82308 avg_loss = 2.40381\n",
            "epoch no.1 train no.75030  loss = 2.78250 avg_loss = 2.41479\n",
            "epoch no.1 train no.75040  loss = 2.76199 avg_loss = 2.41984\n",
            "epoch no.1 train no.75050  loss = 2.50057 avg_loss = 2.39991\n",
            "epoch no.1 train no.75060  loss = 2.34350 avg_loss = 2.38563\n",
            "epoch no.1 train no.75070  loss = 2.20910 avg_loss = 2.34751\n",
            "epoch no.1 train no.75080  loss = 2.91824 avg_loss = 2.36150\n",
            "epoch no.1 train no.75090  loss = 1.91853 avg_loss = 2.36952\n",
            "epoch no.1 train no.75100  loss = 2.11599 avg_loss = 2.38950\n",
            "epoch no.1 train no.75110  loss = 2.33161 avg_loss = 2.37602\n",
            "epoch no.1 train no.75120  loss = 2.16012 avg_loss = 2.38968\n",
            "epoch no.1 train no.75130  loss = 1.82747 avg_loss = 2.36029\n",
            "epoch no.1 train no.75140  loss = 1.65229 avg_loss = 2.36947\n",
            "epoch no.1 train no.75150  loss = 2.64562 avg_loss = 2.37705\n",
            "epoch no.1 train no.75160  loss = 2.14321 avg_loss = 2.36058\n",
            "epoch no.1 train no.75170  loss = 2.08995 avg_loss = 2.36928\n",
            "epoch no.1 train no.75180  loss = 1.15478 avg_loss = 2.34601\n",
            "epoch no.1 train no.75190  loss = 1.89716 avg_loss = 2.34871\n",
            "epoch no.1 train no.75200  loss = 3.21006 avg_loss = 2.33724\n",
            "epoch no.1 train no.75210  loss = 2.71344 avg_loss = 2.34271\n",
            "epoch no.1 train no.75220  loss = 2.31637 avg_loss = 2.34788\n",
            "epoch no.1 train no.75230  loss = 2.51311 avg_loss = 2.35483\n",
            "epoch no.1 train no.75240  loss = 2.63185 avg_loss = 2.36356\n",
            "epoch no.1 train no.75250  loss = 1.82093 avg_loss = 2.37137\n",
            "epoch no.1 train no.75260  loss = 2.42559 avg_loss = 2.36379\n",
            "epoch no.1 train no.75270  loss = 2.36774 avg_loss = 2.35791\n",
            "epoch no.1 train no.75280  loss = 2.74411 avg_loss = 2.38492\n",
            "epoch no.1 train no.75290  loss = 2.17639 avg_loss = 2.35589\n",
            "epoch no.1 train no.75300  loss = 2.19067 avg_loss = 2.36076\n",
            "epoch no.1 train no.75310  loss = 2.00017 avg_loss = 2.39657\n",
            "epoch no.1 train no.75320  loss = 1.33101 avg_loss = 2.39529\n",
            "epoch no.1 train no.75330  loss = 1.73811 avg_loss = 2.39871\n",
            "epoch no.1 train no.75340  loss = 1.97795 avg_loss = 2.40416\n",
            "epoch no.1 train no.75350  loss = 2.16490 avg_loss = 2.40716\n",
            "epoch no.1 train no.75360  loss = 1.98111 avg_loss = 2.36178\n",
            "epoch no.1 train no.75370  loss = 2.44714 avg_loss = 2.36668\n",
            "epoch no.1 train no.75380  loss = 2.23632 avg_loss = 2.34621\n",
            "epoch no.1 train no.75390  loss = 1.49294 avg_loss = 2.34680\n",
            "epoch no.1 train no.75400  loss = 2.85480 avg_loss = 2.33523\n",
            "epoch no.1 train no.75410  loss = 2.26587 avg_loss = 2.34438\n",
            "epoch no.1 train no.75420  loss = 2.04796 avg_loss = 2.34185\n",
            "epoch no.1 train no.75430  loss = 1.88202 avg_loss = 2.35228\n",
            "epoch no.1 train no.75440  loss = 1.58934 avg_loss = 2.33387\n",
            "epoch no.1 train no.75450  loss = 2.12464 avg_loss = 2.33576\n",
            "epoch no.1 train no.75460  loss = 3.04916 avg_loss = 2.37588\n",
            "epoch no.1 train no.75470  loss = 2.83785 avg_loss = 2.38898\n",
            "epoch no.1 train no.75480  loss = 2.41897 avg_loss = 2.41936\n",
            "epoch no.1 train no.75490  loss = 2.23767 avg_loss = 2.42981\n",
            "epoch no.1 train no.75500  loss = 2.57995 avg_loss = 2.43391\n",
            "epoch no.1 train no.75510  loss = 2.32924 avg_loss = 2.44585\n",
            "epoch no.1 train no.75520  loss = 2.45563 avg_loss = 2.44261\n",
            "epoch no.1 train no.75530  loss = 2.31148 avg_loss = 2.41993\n",
            "epoch no.1 train no.75540  loss = 2.75368 avg_loss = 2.40721\n",
            "epoch no.1 train no.75550  loss = 2.92361 avg_loss = 2.40889\n",
            "epoch no.1 train no.75560  loss = 2.65550 avg_loss = 2.42284\n",
            "epoch no.1 train no.75570  loss = 2.68580 avg_loss = 2.43824\n",
            "epoch no.1 train no.75580  loss = 1.78925 avg_loss = 2.43272\n",
            "epoch no.1 train no.75590  loss = 1.18503 avg_loss = 2.41369\n",
            "epoch no.1 train no.75600  loss = 2.75740 avg_loss = 2.43564\n",
            "epoch no.1 train no.75610  loss = 2.56125 avg_loss = 2.40616\n",
            "epoch no.1 train no.75620  loss = 2.51785 avg_loss = 2.38694\n",
            "epoch no.1 train no.75630  loss = 1.76748 avg_loss = 2.35620\n",
            "epoch no.1 train no.75640  loss = 2.08845 avg_loss = 2.35057\n",
            "epoch no.1 train no.75650  loss = 1.74643 avg_loss = 2.33686\n",
            "epoch no.1 train no.75660  loss = 2.62740 avg_loss = 2.34023\n",
            "epoch no.1 train no.75670  loss = 3.29210 avg_loss = 2.34012\n",
            "epoch no.1 train no.75680  loss = 2.05438 avg_loss = 2.32939\n",
            "epoch no.1 train no.75690  loss = 1.25642 avg_loss = 2.31353\n",
            "epoch no.1 train no.75700  loss = 2.17303 avg_loss = 2.27524\n",
            "epoch no.1 train no.75710  loss = 3.02639 avg_loss = 2.28756\n",
            "epoch no.1 train no.75720  loss = 3.11487 avg_loss = 2.31596\n",
            "epoch no.1 train no.75730  loss = 2.09446 avg_loss = 2.33448\n",
            "epoch no.1 train no.75740  loss = 2.57186 avg_loss = 2.31142\n",
            "epoch no.1 train no.75750  loss = 2.28771 avg_loss = 2.31163\n",
            "epoch no.1 train no.75760  loss = 3.14207 avg_loss = 2.32053\n",
            "epoch no.1 train no.75770  loss = 2.95383 avg_loss = 2.33107\n",
            "epoch no.1 train no.75780  loss = 3.37318 avg_loss = 2.32518\n",
            "epoch no.1 train no.75790  loss = 2.09376 avg_loss = 2.36184\n",
            "epoch no.1 train no.75800  loss = 2.44073 avg_loss = 2.34997\n",
            "epoch no.1 train no.75810  loss = 2.19742 avg_loss = 2.35570\n",
            "epoch no.1 train no.75820  loss = 2.25288 avg_loss = 2.34216\n",
            "epoch no.1 train no.75830  loss = 1.59458 avg_loss = 2.31436\n",
            "epoch no.1 train no.75840  loss = 1.30643 avg_loss = 2.27313\n",
            "epoch no.1 train no.75850  loss = 1.46217 avg_loss = 2.26932\n",
            "epoch no.1 train no.75860  loss = 2.94173 avg_loss = 2.30717\n",
            "epoch no.1 train no.75870  loss = 1.52411 avg_loss = 2.30676\n",
            "epoch no.1 train no.75880  loss = 1.92828 avg_loss = 2.31377\n",
            "epoch no.1 train no.75890  loss = 2.18377 avg_loss = 2.31944\n",
            "epoch no.1 train no.75900  loss = 2.22381 avg_loss = 2.30674\n",
            "epoch no.1 train no.75910  loss = 2.44175 avg_loss = 2.32101\n",
            "epoch no.1 train no.75920  loss = 1.62186 avg_loss = 2.29933\n",
            "epoch no.1 train no.75930  loss = 2.59414 avg_loss = 2.30956\n",
            "epoch no.1 train no.75940  loss = 2.44359 avg_loss = 2.31867\n",
            "epoch no.1 train no.75950  loss = 2.89892 avg_loss = 2.32720\n",
            "epoch no.1 train no.75960  loss = 1.78545 avg_loss = 2.31838\n",
            "epoch no.1 train no.75970  loss = 3.11576 avg_loss = 2.32068\n",
            "epoch no.1 train no.75980  loss = 2.76806 avg_loss = 2.33487\n",
            "epoch no.1 train no.75990  loss = 2.18102 avg_loss = 2.32780\n",
            "epoch no.1 train no.76000  loss = 2.25261 avg_loss = 2.32090\n",
            "epoch no.1 train no.76010  loss = 2.96772 avg_loss = 2.34958\n",
            "epoch no.1 train no.76020  loss = 1.99879 avg_loss = 2.34698\n",
            "epoch no.1 train no.76030  loss = 2.78079 avg_loss = 2.36908\n",
            "epoch no.1 train no.76040  loss = 2.56272 avg_loss = 2.36012\n",
            "epoch no.1 train no.76050  loss = 2.96222 avg_loss = 2.36416\n",
            "epoch no.1 train no.76060  loss = 2.71824 avg_loss = 2.34330\n",
            "epoch no.1 train no.76070  loss = 2.25011 avg_loss = 2.34601\n",
            "epoch no.1 train no.76080  loss = 1.99863 avg_loss = 2.34486\n",
            "epoch no.1 train no.76090  loss = 2.23874 avg_loss = 2.35016\n",
            "epoch no.1 train no.76100  loss = 1.57062 avg_loss = 2.32175\n",
            "epoch no.1 train no.76110  loss = 2.59362 avg_loss = 2.35410\n",
            "epoch no.1 train no.76120  loss = 2.69594 avg_loss = 2.33171\n",
            "epoch no.1 train no.76130  loss = 3.02245 avg_loss = 2.37381\n",
            "epoch no.1 train no.76140  loss = 3.01198 avg_loss = 2.40792\n",
            "epoch no.1 train no.76150  loss = 3.28630 avg_loss = 2.43240\n",
            "epoch no.1 train no.76160  loss = 2.73578 avg_loss = 2.39713\n",
            "epoch no.1 train no.76170  loss = 1.39508 avg_loss = 2.36642\n",
            "epoch no.1 train no.76180  loss = 1.77704 avg_loss = 2.35797\n",
            "epoch no.1 train no.76190  loss = 2.77925 avg_loss = 2.37074\n",
            "epoch no.1 train no.76200  loss = 3.43062 avg_loss = 2.37706\n",
            "epoch no.1 train no.76210  loss = 1.27830 avg_loss = 2.39110\n",
            "epoch no.1 train no.76220  loss = 2.56226 avg_loss = 2.39635\n",
            "epoch no.1 train no.76230  loss = 2.43089 avg_loss = 2.39528\n",
            "epoch no.1 train no.76240  loss = 2.38949 avg_loss = 2.39638\n",
            "epoch no.1 train no.76250  loss = 1.57127 avg_loss = 2.39196\n",
            "epoch no.1 train no.76260  loss = 2.39124 avg_loss = 2.39702\n",
            "epoch no.1 train no.76270  loss = 2.42340 avg_loss = 2.40724\n",
            "epoch no.1 train no.76280  loss = 2.51862 avg_loss = 2.39639\n",
            "epoch no.1 train no.76290  loss = 3.15720 avg_loss = 2.38270\n",
            "epoch no.1 train no.76300  loss = 2.34421 avg_loss = 2.39442\n",
            "epoch no.1 train no.76310  loss = 1.96374 avg_loss = 2.41448\n",
            "epoch no.1 train no.76320  loss = 1.17663 avg_loss = 2.37405\n",
            "epoch no.1 train no.76330  loss = 2.91649 avg_loss = 2.41516\n",
            "epoch no.1 train no.76340  loss = 2.46607 avg_loss = 2.41550\n",
            "epoch no.1 train no.76350  loss = 1.23289 avg_loss = 2.43018\n",
            "epoch no.1 train no.76360  loss = 1.36340 avg_loss = 2.41149\n",
            "epoch no.1 train no.76370  loss = 2.23553 avg_loss = 2.42466\n",
            "epoch no.1 train no.76380  loss = 2.33854 avg_loss = 2.42264\n",
            "epoch no.1 train no.76390  loss = 2.32883 avg_loss = 2.44168\n",
            "epoch no.1 train no.76400  loss = 1.67954 avg_loss = 2.41361\n",
            "epoch no.1 train no.76410  loss = 2.27237 avg_loss = 2.45111\n",
            "epoch no.1 train no.76420  loss = 1.63069 avg_loss = 2.40297\n",
            "epoch no.1 train no.76430  loss = 1.93337 avg_loss = 2.38895\n",
            "epoch no.1 train no.76440  loss = 2.99722 avg_loss = 2.37890\n",
            "epoch no.1 train no.76450  loss = 2.19091 avg_loss = 2.38686\n",
            "epoch no.1 train no.76460  loss = 2.48790 avg_loss = 2.39554\n",
            "epoch no.1 train no.76470  loss = 2.91263 avg_loss = 2.40588\n",
            "epoch no.1 train no.76480  loss = 2.50952 avg_loss = 2.41211\n",
            "epoch no.1 train no.76490  loss = 2.56457 avg_loss = 2.40736\n",
            "epoch no.1 train no.76500  loss = 1.34251 avg_loss = 2.40004\n",
            "epoch no.1 train no.76510  loss = 3.34226 avg_loss = 2.41208\n",
            "epoch no.1 train no.76520  loss = 2.89442 avg_loss = 2.40648\n",
            "epoch no.1 train no.76530  loss = 1.94801 avg_loss = 2.40736\n",
            "epoch no.1 train no.76540  loss = 2.62807 avg_loss = 2.38083\n",
            "epoch no.1 train no.76550  loss = 2.20557 avg_loss = 2.37081\n",
            "epoch no.1 train no.76560  loss = 2.59602 avg_loss = 2.39128\n",
            "epoch no.1 train no.76570  loss = 1.87738 avg_loss = 2.36685\n",
            "epoch no.1 train no.76580  loss = 1.29254 avg_loss = 2.35593\n",
            "epoch no.1 train no.76590  loss = 2.81977 avg_loss = 2.35043\n",
            "epoch no.1 train no.76600  loss = 2.47218 avg_loss = 2.34026\n",
            "epoch no.1 train no.76610  loss = 2.02309 avg_loss = 2.34018\n",
            "epoch no.1 train no.76620  loss = 2.99374 avg_loss = 2.34103\n",
            "epoch no.1 train no.76630  loss = 1.70155 avg_loss = 2.30927\n",
            "epoch no.1 train no.76640  loss = 2.02471 avg_loss = 2.32354\n",
            "epoch no.1 train no.76650  loss = 1.37991 avg_loss = 2.31160\n",
            "epoch no.1 train no.76660  loss = 2.93804 avg_loss = 2.34376\n",
            "epoch no.1 train no.76670  loss = 3.41094 avg_loss = 2.34878\n",
            "epoch no.1 train no.76680  loss = 2.65417 avg_loss = 2.34894\n",
            "epoch no.1 train no.76690  loss = 2.29026 avg_loss = 2.34405\n",
            "epoch no.1 train no.76700  loss = 2.08819 avg_loss = 2.33162\n",
            "epoch no.1 train no.76710  loss = 2.80742 avg_loss = 2.34304\n",
            "epoch no.1 train no.76720  loss = 2.49424 avg_loss = 2.36073\n",
            "epoch no.1 train no.76730  loss = 2.69732 avg_loss = 2.36229\n",
            "epoch no.1 train no.76740  loss = 3.03299 avg_loss = 2.38575\n",
            "epoch no.1 train no.76750  loss = 2.03793 avg_loss = 2.37844\n",
            "epoch no.1 train no.76760  loss = 2.40643 avg_loss = 2.36795\n",
            "epoch no.1 train no.76770  loss = 2.60259 avg_loss = 2.38168\n",
            "epoch no.1 train no.76780  loss = 2.60279 avg_loss = 2.38329\n",
            "epoch no.1 train no.76790  loss = 3.01169 avg_loss = 2.41106\n",
            "epoch no.1 train no.76800  loss = 2.64834 avg_loss = 2.42536\n",
            "epoch no.1 train no.76810  loss = 2.56331 avg_loss = 2.41831\n",
            "epoch no.1 train no.76820  loss = 1.59151 avg_loss = 2.39264\n",
            "epoch no.1 train no.76830  loss = 2.15769 avg_loss = 2.37741\n",
            "epoch no.1 train no.76840  loss = 3.09455 avg_loss = 2.37991\n",
            "epoch no.1 train no.76850  loss = 1.31080 avg_loss = 2.37488\n",
            "epoch no.1 train no.76860  loss = 2.35482 avg_loss = 2.37752\n",
            "epoch no.1 train no.76870  loss = 3.16132 avg_loss = 2.39034\n",
            "epoch no.1 train no.76880  loss = 1.75387 avg_loss = 2.37794\n",
            "epoch no.1 train no.76890  loss = 3.06168 avg_loss = 2.38807\n",
            "epoch no.1 train no.76900  loss = 2.52170 avg_loss = 2.37396\n",
            "epoch no.1 train no.76910  loss = 2.84546 avg_loss = 2.38970\n",
            "epoch no.1 train no.76920  loss = 1.94532 avg_loss = 2.40096\n",
            "epoch no.1 train no.76930  loss = 2.17866 avg_loss = 2.39274\n",
            "epoch no.1 train no.76940  loss = 2.64833 avg_loss = 2.39635\n",
            "epoch no.1 train no.76950  loss = 2.49384 avg_loss = 2.39861\n",
            "epoch no.1 train no.76960  loss = 2.46370 avg_loss = 2.41144\n",
            "epoch no.1 train no.76970  loss = 1.87181 avg_loss = 2.40101\n",
            "epoch no.1 train no.76980  loss = 2.68068 avg_loss = 2.38841\n",
            "epoch no.1 train no.76990  loss = 1.72980 avg_loss = 2.36248\n",
            "epoch no.1 train no.77000  loss = 3.11523 avg_loss = 2.37923\n",
            "epoch no.1 train no.77010  loss = 3.15158 avg_loss = 2.37216\n",
            "epoch no.1 train no.77020  loss = 2.50269 avg_loss = 2.37772\n",
            "epoch no.1 train no.77030  loss = 1.92935 avg_loss = 2.35877\n",
            "epoch no.1 train no.77040  loss = 2.14664 avg_loss = 2.38506\n",
            "epoch no.1 train no.77050  loss = 1.87145 avg_loss = 2.36119\n",
            "epoch no.1 train no.77060  loss = 3.46606 avg_loss = 2.35418\n",
            "epoch no.1 train no.77070  loss = 2.33701 avg_loss = 2.35185\n",
            "epoch no.1 train no.77080  loss = 2.07204 avg_loss = 2.34714\n",
            "epoch no.1 train no.77090  loss = 1.78329 avg_loss = 2.33408\n",
            "epoch no.1 train no.77100  loss = 2.75902 avg_loss = 2.33183\n",
            "epoch no.1 train no.77110  loss = 1.71274 avg_loss = 2.31043\n",
            "epoch no.1 train no.77120  loss = 2.26543 avg_loss = 2.32772\n",
            "epoch no.1 train no.77130  loss = 1.53275 avg_loss = 2.30632\n",
            "epoch no.1 train no.77140  loss = 2.69000 avg_loss = 2.31081\n",
            "epoch no.1 train no.77150  loss = 2.22437 avg_loss = 2.29125\n",
            "epoch no.1 train no.77160  loss = 1.57273 avg_loss = 2.30090\n",
            "epoch no.1 train no.77170  loss = 2.90054 avg_loss = 2.32925\n",
            "epoch no.1 train no.77180  loss = 3.12469 avg_loss = 2.29156\n",
            "epoch no.1 train no.77190  loss = 2.25980 avg_loss = 2.30338\n",
            "epoch no.1 train no.77200  loss = 2.77183 avg_loss = 2.33225\n",
            "epoch no.1 train no.77210  loss = 3.65377 avg_loss = 2.34865\n",
            "epoch no.1 train no.77220  loss = 2.18772 avg_loss = 2.35270\n",
            "epoch no.1 train no.77230  loss = 2.55698 avg_loss = 2.38039\n",
            "epoch no.1 train no.77240  loss = 2.94401 avg_loss = 2.38020\n",
            "epoch no.1 train no.77250  loss = 2.47240 avg_loss = 2.36984\n",
            "epoch no.1 train no.77260  loss = 2.27704 avg_loss = 2.38470\n",
            "epoch no.1 train no.77270  loss = 3.07350 avg_loss = 2.38924\n",
            "epoch no.1 train no.77280  loss = 1.82971 avg_loss = 2.36334\n",
            "epoch no.1 train no.77290  loss = 1.57491 avg_loss = 2.34771\n",
            "epoch no.1 train no.77300  loss = 2.37990 avg_loss = 2.35733\n",
            "epoch no.1 train no.77310  loss = 1.31051 avg_loss = 2.32884\n",
            "epoch no.1 train no.77320  loss = 2.47874 avg_loss = 2.33414\n",
            "epoch no.1 train no.77330  loss = 1.65246 avg_loss = 2.31342\n",
            "epoch no.1 train no.77340  loss = 1.92098 avg_loss = 2.30801\n",
            "epoch no.1 train no.77350  loss = 2.48534 avg_loss = 2.32054\n",
            "epoch no.1 train no.77360  loss = 1.69971 avg_loss = 2.32864\n",
            "epoch no.1 train no.77370  loss = 2.29368 avg_loss = 2.33369\n",
            "epoch no.1 train no.77380  loss = 2.62499 avg_loss = 2.33724\n",
            "epoch no.1 train no.77390  loss = 2.14804 avg_loss = 2.36132\n",
            "epoch no.1 train no.77400  loss = 3.07811 avg_loss = 2.36108\n",
            "epoch no.1 train no.77410  loss = 3.11501 avg_loss = 2.33793\n",
            "epoch no.1 train no.77420  loss = 2.81365 avg_loss = 2.34816\n",
            "epoch no.1 train no.77430  loss = 2.39223 avg_loss = 2.34059\n",
            "epoch no.1 train no.77440  loss = 2.11078 avg_loss = 2.32875\n",
            "epoch no.1 train no.77450  loss = 2.14591 avg_loss = 2.31979\n",
            "epoch no.1 train no.77460  loss = 2.60745 avg_loss = 2.32384\n",
            "epoch no.1 train no.77470  loss = 3.41988 avg_loss = 2.33652\n",
            "epoch no.1 train no.77480  loss = 2.45571 avg_loss = 2.31975\n",
            "epoch no.1 train no.77490  loss = 2.78656 avg_loss = 2.32759\n",
            "epoch no.1 train no.77500  loss = 2.47717 avg_loss = 2.35712\n",
            "epoch no.1 train no.77510  loss = 2.29858 avg_loss = 2.36205\n",
            "epoch no.1 train no.77520  loss = 2.55488 avg_loss = 2.36980\n",
            "epoch no.1 train no.77530  loss = 2.64313 avg_loss = 2.37093\n",
            "epoch no.1 train no.77540  loss = 2.20268 avg_loss = 2.38070\n",
            "epoch no.1 train no.77550  loss = 2.16258 avg_loss = 2.38712\n",
            "epoch no.1 train no.77560  loss = 1.70676 avg_loss = 2.38133\n",
            "epoch no.1 train no.77570  loss = 2.90151 avg_loss = 2.37024\n",
            "epoch no.1 train no.77580  loss = 2.35262 avg_loss = 2.36591\n",
            "epoch no.1 train no.77590  loss = 2.27435 avg_loss = 2.35764\n",
            "epoch no.1 train no.77600  loss = 2.30893 avg_loss = 2.33696\n",
            "epoch no.1 train no.77610  loss = 2.64536 avg_loss = 2.34836\n",
            "epoch no.1 train no.77620  loss = 2.26414 avg_loss = 2.33488\n",
            "epoch no.1 train no.77630  loss = 2.60052 avg_loss = 2.34109\n",
            "epoch no.1 train no.77640  loss = 2.08652 avg_loss = 2.34441\n",
            "epoch no.1 train no.77650  loss = 2.87710 avg_loss = 2.34820\n",
            "epoch no.1 train no.77660  loss = 2.24862 avg_loss = 2.33297\n",
            "epoch no.1 train no.77670  loss = 1.24143 avg_loss = 2.31139\n",
            "epoch no.1 train no.77680  loss = 1.83648 avg_loss = 2.30743\n",
            "epoch no.1 train no.77690  loss = 2.84721 avg_loss = 2.35440\n",
            "epoch no.1 train no.77700  loss = 2.35218 avg_loss = 2.35928\n",
            "epoch no.1 train no.77710  loss = 1.89384 avg_loss = 2.34481\n",
            "epoch no.1 train no.77720  loss = 1.60164 avg_loss = 2.34337\n",
            "epoch no.1 train no.77730  loss = 0.99769 avg_loss = 2.34319\n",
            "epoch no.1 train no.77740  loss = 1.47130 avg_loss = 2.37508\n",
            "epoch no.1 train no.77750  loss = 2.07165 avg_loss = 2.39038\n",
            "epoch no.1 train no.77760  loss = 1.34168 avg_loss = 2.37712\n",
            "epoch no.1 train no.77770  loss = 1.53267 avg_loss = 2.38885\n",
            "epoch no.1 train no.77780  loss = 2.73516 avg_loss = 2.40249\n",
            "epoch no.1 train no.77790  loss = 1.88654 avg_loss = 2.40641\n",
            "epoch no.1 train no.77800  loss = 2.45691 avg_loss = 2.38317\n",
            "epoch no.1 train no.77810  loss = 2.46493 avg_loss = 2.36909\n",
            "epoch no.1 train no.77820  loss = 1.80747 avg_loss = 2.36315\n",
            "epoch no.1 train no.77830  loss = 2.54772 avg_loss = 2.35889\n",
            "epoch no.1 train no.77840  loss = 1.69283 avg_loss = 2.37200\n",
            "epoch no.1 train no.77850  loss = 2.18744 avg_loss = 2.37084\n",
            "epoch no.1 train no.77860  loss = 2.55310 avg_loss = 2.38454\n",
            "epoch no.1 train no.77870  loss = 2.44254 avg_loss = 2.38145\n",
            "epoch no.1 train no.77880  loss = 2.83403 avg_loss = 2.37517\n",
            "epoch no.1 train no.77890  loss = 2.78538 avg_loss = 2.35676\n",
            "epoch no.1 train no.77900  loss = 2.66987 avg_loss = 2.35516\n",
            "epoch no.1 train no.77910  loss = 1.96293 avg_loss = 2.34547\n",
            "epoch no.1 train no.77920  loss = 2.08878 avg_loss = 2.34811\n",
            "epoch no.1 train no.77930  loss = 1.80027 avg_loss = 2.34775\n",
            "epoch no.1 train no.77940  loss = 2.12809 avg_loss = 2.32293\n",
            "epoch no.1 train no.77950  loss = 3.06151 avg_loss = 2.32434\n",
            "epoch no.1 train no.77960  loss = 2.10731 avg_loss = 2.31226\n",
            "epoch no.1 train no.77970  loss = 2.72359 avg_loss = 2.31381\n",
            "epoch no.1 train no.77980  loss = 1.78838 avg_loss = 2.32071\n",
            "epoch no.1 train no.77990  loss = 1.33071 avg_loss = 2.32356\n",
            "epoch no.1 train no.78000  loss = 1.89161 avg_loss = 2.31947\n",
            "epoch no.1 train no.78010  loss = 2.08704 avg_loss = 2.31984\n",
            "epoch no.1 train no.78020  loss = 2.19519 avg_loss = 2.34753\n",
            "epoch no.1 train no.78030  loss = 3.11629 avg_loss = 2.34721\n",
            "epoch no.1 train no.78040  loss = 1.74514 avg_loss = 2.35045\n",
            "epoch no.1 train no.78050  loss = 2.69412 avg_loss = 2.36279\n",
            "epoch no.1 train no.78060  loss = 2.22632 avg_loss = 2.36498\n",
            "epoch no.1 train no.78070  loss = 2.64167 avg_loss = 2.35654\n",
            "epoch no.1 train no.78080  loss = 2.57278 avg_loss = 2.35755\n",
            "epoch no.1 train no.78090  loss = 2.25147 avg_loss = 2.35608\n",
            "epoch no.1 train no.78100  loss = 2.70841 avg_loss = 2.38794\n",
            "epoch no.1 train no.78110  loss = 2.29534 avg_loss = 2.36313\n",
            "epoch no.1 train no.78120  loss = 2.94081 avg_loss = 2.35943\n",
            "epoch no.1 train no.78130  loss = 2.21153 avg_loss = 2.34510\n",
            "epoch no.1 train no.78140  loss = 1.64577 avg_loss = 2.33907\n",
            "epoch no.1 train no.78150  loss = 2.41683 avg_loss = 2.35413\n",
            "epoch no.1 train no.78160  loss = 2.55656 avg_loss = 2.36059\n",
            "epoch no.1 train no.78170  loss = 1.73788 avg_loss = 2.34841\n",
            "epoch no.1 train no.78180  loss = 2.75337 avg_loss = 2.35502\n",
            "epoch no.1 train no.78190  loss = 2.40971 avg_loss = 2.36166\n",
            "epoch no.1 train no.78200  loss = 2.92038 avg_loss = 2.36573\n",
            "epoch no.1 train no.78210  loss = 1.95466 avg_loss = 2.35135\n",
            "epoch no.1 train no.78220  loss = 2.69262 avg_loss = 2.35565\n",
            "epoch no.1 train no.78230  loss = 1.55932 avg_loss = 2.34079\n",
            "epoch no.1 train no.78240  loss = 2.03883 avg_loss = 2.35294\n",
            "epoch no.1 train no.78250  loss = 1.17192 avg_loss = 2.34601\n",
            "epoch no.1 train no.78260  loss = 2.29081 avg_loss = 2.32423\n",
            "epoch no.1 train no.78270  loss = 2.07478 avg_loss = 2.30798\n",
            "epoch no.1 train no.78280  loss = 2.20864 avg_loss = 2.30475\n",
            "epoch no.1 train no.78290  loss = 1.99232 avg_loss = 2.31401\n",
            "epoch no.1 train no.78300  loss = 1.68659 avg_loss = 2.29461\n",
            "epoch no.1 train no.78310  loss = 3.26259 avg_loss = 2.31716\n",
            "epoch no.1 train no.78320  loss = 3.52959 avg_loss = 2.32690\n",
            "epoch no.1 train no.78330  loss = 2.41627 avg_loss = 2.31388\n",
            "epoch no.1 train no.78340  loss = 2.50537 avg_loss = 2.31057\n",
            "epoch no.1 train no.78350  loss = 2.28666 avg_loss = 2.28773\n",
            "epoch no.1 train no.78360  loss = 2.69968 avg_loss = 2.29935\n",
            "epoch no.1 train no.78370  loss = 3.01517 avg_loss = 2.31752\n",
            "epoch no.1 train no.78380  loss = 2.80441 avg_loss = 2.33349\n",
            "epoch no.1 train no.78390  loss = 1.56678 avg_loss = 2.32551\n",
            "epoch no.1 train no.78400  loss = 1.65185 avg_loss = 2.35062\n",
            "epoch no.1 train no.78410  loss = 2.36865 avg_loss = 2.31611\n",
            "epoch no.1 train no.78420  loss = 2.16928 avg_loss = 2.33034\n",
            "epoch no.1 train no.78430  loss = 3.32834 avg_loss = 2.33311\n",
            "epoch no.1 train no.78440  loss = 3.64190 avg_loss = 2.34927\n",
            "epoch no.1 train no.78450  loss = 2.73306 avg_loss = 2.35708\n",
            "epoch no.1 train no.78460  loss = 2.79929 avg_loss = 2.36943\n",
            "epoch no.1 train no.78470  loss = 1.89373 avg_loss = 2.37070\n",
            "epoch no.1 train no.78480  loss = 1.65304 avg_loss = 2.36803\n",
            "epoch no.1 train no.78490  loss = 2.35721 avg_loss = 2.34404\n",
            "epoch no.1 train no.78500  loss = 2.37234 avg_loss = 2.35168\n",
            "epoch no.1 train no.78510  loss = 2.75536 avg_loss = 2.37489\n",
            "epoch no.1 train no.78520  loss = 2.56391 avg_loss = 2.35391\n",
            "epoch no.1 train no.78530  loss = 2.17405 avg_loss = 2.36385\n",
            "epoch no.1 train no.78540  loss = 2.45162 avg_loss = 2.37094\n",
            "epoch no.1 train no.78550  loss = 2.10866 avg_loss = 2.35183\n",
            "epoch no.1 train no.78560  loss = 2.04100 avg_loss = 2.37857\n",
            "epoch no.1 train no.78570  loss = 2.17488 avg_loss = 2.38200\n",
            "epoch no.1 train no.78580  loss = 2.29483 avg_loss = 2.35330\n",
            "epoch no.1 train no.78590  loss = 2.21647 avg_loss = 2.33004\n",
            "epoch no.1 train no.78600  loss = 2.55820 avg_loss = 2.32871\n",
            "epoch no.1 train no.78610  loss = 1.43015 avg_loss = 2.33180\n",
            "epoch no.1 train no.78620  loss = 1.76791 avg_loss = 2.34536\n",
            "epoch no.1 train no.78630  loss = 2.65587 avg_loss = 2.34001\n",
            "epoch no.1 train no.78640  loss = 2.96392 avg_loss = 2.35376\n",
            "epoch no.1 train no.78650  loss = 2.68554 avg_loss = 2.34069\n",
            "epoch no.1 train no.78660  loss = 2.58093 avg_loss = 2.35769\n",
            "epoch no.1 train no.78670  loss = 2.10407 avg_loss = 2.33568\n",
            "epoch no.1 train no.78680  loss = 2.72959 avg_loss = 2.36170\n",
            "epoch no.1 train no.78690  loss = 0.84358 avg_loss = 2.34211\n",
            "epoch no.1 train no.78700  loss = 2.76616 avg_loss = 2.35264\n",
            "epoch no.1 train no.78710  loss = 3.37130 avg_loss = 2.35230\n",
            "epoch no.1 train no.78720  loss = 1.84243 avg_loss = 2.33705\n",
            "epoch no.1 train no.78730  loss = 2.44025 avg_loss = 2.33295\n",
            "epoch no.1 train no.78740  loss = 2.54323 avg_loss = 2.33472\n",
            "epoch no.1 train no.78750  loss = 2.29914 avg_loss = 2.34644\n",
            "epoch no.1 train no.78760  loss = 2.38987 avg_loss = 2.35482\n",
            "epoch no.1 train no.78770  loss = 2.02671 avg_loss = 2.36757\n",
            "epoch no.1 train no.78780  loss = 2.19967 avg_loss = 2.35038\n",
            "epoch no.1 train no.78790  loss = 2.21401 avg_loss = 2.35182\n",
            "epoch no.1 train no.78800  loss = 2.67611 avg_loss = 2.36482\n",
            "epoch no.1 train no.78810  loss = 2.54369 avg_loss = 2.37081\n",
            "epoch no.1 train no.78820  loss = 1.99137 avg_loss = 2.35934\n",
            "epoch no.1 train no.78830  loss = 2.87796 avg_loss = 2.37469\n",
            "epoch no.1 train no.78840  loss = 1.82177 avg_loss = 2.37794\n",
            "epoch no.1 train no.78850  loss = 1.98506 avg_loss = 2.38530\n",
            "epoch no.1 train no.78860  loss = 2.45409 avg_loss = 2.38466\n",
            "epoch no.1 train no.78870  loss = 1.32983 avg_loss = 2.38196\n",
            "epoch no.1 train no.78880  loss = 2.09405 avg_loss = 2.37461\n",
            "epoch no.1 train no.78890  loss = 2.14734 avg_loss = 2.35873\n",
            "epoch no.1 train no.78900  loss = 3.20859 avg_loss = 2.37767\n",
            "epoch no.1 train no.78910  loss = 2.20586 avg_loss = 2.37078\n",
            "epoch no.1 train no.78920  loss = 1.61476 avg_loss = 2.34532\n",
            "epoch no.1 train no.78930  loss = 2.19922 avg_loss = 2.38078\n",
            "epoch no.1 train no.78940  loss = 2.35633 avg_loss = 2.37342\n",
            "epoch no.1 train no.78950  loss = 2.67047 avg_loss = 2.41608\n",
            "epoch no.1 train no.78960  loss = 1.80015 avg_loss = 2.41397\n",
            "epoch no.1 train no.78970  loss = 2.21996 avg_loss = 2.41408\n",
            "epoch no.1 train no.78980  loss = 2.24771 avg_loss = 2.38594\n",
            "epoch no.1 train no.78990  loss = 2.46093 avg_loss = 2.40399\n",
            "epoch no.1 train no.79000  loss = 2.00368 avg_loss = 2.39026\n",
            "epoch no.1 train no.79010  loss = 1.60304 avg_loss = 2.38885\n",
            "epoch no.1 train no.79020  loss = 1.91643 avg_loss = 2.39973\n",
            "epoch no.1 train no.79030  loss = 2.39347 avg_loss = 2.39269\n",
            "epoch no.1 train no.79040  loss = 3.98983 avg_loss = 2.43260\n",
            "epoch no.1 train no.79050  loss = 2.05550 avg_loss = 2.41663\n",
            "epoch no.1 train no.79060  loss = 2.47333 avg_loss = 2.41773\n",
            "epoch no.1 train no.79070  loss = 3.07964 avg_loss = 2.40094\n",
            "epoch no.1 train no.79080  loss = 2.94052 avg_loss = 2.40005\n",
            "epoch no.1 train no.79090  loss = 2.98920 avg_loss = 2.39322\n",
            "epoch no.1 train no.79100  loss = 2.65806 avg_loss = 2.40713\n",
            "epoch no.1 train no.79110  loss = 2.72627 avg_loss = 2.41239\n",
            "epoch no.1 train no.79120  loss = 1.51578 avg_loss = 2.38902\n",
            "epoch no.1 train no.79130  loss = 2.31958 avg_loss = 2.33399\n",
            "epoch no.1 train no.79140  loss = 2.70487 avg_loss = 2.33187\n",
            "epoch no.1 train no.79150  loss = 2.57065 avg_loss = 2.32653\n",
            "epoch no.1 train no.79160  loss = 2.98668 avg_loss = 2.33714\n",
            "epoch no.1 train no.79170  loss = 2.73492 avg_loss = 2.35755\n",
            "epoch no.1 train no.79180  loss = 3.02592 avg_loss = 2.36124\n",
            "epoch no.1 train no.79190  loss = 2.03217 avg_loss = 2.38162\n",
            "epoch no.1 train no.79200  loss = 2.20156 avg_loss = 2.35365\n",
            "epoch no.1 train no.79210  loss = 1.86038 avg_loss = 2.32873\n",
            "epoch no.1 train no.79220  loss = 2.85538 avg_loss = 2.36108\n",
            "epoch no.1 train no.79230  loss = 1.93167 avg_loss = 2.36168\n",
            "epoch no.1 train no.79240  loss = 2.20776 avg_loss = 2.33584\n",
            "epoch no.1 train no.79250  loss = 1.85347 avg_loss = 2.35541\n",
            "epoch no.1 train no.79260  loss = 3.02597 avg_loss = 2.34060\n",
            "epoch no.1 train no.79270  loss = 2.11085 avg_loss = 2.34460\n",
            "epoch no.1 train no.79280  loss = 3.01655 avg_loss = 2.35624\n",
            "epoch no.1 train no.79290  loss = 2.41998 avg_loss = 2.36378\n",
            "epoch no.1 train no.79300  loss = 1.52343 avg_loss = 2.33459\n",
            "epoch no.1 train no.79310  loss = 2.21501 avg_loss = 2.34007\n",
            "epoch no.1 train no.79320  loss = 1.86590 avg_loss = 2.31404\n",
            "epoch no.1 train no.79330  loss = 2.73991 avg_loss = 2.32336\n",
            "epoch no.1 train no.79340  loss = 2.00274 avg_loss = 2.31945\n",
            "epoch no.1 train no.79350  loss = 2.54662 avg_loss = 2.31609\n",
            "epoch no.1 train no.79360  loss = 3.12691 avg_loss = 2.32384\n",
            "epoch no.1 train no.79370  loss = 2.70865 avg_loss = 2.34925\n",
            "epoch no.1 train no.79380  loss = 2.51581 avg_loss = 2.34897\n",
            "epoch no.1 train no.79390  loss = 2.02122 avg_loss = 2.35634\n",
            "epoch no.1 train no.79400  loss = 2.87954 avg_loss = 2.36235\n",
            "epoch no.1 train no.79410  loss = 2.80010 avg_loss = 2.37335\n",
            "epoch no.1 train no.79420  loss = 2.68823 avg_loss = 2.38590\n",
            "epoch no.1 train no.79430  loss = 3.13529 avg_loss = 2.37666\n",
            "epoch no.1 train no.79440  loss = 1.47130 avg_loss = 2.37109\n",
            "epoch no.1 train no.79450  loss = 1.71852 avg_loss = 2.33869\n",
            "epoch no.1 train no.79460  loss = 2.31547 avg_loss = 2.34427\n",
            "epoch no.1 train no.79470  loss = 2.24482 avg_loss = 2.34573\n",
            "epoch no.1 train no.79480  loss = 2.27508 avg_loss = 2.34435\n",
            "epoch no.1 train no.79490  loss = 2.34558 avg_loss = 2.32651\n",
            "epoch no.1 train no.79500  loss = 2.07117 avg_loss = 2.32641\n",
            "epoch no.1 train no.79510  loss = 2.70291 avg_loss = 2.35150\n",
            "epoch no.1 train no.79520  loss = 2.67986 avg_loss = 2.32877\n",
            "epoch no.1 train no.79530  loss = 2.39946 avg_loss = 2.32815\n",
            "epoch no.1 train no.79540  loss = 2.53446 avg_loss = 2.32750\n",
            "epoch no.1 train no.79550  loss = 2.95573 avg_loss = 2.33858\n",
            "epoch no.1 train no.79560  loss = 2.25136 avg_loss = 2.33196\n",
            "epoch no.1 train no.79570  loss = 2.98896 avg_loss = 2.33297\n",
            "epoch no.1 train no.79580  loss = 2.09710 avg_loss = 2.33376\n",
            "epoch no.1 train no.79590  loss = 1.71930 avg_loss = 2.32287\n",
            "epoch no.1 train no.79600  loss = 1.48969 avg_loss = 2.29460\n",
            "epoch no.1 train no.79610  loss = 3.01233 avg_loss = 2.29629\n",
            "epoch no.1 train no.79620  loss = 2.75257 avg_loss = 2.32328\n",
            "epoch no.1 train no.79630  loss = 2.70891 avg_loss = 2.32138\n",
            "epoch no.1 train no.79640  loss = 2.63074 avg_loss = 2.31897\n",
            "epoch no.1 train no.79650  loss = 1.94128 avg_loss = 2.32590\n",
            "epoch no.1 train no.79660  loss = 2.43151 avg_loss = 2.33718\n",
            "epoch no.1 train no.79670  loss = 3.09829 avg_loss = 2.35314\n",
            "epoch no.1 train no.79680  loss = 2.83484 avg_loss = 2.36063\n",
            "epoch no.1 train no.79690  loss = 2.35603 avg_loss = 2.37813\n",
            "epoch no.1 train no.79700  loss = 2.32563 avg_loss = 2.40188\n",
            "epoch no.1 train no.79710  loss = 1.49778 avg_loss = 2.35427\n",
            "epoch no.1 train no.79720  loss = 3.34566 avg_loss = 2.37076\n",
            "epoch no.1 train no.79730  loss = 2.61433 avg_loss = 2.36653\n",
            "epoch no.1 train no.79740  loss = 2.49966 avg_loss = 2.38082\n",
            "epoch no.1 train no.79750  loss = 2.42082 avg_loss = 2.38002\n",
            "epoch no.1 train no.79760  loss = 1.19067 avg_loss = 2.36093\n",
            "epoch no.1 train no.79770  loss = 2.79096 avg_loss = 2.37769\n",
            "epoch no.1 train no.79780  loss = 1.84002 avg_loss = 2.37797\n",
            "epoch no.1 train no.79790  loss = 2.28600 avg_loss = 2.36246\n",
            "epoch no.1 train no.79800  loss = 1.98060 avg_loss = 2.36169\n",
            "epoch no.1 train no.79810  loss = 1.99582 avg_loss = 2.36738\n",
            "epoch no.1 train no.79820  loss = 3.33113 avg_loss = 2.37100\n",
            "epoch no.1 train no.79830  loss = 3.50943 avg_loss = 2.36077\n",
            "epoch no.1 train no.79840  loss = 2.89217 avg_loss = 2.33878\n",
            "epoch no.1 train no.79850  loss = 2.60420 avg_loss = 2.34095\n",
            "epoch no.1 train no.79860  loss = 2.52119 avg_loss = 2.30519\n",
            "epoch no.1 train no.79870  loss = 1.74896 avg_loss = 2.30800\n",
            "epoch no.1 train no.79880  loss = 2.58057 avg_loss = 2.31949\n",
            "epoch no.1 train no.79890  loss = 2.09496 avg_loss = 2.32851\n",
            "epoch no.1 train no.79900  loss = 2.02854 avg_loss = 2.31525\n",
            "epoch no.1 train no.79910  loss = 1.33405 avg_loss = 2.29103\n",
            "epoch no.1 train no.79920  loss = 1.92834 avg_loss = 2.27683\n",
            "epoch no.1 train no.79930  loss = 2.87427 avg_loss = 2.30561\n",
            "epoch no.1 train no.79940  loss = 2.10222 avg_loss = 2.31639\n",
            "epoch no.1 train no.79950  loss = 2.51801 avg_loss = 2.30632\n",
            "epoch no.1 train no.79960  loss = 2.01267 avg_loss = 2.30391\n",
            "epoch no.1 train no.79970  loss = 1.84965 avg_loss = 2.30701\n",
            "epoch no.1 train no.79980  loss = 2.36213 avg_loss = 2.29185\n",
            "epoch no.1 train no.79990  loss = 2.05626 avg_loss = 2.30384\n",
            "epoch no.1 train no.80000  loss = 2.34201 avg_loss = 2.31092\n",
            "epoch no.1 train no.80010  loss = 1.69770 avg_loss = 2.29614\n",
            "epoch no.1 train no.80020  loss = 1.88644 avg_loss = 2.31478\n",
            "epoch no.1 train no.80030  loss = 2.66228 avg_loss = 2.33554\n",
            "epoch no.1 train no.80040  loss = 1.39819 avg_loss = 2.33243\n",
            "epoch no.1 train no.80050  loss = 2.28663 avg_loss = 2.30663\n",
            "epoch no.1 train no.80060  loss = 2.39951 avg_loss = 2.32470\n",
            "epoch no.1 train no.80070  loss = 2.73272 avg_loss = 2.34271\n",
            "epoch no.1 train no.80080  loss = 2.47219 avg_loss = 2.35050\n",
            "epoch no.1 train no.80090  loss = 1.84947 avg_loss = 2.34820\n",
            "epoch no.1 train no.80100  loss = 1.53290 avg_loss = 2.35837\n",
            "epoch no.1 train no.80110  loss = 1.49977 avg_loss = 2.34520\n",
            "epoch no.1 train no.80120  loss = 2.66196 avg_loss = 2.34589\n",
            "epoch no.1 train no.80130  loss = 2.34557 avg_loss = 2.36701\n",
            "epoch no.1 train no.80140  loss = 3.35777 avg_loss = 2.36432\n",
            "epoch no.1 train no.80150  loss = 1.29992 avg_loss = 2.37045\n",
            "epoch no.1 train no.80160  loss = 2.28801 avg_loss = 2.35638\n",
            "epoch no.1 train no.80170  loss = 2.14124 avg_loss = 2.35066\n",
            "epoch no.1 train no.80180  loss = 1.94373 avg_loss = 2.35814\n",
            "epoch no.1 train no.80190  loss = 3.94987 avg_loss = 2.37672\n",
            "epoch no.1 train no.80200  loss = 2.01228 avg_loss = 2.36580\n",
            "epoch no.1 train no.80210  loss = 1.88596 avg_loss = 2.36457\n",
            "epoch no.1 train no.80220  loss = 2.35076 avg_loss = 2.37534\n",
            "epoch no.1 train no.80230  loss = 2.42114 avg_loss = 2.37022\n",
            "epoch no.1 train no.80240  loss = 1.38735 avg_loss = 2.36993\n",
            "epoch no.1 train no.80250  loss = 2.34600 avg_loss = 2.36485\n",
            "epoch no.1 train no.80260  loss = 1.76747 avg_loss = 2.35055\n",
            "epoch no.1 train no.80270  loss = 2.13921 avg_loss = 2.35064\n",
            "epoch no.1 train no.80280  loss = 2.88058 avg_loss = 2.36395\n",
            "epoch no.1 train no.80290  loss = 2.97717 avg_loss = 2.37278\n",
            "epoch no.1 train no.80300  loss = 2.24978 avg_loss = 2.36170\n",
            "epoch no.1 train no.80310  loss = 2.12393 avg_loss = 2.35181\n",
            "epoch no.1 train no.80320  loss = 1.72991 avg_loss = 2.35050\n",
            "epoch no.1 train no.80330  loss = 2.60082 avg_loss = 2.34724\n",
            "epoch no.1 train no.80340  loss = 2.82887 avg_loss = 2.32908\n",
            "epoch no.1 train no.80350  loss = 2.23751 avg_loss = 2.33058\n",
            "epoch no.1 train no.80360  loss = 2.99998 avg_loss = 2.32720\n",
            "epoch no.1 train no.80370  loss = 1.62959 avg_loss = 2.35242\n",
            "epoch no.1 train no.80380  loss = 3.00755 avg_loss = 2.33497\n",
            "epoch no.1 train no.80390  loss = 3.38083 avg_loss = 2.36429\n",
            "epoch no.1 train no.80400  loss = 2.45589 avg_loss = 2.34958\n",
            "epoch no.1 train no.80410  loss = 3.03277 avg_loss = 2.33479\n",
            "epoch no.1 train no.80420  loss = 2.42124 avg_loss = 2.37258\n",
            "epoch no.1 train no.80430  loss = 2.63197 avg_loss = 2.37773\n",
            "epoch no.1 train no.80440  loss = 1.63897 avg_loss = 2.35753\n",
            "epoch no.1 train no.80450  loss = 2.31301 avg_loss = 2.33285\n",
            "epoch no.1 train no.80460  loss = 2.01927 avg_loss = 2.31346\n",
            "epoch no.1 train no.80470  loss = 1.83704 avg_loss = 2.30924\n",
            "epoch no.1 train no.80480  loss = 2.27134 avg_loss = 2.31980\n",
            "epoch no.1 train no.80490  loss = 1.51442 avg_loss = 2.32226\n",
            "epoch no.1 train no.80500  loss = 2.05360 avg_loss = 2.31747\n",
            "epoch no.1 train no.80510  loss = 2.76303 avg_loss = 2.33907\n",
            "epoch no.1 train no.80520  loss = 3.25350 avg_loss = 2.36108\n",
            "epoch no.1 train no.80530  loss = 2.74158 avg_loss = 2.36152\n",
            "epoch no.1 train no.80540  loss = 1.78423 avg_loss = 2.35887\n",
            "epoch no.1 train no.80550  loss = 1.97136 avg_loss = 2.32517\n",
            "epoch no.1 train no.80560  loss = 2.64175 avg_loss = 2.31238\n",
            "epoch no.1 train no.80570  loss = 2.32344 avg_loss = 2.35392\n",
            "epoch no.1 train no.80580  loss = 2.85653 avg_loss = 2.35183\n",
            "epoch no.1 train no.80590  loss = 2.76790 avg_loss = 2.35678\n",
            "epoch no.1 train no.80600  loss = 1.38935 avg_loss = 2.36047\n",
            "epoch no.1 train no.80610  loss = 2.60201 avg_loss = 2.35179\n",
            "epoch no.1 train no.80620  loss = 1.07931 avg_loss = 2.33595\n",
            "epoch no.1 train no.80630  loss = 2.09316 avg_loss = 2.34092\n",
            "epoch no.1 train no.80640  loss = 3.03545 avg_loss = 2.36741\n",
            "epoch no.1 train no.80650  loss = 2.88292 avg_loss = 2.37965\n",
            "epoch no.1 train no.80660  loss = 1.73295 avg_loss = 2.37395\n",
            "epoch no.1 train no.80670  loss = 2.11926 avg_loss = 2.34439\n",
            "epoch no.1 train no.80680  loss = 2.14039 avg_loss = 2.34922\n",
            "epoch no.1 train no.80690  loss = 2.07629 avg_loss = 2.32410\n",
            "epoch no.1 train no.80700  loss = 1.62855 avg_loss = 2.30183\n",
            "epoch no.1 train no.80710  loss = 1.49564 avg_loss = 2.30925\n",
            "epoch no.1 train no.80720  loss = 2.57469 avg_loss = 2.30015\n",
            "epoch no.1 train no.80730  loss = 2.64383 avg_loss = 2.28127\n",
            "epoch no.1 train no.80740  loss = 2.35308 avg_loss = 2.28712\n",
            "epoch no.1 train no.80750  loss = 1.99644 avg_loss = 2.29755\n",
            "epoch no.1 train no.80760  loss = 3.58095 avg_loss = 2.31625\n",
            "epoch no.1 train no.80770  loss = 1.53180 avg_loss = 2.31062\n",
            "epoch no.1 train no.80780  loss = 2.66108 avg_loss = 2.30595\n",
            "epoch no.1 train no.80790  loss = 3.07543 avg_loss = 2.32269\n",
            "epoch no.1 train no.80800  loss = 1.45208 avg_loss = 2.31571\n",
            "epoch no.1 train no.80810  loss = 2.17382 avg_loss = 2.32670\n",
            "epoch no.1 train no.80820  loss = 2.73954 avg_loss = 2.34310\n",
            "epoch no.1 train no.80830  loss = 2.03339 avg_loss = 2.35131\n",
            "epoch no.1 train no.80840  loss = 2.27547 avg_loss = 2.34972\n",
            "epoch no.1 train no.80850  loss = 2.62779 avg_loss = 2.34291\n",
            "epoch no.1 train no.80860  loss = 2.72908 avg_loss = 2.34693\n",
            "epoch no.1 train no.80870  loss = 4.14255 avg_loss = 2.35061\n",
            "epoch no.1 train no.80880  loss = 1.64045 avg_loss = 2.33963\n",
            "epoch no.1 train no.80890  loss = 2.53681 avg_loss = 2.36981\n",
            "epoch no.1 train no.80900  loss = 1.90160 avg_loss = 2.34195\n",
            "epoch no.1 train no.80910  loss = 1.78199 avg_loss = 2.36113\n",
            "epoch no.1 train no.80920  loss = 2.18217 avg_loss = 2.35574\n",
            "epoch no.1 train no.80930  loss = 1.19024 avg_loss = 2.35672\n",
            "epoch no.1 train no.80940  loss = 1.94306 avg_loss = 2.35935\n",
            "epoch no.1 train no.80950  loss = 2.77606 avg_loss = 2.37158\n",
            "epoch no.1 train no.80960  loss = 2.57074 avg_loss = 2.38385\n",
            "epoch no.1 train no.80970  loss = 2.34320 avg_loss = 2.36688\n",
            "epoch no.1 train no.80980  loss = 2.77512 avg_loss = 2.35804\n",
            "epoch no.1 train no.80990  loss = 2.79402 avg_loss = 2.34686\n",
            "epoch no.1 train no.81000  loss = 2.58961 avg_loss = 2.34792\n",
            "epoch no.1 train no.81010  loss = 3.67701 avg_loss = 2.34743\n",
            "epoch no.1 train no.81020  loss = 2.33413 avg_loss = 2.36353\n",
            "epoch no.1 train no.81030  loss = 3.27991 avg_loss = 2.37467\n",
            "epoch no.1 train no.81040  loss = 1.71306 avg_loss = 2.35781\n",
            "epoch no.1 train no.81050  loss = 2.86092 avg_loss = 2.36037\n",
            "epoch no.1 train no.81060  loss = 2.03864 avg_loss = 2.34710\n",
            "epoch no.1 train no.81070  loss = 2.42665 avg_loss = 2.34124\n",
            "epoch no.1 train no.81080  loss = 2.35545 avg_loss = 2.32845\n",
            "epoch no.1 train no.81090  loss = 2.98212 avg_loss = 2.33942\n",
            "epoch no.1 train no.81100  loss = 2.22660 avg_loss = 2.32610\n",
            "epoch no.1 train no.81110  loss = 2.26589 avg_loss = 2.34781\n",
            "epoch no.1 train no.81120  loss = 2.58503 avg_loss = 2.37185\n",
            "epoch no.1 train no.81130  loss = 2.74823 avg_loss = 2.36927\n",
            "epoch no.1 train no.81140  loss = 2.14861 avg_loss = 2.34070\n",
            "epoch no.1 train no.81150  loss = 2.24800 avg_loss = 2.34067\n",
            "epoch no.1 train no.81160  loss = 3.88351 avg_loss = 2.33505\n",
            "epoch no.1 train no.81170  loss = 2.34659 avg_loss = 2.32644\n",
            "epoch no.1 train no.81180  loss = 1.74270 avg_loss = 2.32012\n",
            "epoch no.1 train no.81190  loss = 3.20187 avg_loss = 2.33188\n",
            "epoch no.1 train no.81200  loss = 2.44570 avg_loss = 2.32070\n",
            "epoch no.1 train no.81210  loss = 2.12491 avg_loss = 2.30303\n",
            "epoch no.1 train no.81220  loss = 1.91929 avg_loss = 2.29244\n",
            "epoch no.1 train no.81230  loss = 2.54910 avg_loss = 2.30036\n",
            "epoch no.1 train no.81240  loss = 2.69755 avg_loss = 2.29696\n",
            "epoch no.1 train no.81250  loss = 2.41800 avg_loss = 2.29677\n",
            "epoch no.1 train no.81260  loss = 2.54749 avg_loss = 2.29163\n",
            "epoch no.1 train no.81270  loss = 1.47243 avg_loss = 2.30684\n",
            "epoch no.1 train no.81280  loss = 2.40811 avg_loss = 2.29293\n",
            "epoch no.1 train no.81290  loss = 2.79532 avg_loss = 2.28299\n",
            "epoch no.1 train no.81300  loss = 2.36511 avg_loss = 2.28099\n",
            "epoch no.1 train no.81310  loss = 1.77759 avg_loss = 2.29257\n",
            "epoch no.1 train no.81320  loss = 2.39909 avg_loss = 2.28579\n",
            "epoch no.1 train no.81330  loss = 1.46572 avg_loss = 2.27529\n",
            "epoch no.1 train no.81340  loss = 1.44315 avg_loss = 2.27126\n",
            "epoch no.1 train no.81350  loss = 2.73322 avg_loss = 2.29744\n",
            "epoch no.1 train no.81360  loss = 2.48940 avg_loss = 2.28848\n",
            "epoch no.1 train no.81370  loss = 2.14287 avg_loss = 2.28515\n",
            "epoch no.1 train no.81380  loss = 2.10944 avg_loss = 2.30849\n",
            "epoch no.1 train no.81390  loss = 2.36335 avg_loss = 2.32428\n",
            "epoch no.1 train no.81400  loss = 3.02637 avg_loss = 2.31925\n",
            "epoch no.1 train no.81410  loss = 2.35374 avg_loss = 2.32121\n",
            "epoch no.1 train no.81420  loss = 2.17124 avg_loss = 2.31724\n",
            "epoch no.1 train no.81430  loss = 2.53188 avg_loss = 2.33381\n",
            "epoch no.1 train no.81440  loss = 1.99791 avg_loss = 2.36047\n",
            "epoch no.1 train no.81450  loss = 1.80964 avg_loss = 2.35753\n",
            "epoch no.1 train no.81460  loss = 1.81242 avg_loss = 2.32332\n",
            "epoch no.1 train no.81470  loss = 2.15803 avg_loss = 2.30687\n",
            "epoch no.1 train no.81480  loss = 2.12674 avg_loss = 2.30093\n",
            "epoch no.1 train no.81490  loss = 2.51020 avg_loss = 2.32601\n",
            "epoch no.1 train no.81500  loss = 1.91881 avg_loss = 2.33318\n",
            "epoch no.1 train no.81510  loss = 2.41367 avg_loss = 2.31670\n",
            "epoch no.1 train no.81520  loss = 3.01692 avg_loss = 2.33213\n",
            "epoch no.1 train no.81530  loss = 2.94906 avg_loss = 2.32876\n",
            "epoch no.1 train no.81540  loss = 2.13914 avg_loss = 2.33765\n",
            "epoch no.1 train no.81550  loss = 2.68455 avg_loss = 2.33928\n",
            "epoch no.1 train no.81560  loss = 2.77516 avg_loss = 2.34357\n",
            "epoch no.1 train no.81570  loss = 3.02180 avg_loss = 2.36502\n",
            "epoch no.1 train no.81580  loss = 2.65441 avg_loss = 2.35534\n",
            "epoch no.1 train no.81590  loss = 1.98404 avg_loss = 2.32059\n",
            "epoch no.1 train no.81600  loss = 2.71887 avg_loss = 2.31632\n",
            "epoch no.1 train no.81610  loss = 2.27671 avg_loss = 2.34280\n",
            "epoch no.1 train no.81620  loss = 3.52783 avg_loss = 2.35177\n",
            "epoch no.1 train no.81630  loss = 2.57640 avg_loss = 2.33772\n",
            "epoch no.1 train no.81640  loss = 2.52053 avg_loss = 2.33735\n",
            "epoch no.1 train no.81650  loss = 2.46665 avg_loss = 2.33473\n",
            "epoch no.1 train no.81660  loss = 2.57202 avg_loss = 2.31163\n",
            "epoch no.1 train no.81670  loss = 2.62993 avg_loss = 2.31819\n",
            "epoch no.1 train no.81680  loss = 2.63841 avg_loss = 2.34331\n",
            "epoch no.1 train no.81690  loss = 2.79379 avg_loss = 2.34801\n",
            "epoch no.1 train no.81700  loss = 1.66839 avg_loss = 2.34907\n",
            "epoch no.1 train no.81710  loss = 2.57545 avg_loss = 2.31896\n",
            "epoch no.1 train no.81720  loss = 1.46577 avg_loss = 2.31682\n",
            "epoch no.1 train no.81730  loss = 2.43759 avg_loss = 2.32675\n",
            "epoch no.1 train no.81740  loss = 2.82031 avg_loss = 2.30585\n",
            "epoch no.1 train no.81750  loss = 2.28487 avg_loss = 2.30384\n",
            "epoch no.1 train no.81760  loss = 2.18025 avg_loss = 2.30048\n",
            "epoch no.1 train no.81770  loss = 1.53637 avg_loss = 2.27495\n",
            "epoch no.1 train no.81780  loss = 2.54711 avg_loss = 2.28267\n",
            "epoch no.1 train no.81790  loss = 2.78877 avg_loss = 2.29767\n",
            "epoch no.1 train no.81800  loss = 2.47820 avg_loss = 2.29907\n",
            "epoch no.1 train no.81810  loss = 2.10827 avg_loss = 2.30112\n",
            "epoch no.1 train no.81820  loss = 3.01308 avg_loss = 2.29985\n",
            "epoch no.1 train no.81830  loss = 1.88916 avg_loss = 2.29382\n",
            "epoch no.1 train no.81840  loss = 2.70935 avg_loss = 2.29094\n",
            "epoch no.1 train no.81850  loss = 2.16212 avg_loss = 2.30304\n",
            "epoch no.1 train no.81860  loss = 1.67581 avg_loss = 2.29155\n",
            "epoch no.1 train no.81870  loss = 2.27159 avg_loss = 2.28699\n",
            "epoch no.1 train no.81880  loss = 1.97849 avg_loss = 2.29787\n",
            "epoch no.1 train no.81890  loss = 2.91088 avg_loss = 2.29423\n",
            "epoch no.1 train no.81900  loss = 2.05841 avg_loss = 2.28348\n",
            "epoch no.1 train no.81910  loss = 2.57471 avg_loss = 2.29382\n",
            "epoch no.1 train no.81920  loss = 1.88364 avg_loss = 2.29759\n",
            "epoch no.1 train no.81930  loss = 2.96280 avg_loss = 2.30651\n",
            "epoch no.1 train no.81940  loss = 2.24161 avg_loss = 2.27850\n",
            "epoch no.1 train no.81950  loss = 1.65377 avg_loss = 2.27655\n",
            "epoch no.1 train no.81960  loss = 2.71510 avg_loss = 2.28145\n",
            "epoch no.1 train no.81970  loss = 2.77108 avg_loss = 2.28176\n",
            "epoch no.1 train no.81980  loss = 2.20512 avg_loss = 2.27811\n",
            "epoch no.1 train no.81990  loss = 2.19677 avg_loss = 2.28129\n",
            "epoch no.1 train no.82000  loss = 2.98315 avg_loss = 2.30738\n",
            "epoch no.1 train no.82010  loss = 2.50295 avg_loss = 2.32112\n",
            "epoch no.1 train no.82020  loss = 1.71272 avg_loss = 2.31157\n",
            "epoch no.1 train no.82030  loss = 2.35382 avg_loss = 2.31108\n",
            "epoch no.1 train no.82040  loss = 2.81255 avg_loss = 2.35118\n",
            "epoch no.1 train no.82050  loss = 2.59835 avg_loss = 2.36090\n",
            "epoch no.1 train no.82060  loss = 2.94825 avg_loss = 2.37943\n",
            "epoch no.1 train no.82070  loss = 2.17945 avg_loss = 2.37884\n",
            "epoch no.1 train no.82080  loss = 2.79593 avg_loss = 2.39835\n",
            "epoch no.1 train no.82090  loss = 2.64398 avg_loss = 2.38466\n",
            "epoch no.1 train no.82100  loss = 3.21930 avg_loss = 2.38529\n",
            "epoch no.1 train no.82110  loss = 2.55472 avg_loss = 2.37296\n",
            "epoch no.1 train no.82120  loss = 2.55391 avg_loss = 2.39485\n",
            "epoch no.1 train no.82130  loss = 2.11770 avg_loss = 2.38017\n",
            "epoch no.1 train no.82140  loss = 3.06232 avg_loss = 2.36055\n",
            "epoch no.1 train no.82150  loss = 2.47022 avg_loss = 2.37078\n",
            "epoch no.1 train no.82160  loss = 1.68679 avg_loss = 2.35740\n",
            "epoch no.1 train no.82170  loss = 2.34765 avg_loss = 2.36516\n",
            "epoch no.1 train no.82180  loss = 1.33458 avg_loss = 2.37173\n",
            "epoch no.1 train no.82190  loss = 2.75829 avg_loss = 2.38841\n",
            "epoch no.1 train no.82200  loss = 1.97574 avg_loss = 2.36678\n",
            "epoch no.1 train no.82210  loss = 2.55782 avg_loss = 2.37106\n",
            "epoch no.1 train no.82220  loss = 2.62366 avg_loss = 2.35286\n",
            "epoch no.1 train no.82230  loss = 2.15103 avg_loss = 2.36450\n",
            "epoch no.1 train no.82240  loss = 2.24601 avg_loss = 2.35093\n",
            "epoch no.1 train no.82250  loss = 2.47366 avg_loss = 2.34741\n",
            "epoch no.1 train no.82260  loss = 1.77894 avg_loss = 2.32881\n",
            "epoch no.1 train no.82270  loss = 2.48443 avg_loss = 2.30921\n",
            "epoch no.1 train no.82280  loss = 2.13835 avg_loss = 2.31231\n",
            "epoch no.1 train no.82290  loss = 2.13619 avg_loss = 2.30036\n",
            "epoch no.1 train no.82300  loss = 2.83435 avg_loss = 2.29580\n",
            "epoch no.1 train no.82310  loss = 3.14787 avg_loss = 2.31491\n",
            "epoch no.1 train no.82320  loss = 3.47417 avg_loss = 2.32033\n",
            "epoch no.1 train no.82330  loss = 2.25340 avg_loss = 2.31360\n",
            "epoch no.1 train no.82340  loss = 2.94059 avg_loss = 2.31406\n",
            "epoch no.2 train no.82350  loss = 2.01302 avg_loss = 2.32158\n",
            "epoch no.2 train no.82360  loss = 2.82135 avg_loss = 2.32177\n",
            "epoch no.2 train no.82370  loss = 2.26042 avg_loss = 2.32721\n",
            "epoch no.2 train no.82380  loss = 1.98332 avg_loss = 2.31139\n",
            "epoch no.2 train no.82390  loss = 1.40077 avg_loss = 2.31389\n",
            "epoch no.2 train no.82400  loss = 2.08716 avg_loss = 2.34541\n",
            "epoch no.2 train no.82410  loss = 2.36899 avg_loss = 2.31762\n",
            "epoch no.2 train no.82420  loss = 1.68768 avg_loss = 2.31667\n",
            "epoch no.2 train no.82430  loss = 3.68915 avg_loss = 2.35077\n",
            "epoch no.2 train no.82440  loss = 2.14055 avg_loss = 2.33108\n",
            "epoch no.2 train no.82450  loss = 2.87202 avg_loss = 2.32476\n",
            "epoch no.2 train no.82460  loss = 1.22764 avg_loss = 2.30787\n",
            "epoch no.2 train no.82470  loss = 2.13019 avg_loss = 2.30195\n",
            "epoch no.2 train no.82480  loss = 2.68074 avg_loss = 2.29379\n",
            "epoch no.2 train no.82490  loss = 2.84309 avg_loss = 2.30404\n",
            "epoch no.2 train no.82500  loss = 2.46327 avg_loss = 2.29813\n",
            "epoch no.2 train no.82510  loss = 2.85241 avg_loss = 2.30611\n",
            "epoch no.2 train no.82520  loss = 1.24853 avg_loss = 2.31287\n",
            "epoch no.2 train no.82530  loss = 2.13493 avg_loss = 2.29906\n",
            "epoch no.2 train no.82540  loss = 2.35797 avg_loss = 2.30052\n",
            "epoch no.2 train no.82550  loss = 2.02961 avg_loss = 2.30946\n",
            "epoch no.2 train no.82560  loss = 2.12018 avg_loss = 2.29482\n",
            "epoch no.2 train no.82570  loss = 3.27301 avg_loss = 2.29378\n",
            "epoch no.2 train no.82580  loss = 1.84499 avg_loss = 2.30185\n",
            "epoch no.2 train no.82590  loss = 2.47362 avg_loss = 2.30444\n",
            "epoch no.2 train no.82600  loss = 2.74001 avg_loss = 2.29970\n",
            "epoch no.2 train no.82610  loss = 2.83310 avg_loss = 2.32580\n",
            "epoch no.2 train no.82620  loss = 2.54840 avg_loss = 2.33123\n",
            "epoch no.2 train no.82630  loss = 1.87438 avg_loss = 2.32619\n",
            "epoch no.2 train no.82640  loss = 2.61284 avg_loss = 2.32589\n",
            "epoch no.2 train no.82650  loss = 1.93473 avg_loss = 2.32094\n",
            "epoch no.2 train no.82660  loss = 1.48117 avg_loss = 2.35197\n",
            "epoch no.2 train no.82670  loss = 2.46136 avg_loss = 2.36553\n",
            "epoch no.2 train no.82680  loss = 2.05363 avg_loss = 2.35741\n",
            "epoch no.2 train no.82690  loss = 1.96363 avg_loss = 2.33511\n",
            "epoch no.2 train no.82700  loss = 2.08768 avg_loss = 2.32332\n",
            "epoch no.2 train no.82710  loss = 2.05023 avg_loss = 2.28569\n",
            "epoch no.2 train no.82720  loss = 2.30644 avg_loss = 2.30293\n",
            "epoch no.2 train no.82730  loss = 1.75388 avg_loss = 2.28634\n",
            "epoch no.2 train no.82740  loss = 1.82683 avg_loss = 2.28016\n",
            "epoch no.2 train no.82750  loss = 2.99649 avg_loss = 2.26750\n",
            "epoch no.2 train no.82760  loss = 2.61109 avg_loss = 2.25809\n",
            "epoch no.2 train no.82770  loss = 2.52494 avg_loss = 2.25672\n",
            "epoch no.2 train no.82780  loss = 1.92035 avg_loss = 2.25612\n",
            "epoch no.2 train no.82790  loss = 1.95014 avg_loss = 2.25928\n",
            "epoch no.2 train no.82800  loss = 2.47466 avg_loss = 2.24196\n",
            "epoch no.2 train no.82810  loss = 1.81082 avg_loss = 2.22195\n",
            "epoch no.2 train no.82820  loss = 2.26898 avg_loss = 2.22804\n",
            "epoch no.2 train no.82830  loss = 1.26024 avg_loss = 2.23664\n",
            "epoch no.2 train no.82840  loss = 1.96204 avg_loss = 2.24839\n",
            "epoch no.2 train no.82850  loss = 1.81963 avg_loss = 2.25170\n",
            "epoch no.2 train no.82860  loss = 2.53001 avg_loss = 2.26768\n",
            "epoch no.2 train no.82870  loss = 1.90380 avg_loss = 2.27138\n",
            "epoch no.2 train no.82880  loss = 2.16539 avg_loss = 2.29109\n",
            "epoch no.2 train no.82890  loss = 2.09102 avg_loss = 2.29206\n",
            "epoch no.2 train no.82900  loss = 1.89629 avg_loss = 2.27910\n",
            "epoch no.2 train no.82910  loss = 2.37290 avg_loss = 2.28846\n",
            "epoch no.2 train no.82920  loss = 2.62144 avg_loss = 2.28091\n",
            "epoch no.2 train no.82930  loss = 2.09613 avg_loss = 2.28033\n",
            "epoch no.2 train no.82940  loss = 0.95132 avg_loss = 2.26356\n",
            "epoch no.2 train no.82950  loss = 3.09521 avg_loss = 2.28431\n",
            "epoch no.2 train no.82960  loss = 2.00359 avg_loss = 2.28340\n",
            "epoch no.2 train no.82970  loss = 2.29183 avg_loss = 2.27574\n",
            "epoch no.2 train no.82980  loss = 2.05604 avg_loss = 2.25893\n",
            "epoch no.2 train no.82990  loss = 1.81431 avg_loss = 2.25261\n",
            "epoch no.2 train no.83000  loss = 2.27101 avg_loss = 2.24682\n",
            "epoch no.2 train no.83010  loss = 2.08157 avg_loss = 2.23547\n",
            "epoch no.2 train no.83020  loss = 2.46798 avg_loss = 2.23507\n",
            "epoch no.2 train no.83030  loss = 2.72649 avg_loss = 2.23091\n",
            "epoch no.2 train no.83040  loss = 2.66669 avg_loss = 2.25810\n",
            "epoch no.2 train no.83050  loss = 2.19873 avg_loss = 2.22704\n",
            "epoch no.2 train no.83060  loss = 2.58218 avg_loss = 2.25918\n",
            "epoch no.2 train no.83070  loss = 3.05368 avg_loss = 2.29059\n",
            "epoch no.2 train no.83080  loss = 1.81142 avg_loss = 2.29089\n",
            "epoch no.2 train no.83090  loss = 2.46018 avg_loss = 2.26813\n",
            "epoch no.2 train no.83100  loss = 2.32758 avg_loss = 2.28806\n",
            "epoch no.2 train no.83110  loss = 2.93093 avg_loss = 2.27388\n",
            "epoch no.2 train no.83120  loss = 1.90204 avg_loss = 2.27422\n",
            "epoch no.2 train no.83130  loss = 2.41636 avg_loss = 2.29963\n",
            "epoch no.2 train no.83140  loss = 2.35310 avg_loss = 2.28332\n",
            "epoch no.2 train no.83150  loss = 2.65142 avg_loss = 2.27903\n",
            "epoch no.2 train no.83160  loss = 3.21848 avg_loss = 2.29852\n",
            "epoch no.2 train no.83170  loss = 2.13770 avg_loss = 2.30543\n",
            "epoch no.2 train no.83180  loss = 2.74927 avg_loss = 2.30140\n",
            "epoch no.2 train no.83190  loss = 2.56318 avg_loss = 2.30611\n",
            "epoch no.2 train no.83200  loss = 0.94188 avg_loss = 2.30295\n",
            "epoch no.2 train no.83210  loss = 2.46240 avg_loss = 2.28064\n",
            "epoch no.2 train no.83220  loss = 2.49019 avg_loss = 2.30227\n",
            "epoch no.2 train no.83230  loss = 2.17627 avg_loss = 2.29521\n",
            "epoch no.2 train no.83240  loss = 2.15109 avg_loss = 2.28727\n",
            "epoch no.2 train no.83250  loss = 2.82556 avg_loss = 2.29854\n",
            "epoch no.2 train no.83260  loss = 2.54327 avg_loss = 2.31679\n",
            "epoch no.2 train no.83270  loss = 2.04423 avg_loss = 2.31428\n",
            "epoch no.2 train no.83280  loss = 2.14611 avg_loss = 2.31436\n",
            "epoch no.2 train no.83290  loss = 2.10280 avg_loss = 2.30465\n",
            "epoch no.2 train no.83300  loss = 2.34931 avg_loss = 2.28767\n",
            "epoch no.2 train no.83310  loss = 2.20558 avg_loss = 2.26954\n",
            "epoch no.2 train no.83320  loss = 2.29942 avg_loss = 2.25458\n",
            "epoch no.2 train no.83330  loss = 2.17638 avg_loss = 2.24732\n",
            "epoch no.2 train no.83340  loss = 2.06799 avg_loss = 2.26059\n",
            "epoch no.2 train no.83350  loss = 2.56063 avg_loss = 2.25979\n",
            "epoch no.2 train no.83360  loss = 2.44032 avg_loss = 2.26768\n",
            "epoch no.2 train no.83370  loss = 2.52529 avg_loss = 2.26920\n",
            "epoch no.2 train no.83380  loss = 3.06064 avg_loss = 2.26368\n",
            "epoch no.2 train no.83390  loss = 2.51471 avg_loss = 2.28244\n",
            "epoch no.2 train no.83400  loss = 2.60580 avg_loss = 2.26998\n",
            "epoch no.2 train no.83410  loss = 1.70801 avg_loss = 2.27345\n",
            "epoch no.2 train no.83420  loss = 2.16823 avg_loss = 2.27269\n",
            "epoch no.2 train no.83430  loss = 1.39674 avg_loss = 2.26089\n",
            "epoch no.2 train no.83440  loss = 2.21252 avg_loss = 2.23756\n",
            "epoch no.2 train no.83450  loss = 1.99949 avg_loss = 2.22265\n",
            "epoch no.2 train no.83460  loss = 2.48364 avg_loss = 2.22309\n",
            "epoch no.2 train no.83470  loss = 1.29407 avg_loss = 2.22386\n",
            "epoch no.2 train no.83480  loss = 3.12239 avg_loss = 2.24026\n",
            "epoch no.2 train no.83490  loss = 1.88785 avg_loss = 2.24384\n",
            "epoch no.2 train no.83500  loss = 3.32837 avg_loss = 2.28028\n",
            "epoch no.2 train no.83510  loss = 2.02424 avg_loss = 2.30520\n",
            "epoch no.2 train no.83520  loss = 2.83929 avg_loss = 2.31584\n",
            "epoch no.2 train no.83530  loss = 3.41182 avg_loss = 2.32386\n",
            "epoch no.2 train no.83540  loss = 3.03242 avg_loss = 2.31646\n",
            "epoch no.2 train no.83550  loss = 2.03015 avg_loss = 2.29461\n",
            "epoch no.2 train no.83560  loss = 2.42914 avg_loss = 2.33449\n",
            "epoch no.2 train no.83570  loss = 2.77682 avg_loss = 2.33732\n",
            "epoch no.2 train no.83580  loss = 1.99196 avg_loss = 2.33524\n",
            "epoch no.2 train no.83590  loss = 2.86422 avg_loss = 2.35258\n",
            "epoch no.2 train no.83600  loss = 1.86554 avg_loss = 2.32642\n",
            "epoch no.2 train no.83610  loss = 2.76121 avg_loss = 2.36490\n",
            "epoch no.2 train no.83620  loss = 2.05144 avg_loss = 2.34659\n",
            "epoch no.2 train no.83630  loss = 2.46967 avg_loss = 2.33930\n",
            "epoch no.2 train no.83640  loss = 2.74976 avg_loss = 2.34740\n",
            "epoch no.2 train no.83650  loss = 1.64178 avg_loss = 2.32837\n",
            "epoch no.2 train no.83660  loss = 1.52894 avg_loss = 2.30539\n",
            "epoch no.2 train no.83670  loss = 2.92652 avg_loss = 2.29519\n",
            "epoch no.2 train no.83680  loss = 2.15822 avg_loss = 2.28361\n",
            "epoch no.2 train no.83690  loss = 2.10348 avg_loss = 2.25938\n",
            "epoch no.2 train no.83700  loss = 2.89872 avg_loss = 2.28531\n",
            "epoch no.2 train no.83710  loss = 1.75294 avg_loss = 2.29653\n",
            "epoch no.2 train no.83720  loss = 2.32135 avg_loss = 2.28117\n",
            "epoch no.2 train no.83730  loss = 2.10596 avg_loss = 2.27401\n",
            "epoch no.2 train no.83740  loss = 1.58022 avg_loss = 2.29297\n",
            "epoch no.2 train no.83750  loss = 2.77483 avg_loss = 2.30783\n",
            "epoch no.2 train no.83760  loss = 2.06890 avg_loss = 2.28198\n",
            "epoch no.2 train no.83770  loss = 3.07628 avg_loss = 2.26963\n",
            "epoch no.2 train no.83780  loss = 1.99583 avg_loss = 2.27676\n",
            "epoch no.2 train no.83790  loss = 2.06860 avg_loss = 2.27091\n",
            "epoch no.2 train no.83800  loss = 2.21371 avg_loss = 2.25479\n",
            "epoch no.2 train no.83810  loss = 2.41979 avg_loss = 2.25443\n",
            "epoch no.2 train no.83820  loss = 1.72522 avg_loss = 2.27649\n",
            "epoch no.2 train no.83830  loss = 1.78179 avg_loss = 2.27910\n",
            "epoch no.2 train no.83840  loss = 2.34159 avg_loss = 2.27816\n",
            "epoch no.2 train no.83850  loss = 2.82922 avg_loss = 2.26732\n",
            "epoch no.2 train no.83860  loss = 1.60721 avg_loss = 2.28277\n",
            "epoch no.2 train no.83870  loss = 1.41596 avg_loss = 2.30363\n",
            "epoch no.2 train no.83880  loss = 2.28043 avg_loss = 2.30254\n",
            "epoch no.2 train no.83890  loss = 2.83219 avg_loss = 2.30842\n",
            "epoch no.2 train no.83900  loss = 2.01740 avg_loss = 2.30195\n",
            "epoch no.2 train no.83910  loss = 1.78702 avg_loss = 2.30276\n",
            "epoch no.2 train no.83920  loss = 1.61909 avg_loss = 2.29137\n",
            "epoch no.2 train no.83930  loss = 2.07217 avg_loss = 2.27233\n",
            "epoch no.2 train no.83940  loss = 2.79144 avg_loss = 2.28052\n",
            "epoch no.2 train no.83950  loss = 1.50170 avg_loss = 2.28862\n",
            "epoch no.2 train no.83960  loss = 1.68235 avg_loss = 2.26835\n",
            "epoch no.2 train no.83970  loss = 1.95116 avg_loss = 2.23248\n",
            "epoch no.2 train no.83980  loss = 2.12561 avg_loss = 2.22231\n",
            "epoch no.2 train no.83990  loss = 3.21557 avg_loss = 2.23936\n",
            "epoch no.2 train no.84000  loss = 1.84074 avg_loss = 2.23929\n",
            "epoch no.2 train no.84010  loss = 2.76737 avg_loss = 2.25030\n",
            "epoch no.2 train no.84020  loss = 2.16813 avg_loss = 2.27377\n",
            "epoch no.2 train no.84030  loss = 3.08130 avg_loss = 2.28151\n",
            "epoch no.2 train no.84040  loss = 2.61334 avg_loss = 2.28632\n",
            "epoch no.2 train no.84050  loss = 2.60497 avg_loss = 2.28553\n",
            "epoch no.2 train no.84060  loss = 2.70703 avg_loss = 2.29546\n",
            "epoch no.2 train no.84070  loss = 2.93146 avg_loss = 2.28798\n",
            "epoch no.2 train no.84080  loss = 1.63573 avg_loss = 2.26602\n",
            "epoch no.2 train no.84090  loss = 1.74849 avg_loss = 2.26968\n",
            "epoch no.2 train no.84100  loss = 3.20008 avg_loss = 2.27160\n",
            "epoch no.2 train no.84110  loss = 2.80791 avg_loss = 2.28010\n",
            "epoch no.2 train no.84120  loss = 2.85706 avg_loss = 2.28448\n",
            "epoch no.2 train no.84130  loss = 2.69650 avg_loss = 2.29839\n",
            "epoch no.2 train no.84140  loss = 2.17215 avg_loss = 2.29515\n",
            "epoch no.2 train no.84150  loss = 2.23641 avg_loss = 2.29875\n",
            "epoch no.2 train no.84160  loss = 1.73559 avg_loss = 2.29308\n",
            "epoch no.2 train no.84170  loss = 2.60426 avg_loss = 2.28840\n",
            "epoch no.2 train no.84180  loss = 2.16524 avg_loss = 2.28582\n",
            "epoch no.2 train no.84190  loss = 1.92203 avg_loss = 2.27460\n",
            "epoch no.2 train no.84200  loss = 2.45436 avg_loss = 2.28911\n",
            "epoch no.2 train no.84210  loss = 1.78388 avg_loss = 2.27229\n",
            "epoch no.2 train no.84220  loss = 2.30791 avg_loss = 2.27511\n",
            "epoch no.2 train no.84230  loss = 2.81941 avg_loss = 2.28398\n",
            "epoch no.2 train no.84240  loss = 1.55441 avg_loss = 2.27277\n",
            "epoch no.2 train no.84250  loss = 1.26870 avg_loss = 2.24242\n",
            "epoch no.2 train no.84260  loss = 2.69562 avg_loss = 2.26044\n",
            "epoch no.2 train no.84270  loss = 2.21261 avg_loss = 2.27539\n",
            "epoch no.2 train no.84280  loss = 2.81571 avg_loss = 2.29906\n",
            "epoch no.2 train no.84290  loss = 2.09413 avg_loss = 2.30540\n",
            "epoch no.2 train no.84300  loss = 2.57434 avg_loss = 2.29043\n",
            "epoch no.2 train no.84310  loss = 2.37151 avg_loss = 2.30233\n",
            "epoch no.2 train no.84320  loss = 1.95808 avg_loss = 2.29900\n",
            "epoch no.2 train no.84330  loss = 1.97883 avg_loss = 2.28563\n",
            "epoch no.2 train no.84340  loss = 2.94794 avg_loss = 2.28665\n",
            "epoch no.2 train no.84350  loss = 1.82735 avg_loss = 2.28555\n",
            "epoch no.2 train no.84360  loss = 2.74908 avg_loss = 2.29383\n",
            "epoch no.2 train no.84370  loss = 3.32762 avg_loss = 2.32951\n",
            "epoch no.2 train no.84380  loss = 3.02127 avg_loss = 2.35160\n",
            "epoch no.2 train no.84390  loss = 2.42495 avg_loss = 2.34563\n",
            "epoch no.2 train no.84400  loss = 3.13645 avg_loss = 2.34010\n",
            "epoch no.2 train no.84410  loss = 1.57657 avg_loss = 2.32806\n",
            "epoch no.2 train no.84420  loss = 2.91007 avg_loss = 2.35384\n",
            "epoch no.2 train no.84430  loss = 2.34355 avg_loss = 2.35567\n",
            "epoch no.2 train no.84440  loss = 2.67205 avg_loss = 2.33860\n",
            "epoch no.2 train no.84450  loss = 2.33651 avg_loss = 2.34202\n",
            "epoch no.2 train no.84460  loss = 2.27324 avg_loss = 2.35617\n",
            "epoch no.2 train no.84470  loss = 1.89483 avg_loss = 2.32694\n",
            "epoch no.2 train no.84480  loss = 2.19897 avg_loss = 2.31811\n",
            "epoch no.2 train no.84490  loss = 2.72927 avg_loss = 2.33062\n",
            "epoch no.2 train no.84500  loss = 2.83632 avg_loss = 2.34663\n",
            "epoch no.2 train no.84510  loss = 2.49861 avg_loss = 2.33716\n",
            "epoch no.2 train no.84520  loss = 2.38979 avg_loss = 2.35253\n",
            "epoch no.2 train no.84530  loss = 1.63559 avg_loss = 2.32313\n",
            "epoch no.2 train no.84540  loss = 2.61212 avg_loss = 2.31149\n",
            "epoch no.2 train no.84550  loss = 2.26058 avg_loss = 2.30168\n",
            "epoch no.2 train no.84560  loss = 2.16850 avg_loss = 2.29212\n",
            "epoch no.2 train no.84570  loss = 1.52713 avg_loss = 2.26020\n",
            "epoch no.2 train no.84580  loss = 2.31069 avg_loss = 2.26403\n",
            "epoch no.2 train no.84590  loss = 2.28611 avg_loss = 2.24778\n",
            "epoch no.2 train no.84600  loss = 1.71628 avg_loss = 2.27254\n",
            "epoch no.2 train no.84610  loss = 1.33333 avg_loss = 2.24661\n",
            "epoch no.2 train no.84620  loss = 3.09254 avg_loss = 2.24701\n",
            "epoch no.2 train no.84630  loss = 1.65355 avg_loss = 2.22682\n",
            "epoch no.2 train no.84640  loss = 0.93495 avg_loss = 2.21655\n",
            "epoch no.2 train no.84650  loss = 2.49916 avg_loss = 2.22519\n",
            "epoch no.2 train no.84660  loss = 2.04770 avg_loss = 2.22463\n",
            "epoch no.2 train no.84670  loss = 1.40740 avg_loss = 2.22838\n",
            "epoch no.2 train no.84680  loss = 2.26810 avg_loss = 2.20396\n",
            "epoch no.2 train no.84690  loss = 2.86272 avg_loss = 2.23387\n",
            "epoch no.2 train no.84700  loss = 2.48346 avg_loss = 2.24208\n",
            "epoch no.2 train no.84710  loss = 2.10310 avg_loss = 2.22331\n",
            "epoch no.2 train no.84720  loss = 2.76609 avg_loss = 2.21586\n",
            "epoch no.2 train no.84730  loss = 2.58075 avg_loss = 2.23177\n",
            "epoch no.2 train no.84740  loss = 1.81657 avg_loss = 2.23957\n",
            "epoch no.2 train no.84750  loss = 1.37069 avg_loss = 2.22859\n",
            "epoch no.2 train no.84760  loss = 1.57211 avg_loss = 2.21328\n",
            "epoch no.2 train no.84770  loss = 3.10827 avg_loss = 2.25005\n",
            "epoch no.2 train no.84780  loss = 3.11494 avg_loss = 2.25926\n",
            "epoch no.2 train no.84790  loss = 2.75853 avg_loss = 2.27767\n",
            "epoch no.2 train no.84800  loss = 2.04090 avg_loss = 2.27333\n",
            "epoch no.2 train no.84810  loss = 1.81052 avg_loss = 2.28499\n",
            "epoch no.2 train no.84820  loss = 2.46487 avg_loss = 2.30121\n",
            "epoch no.2 train no.84830  loss = 2.05131 avg_loss = 2.29499\n",
            "epoch no.2 train no.84840  loss = 1.44665 avg_loss = 2.27831\n",
            "epoch no.2 train no.84850  loss = 2.49660 avg_loss = 2.27759\n",
            "epoch no.2 train no.84860  loss = 1.47794 avg_loss = 2.27907\n",
            "epoch no.2 train no.84870  loss = 2.50636 avg_loss = 2.29470\n",
            "epoch no.2 train no.84880  loss = 2.56391 avg_loss = 2.29658\n",
            "epoch no.2 train no.84890  loss = 2.06360 avg_loss = 2.30474\n",
            "epoch no.2 train no.84900  loss = 2.65729 avg_loss = 2.30485\n",
            "epoch no.2 train no.84910  loss = 3.05006 avg_loss = 2.32351\n",
            "epoch no.2 train no.84920  loss = 2.89660 avg_loss = 2.31859\n",
            "epoch no.2 train no.84930  loss = 2.35820 avg_loss = 2.29182\n",
            "epoch no.2 train no.84940  loss = 3.19681 avg_loss = 2.30878\n",
            "epoch no.2 train no.84950  loss = 1.38181 avg_loss = 2.30487\n",
            "epoch no.2 train no.84960  loss = 1.59676 avg_loss = 2.28810\n",
            "epoch no.2 train no.84970  loss = 3.42559 avg_loss = 2.29572\n",
            "epoch no.2 train no.84980  loss = 2.52057 avg_loss = 2.33000\n",
            "epoch no.2 train no.84990  loss = 1.18082 avg_loss = 2.29609\n",
            "epoch no.2 train no.85000  loss = 1.41878 avg_loss = 2.29639\n",
            "epoch no.2 train no.85010  loss = 2.22509 avg_loss = 2.30768\n",
            "epoch no.2 train no.85020  loss = 1.96202 avg_loss = 2.28328\n",
            "epoch no.2 train no.85030  loss = 1.61695 avg_loss = 2.27268\n",
            "epoch no.2 train no.85040  loss = 2.48271 avg_loss = 2.27021\n",
            "epoch no.2 train no.85050  loss = 1.94742 avg_loss = 2.25474\n",
            "epoch no.2 train no.85060  loss = 1.99887 avg_loss = 2.25195\n",
            "epoch no.2 train no.85070  loss = 2.42839 avg_loss = 2.25079\n",
            "epoch no.2 train no.85080  loss = 3.06128 avg_loss = 2.27475\n",
            "epoch no.2 train no.85090  loss = 2.43866 avg_loss = 2.28606\n",
            "epoch no.2 train no.85100  loss = 3.09853 avg_loss = 2.30110\n",
            "epoch no.2 train no.85110  loss = 2.67987 avg_loss = 2.29933\n",
            "epoch no.2 train no.85120  loss = 2.51253 avg_loss = 2.29108\n",
            "epoch no.2 train no.85130  loss = 2.42011 avg_loss = 2.30563\n",
            "epoch no.2 train no.85140  loss = 2.66695 avg_loss = 2.33193\n",
            "epoch no.2 train no.85150  loss = 2.43387 avg_loss = 2.32325\n",
            "epoch no.2 train no.85160  loss = 2.17644 avg_loss = 2.30754\n",
            "epoch no.2 train no.85170  loss = 1.96408 avg_loss = 2.28503\n",
            "epoch no.2 train no.85180  loss = 1.85479 avg_loss = 2.25157\n",
            "epoch no.2 train no.85190  loss = 1.98760 avg_loss = 2.23406\n",
            "epoch no.2 train no.85200  loss = 2.19791 avg_loss = 2.25966\n",
            "epoch no.2 train no.85210  loss = 3.39276 avg_loss = 2.29037\n",
            "epoch no.2 train no.85220  loss = 1.73690 avg_loss = 2.29273\n",
            "epoch no.2 train no.85230  loss = 2.49522 avg_loss = 2.28238\n",
            "epoch no.2 train no.85240  loss = 2.25124 avg_loss = 2.31485\n",
            "epoch no.2 train no.85250  loss = 2.29757 avg_loss = 2.31569\n",
            "epoch no.2 train no.85260  loss = 2.59054 avg_loss = 2.28361\n",
            "epoch no.2 train no.85270  loss = 2.04896 avg_loss = 2.27974\n",
            "epoch no.2 train no.85280  loss = 1.40407 avg_loss = 2.25925\n",
            "epoch no.2 train no.85290  loss = 2.64748 avg_loss = 2.28022\n",
            "epoch no.2 train no.85300  loss = 2.03708 avg_loss = 2.27266\n",
            "epoch no.2 train no.85310  loss = 1.95724 avg_loss = 2.24704\n",
            "epoch no.2 train no.85320  loss = 3.52264 avg_loss = 2.25255\n",
            "epoch no.2 train no.85330  loss = 2.50056 avg_loss = 2.26493\n",
            "epoch no.2 train no.85340  loss = 2.26425 avg_loss = 2.25419\n",
            "epoch no.2 train no.85350  loss = 2.80958 avg_loss = 2.26224\n",
            "epoch no.2 train no.85360  loss = 2.80417 avg_loss = 2.28082\n",
            "epoch no.2 train no.85370  loss = 2.93802 avg_loss = 2.29626\n",
            "epoch no.2 train no.85380  loss = 3.28834 avg_loss = 2.33259\n",
            "epoch no.2 train no.85390  loss = 2.34473 avg_loss = 2.35109\n",
            "epoch no.2 train no.85400  loss = 2.93586 avg_loss = 2.36762\n",
            "epoch no.2 train no.85410  loss = 2.41972 avg_loss = 2.35283\n",
            "epoch no.2 train no.85420  loss = 3.09976 avg_loss = 2.34380\n",
            "epoch no.2 train no.85430  loss = 2.29606 avg_loss = 2.32356\n",
            "epoch no.2 train no.85440  loss = 2.36577 avg_loss = 2.34161\n",
            "epoch no.2 train no.85450  loss = 3.17473 avg_loss = 2.33800\n",
            "epoch no.2 train no.85460  loss = 1.67352 avg_loss = 2.34266\n",
            "epoch no.2 train no.85470  loss = 1.84510 avg_loss = 2.30101\n",
            "epoch no.2 train no.85480  loss = 2.48234 avg_loss = 2.28818\n",
            "epoch no.2 train no.85490  loss = 2.46395 avg_loss = 2.26755\n",
            "epoch no.2 train no.85500  loss = 2.63694 avg_loss = 2.27308\n",
            "epoch no.2 train no.85510  loss = 2.36792 avg_loss = 2.26282\n",
            "epoch no.2 train no.85520  loss = 2.21375 avg_loss = 2.24193\n",
            "epoch no.2 train no.85530  loss = 3.28041 avg_loss = 2.27372\n",
            "epoch no.2 train no.85540  loss = 3.36320 avg_loss = 2.30160\n",
            "epoch no.2 train no.85550  loss = 1.24380 avg_loss = 2.28040\n",
            "epoch no.2 train no.85560  loss = 2.48610 avg_loss = 2.25366\n",
            "epoch no.2 train no.85570  loss = 1.61146 avg_loss = 2.23323\n",
            "epoch no.2 train no.85580  loss = 2.36606 avg_loss = 2.25769\n",
            "epoch no.2 train no.85590  loss = 2.33758 avg_loss = 2.29953\n",
            "epoch no.2 train no.85600  loss = 2.51689 avg_loss = 2.31291\n",
            "epoch no.2 train no.85610  loss = 3.05490 avg_loss = 2.32074\n",
            "epoch no.2 train no.85620  loss = 1.49404 avg_loss = 2.29748\n",
            "epoch no.2 train no.85630  loss = 3.52359 avg_loss = 2.32705\n",
            "epoch no.2 train no.85640  loss = 1.88926 avg_loss = 2.31674\n",
            "epoch no.2 train no.85650  loss = 2.09821 avg_loss = 2.30276\n",
            "epoch no.2 train no.85660  loss = 1.84874 avg_loss = 2.27649\n",
            "epoch no.2 train no.85670  loss = 2.38675 avg_loss = 2.27348\n",
            "epoch no.2 train no.85680  loss = 2.82778 avg_loss = 2.29505\n",
            "epoch no.2 train no.85690  loss = 2.55299 avg_loss = 2.28675\n",
            "epoch no.2 train no.85700  loss = 1.45440 avg_loss = 2.27908\n",
            "epoch no.2 train no.85710  loss = 2.04304 avg_loss = 2.25667\n",
            "epoch no.2 train no.85720  loss = 1.83216 avg_loss = 2.28420\n",
            "epoch no.2 train no.85730  loss = 2.45650 avg_loss = 2.28651\n",
            "epoch no.2 train no.85740  loss = 2.69597 avg_loss = 2.25964\n",
            "epoch no.2 train no.85750  loss = 2.05197 avg_loss = 2.25659\n",
            "epoch no.2 train no.85760  loss = 2.53408 avg_loss = 2.23633\n",
            "epoch no.2 train no.85770  loss = 2.50109 avg_loss = 2.22813\n",
            "epoch no.2 train no.85780  loss = 2.63215 avg_loss = 2.22105\n",
            "epoch no.2 train no.85790  loss = 2.01365 avg_loss = 2.22754\n",
            "epoch no.2 train no.85800  loss = 1.70834 avg_loss = 2.22545\n",
            "epoch no.2 train no.85810  loss = 2.37062 avg_loss = 2.21245\n",
            "epoch no.2 train no.85820  loss = 1.39446 avg_loss = 2.19466\n",
            "epoch no.2 train no.85830  loss = 1.43979 avg_loss = 2.21811\n",
            "epoch no.2 train no.85840  loss = 2.61690 avg_loss = 2.25770\n",
            "epoch no.2 train no.85850  loss = 2.79290 avg_loss = 2.26897\n",
            "epoch no.2 train no.85860  loss = 1.69469 avg_loss = 2.25732\n",
            "epoch no.2 train no.85870  loss = 1.87648 avg_loss = 2.30355\n",
            "epoch no.2 train no.85880  loss = 2.31390 avg_loss = 2.31010\n",
            "epoch no.2 train no.85890  loss = 2.24190 avg_loss = 2.29859\n",
            "epoch no.2 train no.85900  loss = 0.77502 avg_loss = 2.31348\n",
            "epoch no.2 train no.85910  loss = 1.86909 avg_loss = 2.30611\n",
            "epoch no.2 train no.85920  loss = 1.84631 avg_loss = 2.29733\n",
            "epoch no.2 train no.85930  loss = 2.10738 avg_loss = 2.30378\n",
            "epoch no.2 train no.85940  loss = 3.32461 avg_loss = 2.31442\n",
            "epoch no.2 train no.85950  loss = 1.99152 avg_loss = 2.31783\n",
            "epoch no.2 train no.85960  loss = 2.59448 avg_loss = 2.30694\n",
            "epoch no.2 train no.85970  loss = 2.08775 avg_loss = 2.30288\n",
            "epoch no.2 train no.85980  loss = 2.87859 avg_loss = 2.31351\n",
            "epoch no.2 train no.85990  loss = 2.08559 avg_loss = 2.30804\n",
            "epoch no.2 train no.86000  loss = 2.50279 avg_loss = 2.31820\n",
            "epoch no.2 train no.86010  loss = 2.58472 avg_loss = 2.33741\n",
            "epoch no.2 train no.86020  loss = 2.08081 avg_loss = 2.34778\n",
            "epoch no.2 train no.86030  loss = 2.62862 avg_loss = 2.32023\n",
            "epoch no.2 train no.86040  loss = 1.66967 avg_loss = 2.28798\n",
            "epoch no.2 train no.86050  loss = 2.45050 avg_loss = 2.29697\n",
            "epoch no.2 train no.86060  loss = 3.29868 avg_loss = 2.30615\n",
            "epoch no.2 train no.86070  loss = 2.27078 avg_loss = 2.33288\n",
            "epoch no.2 train no.86080  loss = 1.66813 avg_loss = 2.29451\n",
            "epoch no.2 train no.86090  loss = 2.93669 avg_loss = 2.29757\n",
            "epoch no.2 train no.86100  loss = 2.33118 avg_loss = 2.28668\n",
            "epoch no.2 train no.86110  loss = 1.63242 avg_loss = 2.26749\n",
            "epoch no.2 train no.86120  loss = 2.93852 avg_loss = 2.26919\n",
            "epoch no.2 train no.86130  loss = 2.14669 avg_loss = 2.24697\n",
            "epoch no.2 train no.86140  loss = 1.60692 avg_loss = 2.24344\n",
            "epoch no.2 train no.86150  loss = 1.58856 avg_loss = 2.23123\n",
            "epoch no.2 train no.86160  loss = 2.57934 avg_loss = 2.21094\n",
            "epoch no.2 train no.86170  loss = 1.70625 avg_loss = 2.20939\n",
            "epoch no.2 train no.86180  loss = 2.67410 avg_loss = 2.21846\n",
            "epoch no.2 train no.86190  loss = 2.02788 avg_loss = 2.24165\n",
            "epoch no.2 train no.86200  loss = 2.14857 avg_loss = 2.25541\n",
            "epoch no.2 train no.86210  loss = 2.08476 avg_loss = 2.24829\n",
            "epoch no.2 train no.86220  loss = 3.00623 avg_loss = 2.26540\n",
            "epoch no.2 train no.86230  loss = 3.10582 avg_loss = 2.27596\n",
            "epoch no.2 train no.86240  loss = 1.69246 avg_loss = 2.28534\n",
            "epoch no.2 train no.86250  loss = 3.39233 avg_loss = 2.28981\n",
            "epoch no.2 train no.86260  loss = 2.15013 avg_loss = 2.29651\n",
            "epoch no.2 train no.86270  loss = 2.42493 avg_loss = 2.30765\n",
            "epoch no.2 train no.86280  loss = 2.61036 avg_loss = 2.29964\n",
            "epoch no.2 train no.86290  loss = 1.94096 avg_loss = 2.31272\n",
            "epoch no.2 train no.86300  loss = 2.45160 avg_loss = 2.29654\n",
            "epoch no.2 train no.86310  loss = 3.22616 avg_loss = 2.29347\n",
            "epoch no.2 train no.86320  loss = 1.81018 avg_loss = 2.26042\n",
            "epoch no.2 train no.86330  loss = 1.71468 avg_loss = 2.24764\n",
            "epoch no.2 train no.86340  loss = 2.45609 avg_loss = 2.26023\n",
            "epoch no.2 train no.86350  loss = 2.57176 avg_loss = 2.25549\n",
            "epoch no.2 train no.86360  loss = 2.44151 avg_loss = 2.22686\n",
            "epoch no.2 train no.86370  loss = 2.13385 avg_loss = 2.21258\n",
            "epoch no.2 train no.86380  loss = 2.32192 avg_loss = 2.20892\n",
            "epoch no.2 train no.86390  loss = 1.84200 avg_loss = 2.21931\n",
            "epoch no.2 train no.86400  loss = 2.17569 avg_loss = 2.22377\n",
            "epoch no.2 train no.86410  loss = 2.25492 avg_loss = 2.20369\n",
            "epoch no.2 train no.86420  loss = 2.40417 avg_loss = 2.22464\n",
            "epoch no.2 train no.86430  loss = 1.78913 avg_loss = 2.22414\n",
            "epoch no.2 train no.86440  loss = 2.07527 avg_loss = 2.21129\n",
            "epoch no.2 train no.86450  loss = 2.54141 avg_loss = 2.22369\n",
            "epoch no.2 train no.86460  loss = 1.92689 avg_loss = 2.21934\n",
            "epoch no.2 train no.86470  loss = 1.87015 avg_loss = 2.22039\n",
            "epoch no.2 train no.86480  loss = 2.54587 avg_loss = 2.23426\n",
            "epoch no.2 train no.86490  loss = 1.68416 avg_loss = 2.22510\n",
            "epoch no.2 train no.86500  loss = 1.89003 avg_loss = 2.21247\n",
            "epoch no.2 train no.86510  loss = 2.29944 avg_loss = 2.20042\n",
            "epoch no.2 train no.86520  loss = 2.52878 avg_loss = 2.22232\n",
            "epoch no.2 train no.86530  loss = 2.21855 avg_loss = 2.22277\n",
            "epoch no.2 train no.86540  loss = 1.85994 avg_loss = 2.24690\n",
            "epoch no.2 train no.86550  loss = 2.88750 avg_loss = 2.26141\n",
            "epoch no.2 train no.86560  loss = 2.90518 avg_loss = 2.29255\n",
            "epoch no.2 train no.86570  loss = 2.36696 avg_loss = 2.29709\n",
            "epoch no.2 train no.86580  loss = 2.26243 avg_loss = 2.31049\n",
            "epoch no.2 train no.86590  loss = 1.87687 avg_loss = 2.30700\n",
            "epoch no.2 train no.86600  loss = 2.22959 avg_loss = 2.30958\n",
            "epoch no.2 train no.86610  loss = 1.96938 avg_loss = 2.30281\n",
            "epoch no.2 train no.86620  loss = 1.90574 avg_loss = 2.29711\n",
            "epoch no.2 train no.86630  loss = 1.54313 avg_loss = 2.28349\n",
            "epoch no.2 train no.86640  loss = 1.59550 avg_loss = 2.26524\n",
            "epoch no.2 train no.86650  loss = 1.95188 avg_loss = 2.24060\n",
            "epoch no.2 train no.86660  loss = 2.80768 avg_loss = 2.24341\n",
            "epoch no.2 train no.86670  loss = 2.81617 avg_loss = 2.24082\n",
            "epoch no.2 train no.86680  loss = 2.81857 avg_loss = 2.23281\n",
            "epoch no.2 train no.86690  loss = 2.16732 avg_loss = 2.21252\n",
            "epoch no.2 train no.86700  loss = 2.11635 avg_loss = 2.20530\n",
            "epoch no.2 train no.86710  loss = 2.41862 avg_loss = 2.22205\n",
            "epoch no.2 train no.86720  loss = 2.25421 avg_loss = 2.22000\n",
            "epoch no.2 train no.86730  loss = 2.07530 avg_loss = 2.24341\n",
            "epoch no.2 train no.86740  loss = 2.00774 avg_loss = 2.22724\n",
            "epoch no.2 train no.86750  loss = 2.64197 avg_loss = 2.24480\n",
            "epoch no.2 train no.86760  loss = 1.75741 avg_loss = 2.23372\n",
            "epoch no.2 train no.86770  loss = 1.59146 avg_loss = 2.21938\n",
            "epoch no.2 train no.86780  loss = 1.08295 avg_loss = 2.18512\n",
            "epoch no.2 train no.86790  loss = 2.86634 avg_loss = 2.19825\n",
            "epoch no.2 train no.86800  loss = 2.89743 avg_loss = 2.21132\n",
            "epoch no.2 train no.86810  loss = 2.80946 avg_loss = 2.20582\n",
            "epoch no.2 train no.86820  loss = 1.65641 avg_loss = 2.22487\n",
            "epoch no.2 train no.86830  loss = 3.16485 avg_loss = 2.22791\n",
            "epoch no.2 train no.86840  loss = 2.66579 avg_loss = 2.24068\n",
            "epoch no.2 train no.86850  loss = 2.39665 avg_loss = 2.25431\n",
            "epoch no.2 train no.86860  loss = 2.63215 avg_loss = 2.27476\n",
            "epoch no.2 train no.86870  loss = 1.72082 avg_loss = 2.26311\n",
            "epoch no.2 train no.86880  loss = 1.69955 avg_loss = 2.27079\n",
            "epoch no.2 train no.86890  loss = 1.56579 avg_loss = 2.27149\n",
            "epoch no.2 train no.86900  loss = 2.07879 avg_loss = 2.26250\n",
            "epoch no.2 train no.86910  loss = 1.99713 avg_loss = 2.25638\n",
            "epoch no.2 train no.86920  loss = 3.14509 avg_loss = 2.27321\n",
            "epoch no.2 train no.86930  loss = 2.14035 avg_loss = 2.25908\n",
            "epoch no.2 train no.86940  loss = 2.70959 avg_loss = 2.24244\n",
            "epoch no.2 train no.86950  loss = 2.07744 avg_loss = 2.25038\n",
            "epoch no.2 train no.86960  loss = 2.31551 avg_loss = 2.23386\n",
            "epoch no.2 train no.86970  loss = 2.51107 avg_loss = 2.22481\n",
            "epoch no.2 train no.86980  loss = 2.26627 avg_loss = 2.22889\n",
            "epoch no.2 train no.86990  loss = 2.29615 avg_loss = 2.21628\n",
            "epoch no.2 train no.87000  loss = 2.63071 avg_loss = 2.24959\n",
            "epoch no.2 train no.87010  loss = 2.89714 avg_loss = 2.25773\n",
            "epoch no.2 train no.87020  loss = 2.51108 avg_loss = 2.26979\n",
            "epoch no.2 train no.87030  loss = 2.88367 avg_loss = 2.27213\n",
            "epoch no.2 train no.87040  loss = 2.15058 avg_loss = 2.26013\n",
            "epoch no.2 train no.87050  loss = 3.03651 avg_loss = 2.25636\n",
            "epoch no.2 train no.87060  loss = 1.12837 avg_loss = 2.24671\n",
            "epoch no.2 train no.87070  loss = 2.38093 avg_loss = 2.24387\n",
            "epoch no.2 train no.87080  loss = 1.99201 avg_loss = 2.24363\n",
            "epoch no.2 train no.87090  loss = 2.51250 avg_loss = 2.25818\n",
            "epoch no.2 train no.87100  loss = 2.19994 avg_loss = 2.23992\n",
            "epoch no.2 train no.87110  loss = 2.78850 avg_loss = 2.22405\n",
            "epoch no.2 train no.87120  loss = 2.13541 avg_loss = 2.21245\n",
            "epoch no.2 train no.87130  loss = 1.33754 avg_loss = 2.20342\n",
            "epoch no.2 train no.87140  loss = 2.21026 avg_loss = 2.16698\n",
            "epoch no.2 train no.87150  loss = 2.23023 avg_loss = 2.20849\n",
            "epoch no.2 train no.87160  loss = 2.08076 avg_loss = 2.22679\n",
            "epoch no.2 train no.87170  loss = 1.91769 avg_loss = 2.23473\n",
            "epoch no.2 train no.87180  loss = 2.75042 avg_loss = 2.23452\n",
            "epoch no.2 train no.87190  loss = 1.50380 avg_loss = 2.23942\n",
            "epoch no.2 train no.87200  loss = 3.33485 avg_loss = 2.25789\n",
            "epoch no.2 train no.87210  loss = 2.15267 avg_loss = 2.25992\n",
            "epoch no.2 train no.87220  loss = 2.83374 avg_loss = 2.27599\n",
            "epoch no.2 train no.87230  loss = 2.03536 avg_loss = 2.24731\n",
            "epoch no.2 train no.87240  loss = 1.87971 avg_loss = 2.23476\n",
            "epoch no.2 train no.87250  loss = 2.47090 avg_loss = 2.20399\n",
            "epoch no.2 train no.87260  loss = 2.24428 avg_loss = 2.21687\n",
            "epoch no.2 train no.87270  loss = 2.37108 avg_loss = 2.22297\n",
            "epoch no.2 train no.87280  loss = 1.26011 avg_loss = 2.20966\n",
            "epoch no.2 train no.87290  loss = 1.94970 avg_loss = 2.21544\n",
            "epoch no.2 train no.87300  loss = 1.98683 avg_loss = 2.22352\n",
            "epoch no.2 train no.87310  loss = 2.43830 avg_loss = 2.22583\n",
            "epoch no.2 train no.87320  loss = 2.52221 avg_loss = 2.24607\n",
            "epoch no.2 train no.87330  loss = 2.54715 avg_loss = 2.25525\n",
            "epoch no.2 train no.87340  loss = 2.23971 avg_loss = 2.26191\n",
            "epoch no.2 train no.87350  loss = 2.74497 avg_loss = 2.27140\n",
            "epoch no.2 train no.87360  loss = 1.66754 avg_loss = 2.26512\n",
            "epoch no.2 train no.87370  loss = 1.60765 avg_loss = 2.25696\n",
            "epoch no.2 train no.87380  loss = 2.67965 avg_loss = 2.27285\n",
            "epoch no.2 train no.87390  loss = 2.01050 avg_loss = 2.24856\n",
            "epoch no.2 train no.87400  loss = 2.20463 avg_loss = 2.24451\n",
            "epoch no.2 train no.87410  loss = 2.02452 avg_loss = 2.23230\n",
            "epoch no.2 train no.87420  loss = 1.59169 avg_loss = 2.24183\n",
            "epoch no.2 train no.87430  loss = 1.84487 avg_loss = 2.21404\n",
            "epoch no.2 train no.87440  loss = 2.32622 avg_loss = 2.23724\n",
            "epoch no.2 train no.87450  loss = 2.37758 avg_loss = 2.24455\n",
            "epoch no.2 train no.87460  loss = 1.38028 avg_loss = 2.26331\n",
            "epoch no.2 train no.87470  loss = 3.09638 avg_loss = 2.25781\n",
            "epoch no.2 train no.87480  loss = 2.42697 avg_loss = 2.27706\n",
            "epoch no.2 train no.87490  loss = 2.35130 avg_loss = 2.28332\n",
            "epoch no.2 train no.87500  loss = 2.10023 avg_loss = 2.28660\n",
            "epoch no.2 train no.87510  loss = 1.68758 avg_loss = 2.26661\n",
            "epoch no.2 train no.87520  loss = 2.49984 avg_loss = 2.25027\n",
            "epoch no.2 train no.87530  loss = 2.17148 avg_loss = 2.24291\n",
            "epoch no.2 train no.87540  loss = 2.65903 avg_loss = 2.25986\n",
            "epoch no.2 train no.87550  loss = 3.06543 avg_loss = 2.28166\n",
            "epoch no.2 train no.87560  loss = 2.65674 avg_loss = 2.30561\n",
            "epoch no.2 train no.87570  loss = 2.05095 avg_loss = 2.30690\n",
            "epoch no.2 train no.87580  loss = 2.42128 avg_loss = 2.28953\n",
            "epoch no.2 train no.87590  loss = 3.19795 avg_loss = 2.32947\n",
            "epoch no.2 train no.87600  loss = 2.62952 avg_loss = 2.34362\n",
            "epoch no.2 train no.87610  loss = 3.13203 avg_loss = 2.35958\n",
            "epoch no.2 train no.87620  loss = 1.20153 avg_loss = 2.31412\n",
            "epoch no.2 train no.87630  loss = 2.52918 avg_loss = 2.29597\n",
            "epoch no.2 train no.87640  loss = 1.44682 avg_loss = 2.26986\n",
            "epoch no.2 train no.87650  loss = 1.91318 avg_loss = 2.27204\n",
            "epoch no.2 train no.87660  loss = 2.12088 avg_loss = 2.25999\n",
            "epoch no.2 train no.87670  loss = 1.75714 avg_loss = 2.24122\n",
            "epoch no.2 train no.87680  loss = 2.10120 avg_loss = 2.24141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWl5Z0fmbqeU",
        "colab_type": "text"
      },
      "source": [
        "# 이제 부터는 관련 연구들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjlJXSPUaRfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytBhpEJnbO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"사랑한다 <|endoftext|> [Verse 1: J-Hope] 힙합이란 것은 원래 그런 거지 원래 그런 거지 원래 그런 거지 원래 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐 [Verse 2: Suga] 뭐 어쩌겠어 그딴 게 어<unk> Hangul [Verse 3: RM, Suga] 그래 내가 뭐 틀린 말했어 내가 뭐 거짓말했어 이길 수 있을까 이 기적 아닌 기적을 우리가 만든 걸까 (No) 난 여기 있었고 니가 내게 다가와준 거야 I do believe your galaxy 듣고 싶어 너의 멜로디 너의 은하수의 별들은 너의 하늘을 과연 어떻게 수놓을지 나의 절망 끝에 결국 내가 널 찾았음을 잊지마 넌 절벽 끝에 서 있던 내 마지막 이유야 Live [Pre-Chorus: Jin, RM] 나의 Daydream Daydream Daydream Daydream Daydream Daydream Last Daydream Daydream Daydream Daydream</s>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Iow6H0aRrw",
        "colab_type": "code",
        "outputId": "fc6220a7-42b5-42d1-e3b1-e2c7884f6962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "for sent in kss.split_sentences(text):\n",
        "    print(sent)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "사랑한다 <|endoftext|> [Verse 1: J-Hope] 힙합이란 것은 원래 그런 거지 원래 그런 거지 원래 그런 거지 원래 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐, 그냥 그런 거지 뭐 [Verse 2: Suga] 뭐 어쩌겠어 그딴 게 어<unk> Hangul [Verse 3: RM, Suga] 그래 내가 뭐 틀린 말했어 내가 뭐 거짓말했어 이길 수 있을까 이 기적 아닌 기적을 우리가 만든 걸까 (No) 난 여기 있었고 니가 내게 다가와준 거야 I do believe your galaxy 듣고 싶어 너의 멜로디 너의 은하수의 별들은 너의 하늘을 과연 어떻게 수놓을지 나의 절망 끝에 결국 내가 널 찾았음을 잊지마 넌 절벽 끝에 서 있던 내 마지막 이유야 Live [Pre-Chorus: Jin, RM] 나의 Daydream Daydream Daydream Daydream Daydream Daydream Last Daydream Daydream Daydream Daydream</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vsTa7iHtxlV",
        "colab_type": "code",
        "outputId": "325c3216-1c0c-45fc-d94d-33f63b747581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "s = \"회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요 다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다 강남역 맛집 토끼정의 외부 모습.\"\n",
        "for sent in kss.split_sentences(s):\n",
        "    print(sent)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "회사 동료 분들과 다녀왔는데 분위기도 좋고 음식도 맛있었어요\n",
            "다만, 강남 토끼정이 강남 쉑쉑버거 골목길로 쭉 올라가야 하는데 다들 쉑쉑버거의 유혹에 넘어갈 뻔 했답니다\n",
            "강남역 맛집 토끼정의 외부 모습.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwCBhD1bmEJX",
        "colab_type": "code",
        "outputId": "889ee824-2e78-4cea-f93a-74577fc36876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!pip install py-hanspell"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py-hanspell\n",
            "  Using cached https://files.pythonhosted.org/packages/24/81/baac88868b58eea5b0aab675da7ef0cdd72bee62b080cd50336a76faf57f/py-hanspell-1.1.tar.gz\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAvL8M8Dpure",
        "colab_type": "code",
        "outputId": "b36e1f48-8b88-4ad2-fe2d-955d47cbcfd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "test = []\n",
        "for sent in kss.split_sentences(text):\n",
        "    test.append(sent)\n",
        "\n",
        "test = \"\\n\".join(test)\n",
        "\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "사랑하지 않을래 널 위해서라면 난 아파도 강한 척 할 수가 있었어 사랑이 사랑만으로 완벽하길 내 모든 약점들은 다 숨겨지길 이뤄지지 않는 꿈속에서 피울 수 없는 꽃을 키웠어 [Hook: Jimin, Jin] I'm so sick of this fake love, fake love, fake love I'm so sorry but it's fake love, fake love, fake love [Verse 2: RM, RM & Jungkook, J-Hope] I wanna be a good man just for you 세상을 줬네 just for you 전부 바꿨어 just for you Now I don't know me, who are you? 우리만의 숲 너는 없었어 나도 내가 되고 싶었던 거야 나도 내가 되고 싶었던 거야 너의 최고가 되고 싶었던 거야 baby I wanna know you I don't know you</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNp-tUu-p3sX",
        "colab_type": "code",
        "outputId": "ea900d4f-a8a7-4682-c168-1939dce70098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "from hanspell import spell_checker\n",
        "result = spell_checker.check(test)\n",
        "result.as_dict()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-95fb84506a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhanspell\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hanspell'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2nd0AbiqA55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}